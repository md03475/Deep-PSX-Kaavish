{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "**Natural Language Processing or NLP** is a branch of Artificial Intelligence which deal with bridging the machines understanding humans in their Natural Language. Natural Language can be in form of text or sound, which are used for humans to communicate each other. NLP can enable humans to communicate to machines in a natural way. \n",
    "\n",
    "\n",
    "**Text Classification** is a process involved in Sentiment Analysis. It is classification of peoples opinion or expressions into different sentiments. Sentiments include *Positive, Neutral*, and *Negative*, *Review Ratings* and *Happy, Sad*. Sentiment Analysis can be done on different consumer centered industries to analyse people's opinion on a particular product or subject. \n",
    "![Sentiment Analysis](https://media-exp1.licdn.com/dms/image/C4D12AQHPAZFZZxBtng/article-cover_image-shrink_600_2000/0?e=1593648000&v=beta&t=eQAR5WOihE2_ZCCAJbsgNyJlaI_GW7u8lDw45zGbfuU)\n",
    "> Sentiment Classification is a perfect problem in NLP for getting started in it. You can really learn a lot of concepts and techniques to master through doing project. Kaggle is a great place to learn and contribute your own ideas and creations. I learnt lot of things from other, now it's my turn to make document my project.\n",
    "\n",
    "I will go through all the key and fundament concepts of NLP and Sequence Models, which you will learn in this notebook. \n",
    "![Sentiment Analysis](https://fiverr-res.cloudinary.com/images/t_main1,q_auto,f_auto,q_auto,f_auto/gigs/121192228/original/677c209a0a064cb9253973d3663684acf91dab84/do-nlp-projects-with-python-nltk-gensim.jpg)\n",
    "Let's get started with code without furthur ado.\n",
    "\n",
    "<font color='red'> If you find this notebook helpful, please leave a UPVOTE to encourage me</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Importing Dependencies\n",
    "   We shall start by importing all the neccessary libraries. I will explain the exact use of each library later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version 1.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\CZ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "print(\"Tensorflow Version\",tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Dataset Preprocessing\n",
    "In this notebook, I am using **Sentiment-140** from [Kaggle](https://www.kaggle.com/kazanova/sentiment140). It contains a labels data of 1.6 Million Tweets and I find it a good amount of data to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Date</th>\n",
       "      <th>News</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>Cum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19 04 2013</td>\n",
       "      <td>PM directs early completion of projects in Bal...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>19 04 2013</td>\n",
       "      <td>RECORDER REPORT: KSE mid day update - KARACHI:...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.5574</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>21 04 2013</td>\n",
       "      <td>Most kitchen items become dearer - ABDUL RASHE...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.7184</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>21 04 2013</td>\n",
       "      <td>Wattoo demands security as per entitlement to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>22 04 2013</td>\n",
       "      <td>SC directs FIA to investigate awarding of lice...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.7579</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        Date                                               News  \\\n",
       "0      0  19 04 2013  PM directs early completion of projects in Bal...   \n",
       "1      1  19 04 2013  RECORDER REPORT: KSE mid day update - KARACHI:...   \n",
       "2      2  21 04 2013  Most kitchen items become dearer - ABDUL RASHE...   \n",
       "3      3  21 04 2013  Wattoo demands security as per entitlement to ...   \n",
       "4      4  22 04 2013  SC directs FIA to investigate awarding of lice...   \n",
       "\n",
       "   Polarity  Sentiment  neg    neu    pos  compound  Cum  \n",
       "0         1          1  0.0  0.884  0.116    0.5267    1  \n",
       "1         1          1  0.0  0.874  0.126    0.5574    1  \n",
       "2         1          1  0.0  0.720  0.280    0.7184    1  \n",
       "3         1          1  0.0  0.918  0.082    0.3400    1  \n",
       "4         1          1  0.0  0.806  0.194    0.7579    1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('All_News.csv')\n",
    "df = df.dropna()\n",
    "df = df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the columns are without any proper names. Lets rename them for our reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train only on text to classify its sentiment. So we can ditch the rest of the useless columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>Cum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10515.000000</td>\n",
       "      <td>10515.000000</td>\n",
       "      <td>10515.000000</td>\n",
       "      <td>10515.000000</td>\n",
       "      <td>10515.000000</td>\n",
       "      <td>10515.000000</td>\n",
       "      <td>10515.000000</td>\n",
       "      <td>10515.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5257.000000</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>0.076030</td>\n",
       "      <td>0.830364</td>\n",
       "      <td>0.093605</td>\n",
       "      <td>0.087019</td>\n",
       "      <td>0.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3035.563374</td>\n",
       "      <td>0.799274</td>\n",
       "      <td>0.799274</td>\n",
       "      <td>0.124271</td>\n",
       "      <td>0.140714</td>\n",
       "      <td>0.109041</td>\n",
       "      <td>0.501812</td>\n",
       "      <td>0.799274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.979900</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2628.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.296000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5257.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7885.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.934000</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.510600</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10514.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.773000</td>\n",
       "      <td>0.973200</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              index      Polarity     Sentiment           neg           neu  \\\n",
       "count  10515.000000  10515.000000  10515.000000  10515.000000  10515.000000   \n",
       "mean    5257.000000      0.167000      0.167000      0.076030      0.830364   \n",
       "std     3035.563374      0.799274      0.799274      0.124271      0.140714   \n",
       "min        0.000000     -1.000000     -1.000000      0.000000      0.099000   \n",
       "25%     2628.500000      0.000000      0.000000      0.000000      0.750000   \n",
       "50%     5257.000000      0.000000      0.000000      0.000000      0.845000   \n",
       "75%     7885.500000      1.000000      1.000000      0.128000      0.934000   \n",
       "max    10514.000000      1.000000      1.000000      0.821000      1.000000   \n",
       "\n",
       "                pos      compound           Cum  \n",
       "count  10515.000000  10515.000000  10515.000000  \n",
       "mean       0.093605      0.087019      0.167000  \n",
       "std        0.109041      0.501812      0.799274  \n",
       "min        0.000000     -0.979900     -1.000000  \n",
       "25%        0.000000     -0.296000      0.000000  \n",
       "50%        0.071000      0.000000      0.000000  \n",
       "75%        0.161000      0.510600      1.000000  \n",
       "max        0.773000      0.973200      1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    4383\n",
       " 0    3505\n",
       "-1    2627\n",
       "Name: Polarity, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Polarity'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are decoding the labels. We map **0 -> Negative and 1 -> Positive** as directed by the datset desciption. Now that we decoded we shall now analyse the dataset by its distribution. Because it's important that we have almost small amount of examples for given classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 4383 , Negative: 2627 , Neutral: 3505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Sentiment Data Distribution')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "count_pos = df['Polarity'].value_counts()[1]\n",
    "count_ntl = df['Polarity'].value_counts()[0]\n",
    "count_neg = df['Polarity'].value_counts()[-1]\n",
    "\n",
    "print('Positive:', count_pos, ',', 'Negative:', count_neg, ',', 'Neutral:', count_ntl)\n",
    "\n",
    "plt.bar(['Negative', 'Neutral', 'Positive'], [count_neg, count_ntl, count_pos])\n",
    "plt.title(\"Sentiment Data Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a very good dataset without any skewness. Thank Goodness.\n",
    "\n",
    "Now let us explore the data we having here... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Date</th>\n",
       "      <th>News</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>Cum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7934</th>\n",
       "      <td>7934</td>\n",
       "      <td>12 07 2018</td>\n",
       "      <td>What can help us out of the mess we're in?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.0516</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8034</th>\n",
       "      <td>8034</td>\n",
       "      <td>25 09 2015</td>\n",
       "      <td>‘Bullish’ trend at cattle market just ahead of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5892</th>\n",
       "      <td>5892</td>\n",
       "      <td>24 01 2015</td>\n",
       "      <td>Two more suspects, linked with IS, held</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4005</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>4598</td>\n",
       "      <td>15 04 2020</td>\n",
       "      <td>Punjab to soon increase coronavirus testing ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.3182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5737</th>\n",
       "      <td>5737</td>\n",
       "      <td>26 02 2014</td>\n",
       "      <td>Three militants arrested in Swabi</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>2046</td>\n",
       "      <td>25 01 2018</td>\n",
       "      <td>Mauritius business confidence jumps to its hig...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.7650</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>1190</td>\n",
       "      <td>28 06 2016</td>\n",
       "      <td>CJ advises LHC staff to work with commitment -...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.8225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4071</th>\n",
       "      <td>4071</td>\n",
       "      <td>11 02 2020</td>\n",
       "      <td>Modi has committed 'Himalayan blunder': Punjab...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7066</th>\n",
       "      <td>7066</td>\n",
       "      <td>10 01 2016</td>\n",
       "      <td>AJK govt opposes moves to convert GB into prov...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>464</td>\n",
       "      <td>18 11 2014</td>\n",
       "      <td>SC disposes of petitions seeking PM's disquali...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.5574</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index        Date                                               News  \\\n",
       "7934   7934  12 07 2018         What can help us out of the mess we're in?   \n",
       "8034   8034  25 09 2015  ‘Bullish’ trend at cattle market just ahead of...   \n",
       "5892   5892  24 01 2015            Two more suspects, linked with IS, held   \n",
       "4598   4598  15 04 2020  Punjab to soon increase coronavirus testing ca...   \n",
       "5737   5737  26 02 2014                  Three militants arrested in Swabi   \n",
       "2046   2046  25 01 2018  Mauritius business confidence jumps to its hig...   \n",
       "1190   1190  28 06 2016  CJ advises LHC staff to work with commitment -...   \n",
       "4071   4071  11 02 2020  Modi has committed 'Himalayan blunder': Punjab...   \n",
       "7066   7066  10 01 2016  AJK govt opposes moves to convert GB into prov...   \n",
       "464     464  18 11 2014  SC disposes of petitions seeking PM's disquali...   \n",
       "\n",
       "      Polarity  Sentiment    neg    neu    pos  compound  Cum  \n",
       "7934         0          0  0.189  0.606  0.205    0.0516    0  \n",
       "8034         0          0  0.000  1.000  0.000    0.0000    0  \n",
       "5892        -1         -1  0.310  0.690  0.000   -0.4005   -1  \n",
       "4598         1          1  0.000  0.922  0.078    0.3182    1  \n",
       "5737        -1         -1  0.437  0.563  0.000   -0.4767   -1  \n",
       "2046         1          1  0.000  0.815  0.185    0.7650    1  \n",
       "1190         1          1  0.000  0.751  0.249    0.8225    1  \n",
       "4071        -1         -1  0.143  0.787  0.070   -0.5423   -1  \n",
       "7066         0          0  0.000  1.000  0.000    0.0000    0  \n",
       "464          1          1  0.000  0.859  0.141    0.5574    1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_idx_list = [random.randint(1,len(df)) for i in range(10)] # creates random indexes to choose from dataframe\n",
    "df.loc[random_idx_list,:].head(10) # Returns the rows with the index and display it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have a nasty data in text. Because in general we use lot of punctuations and other words without any contextual meaning. It have no value as feature to the model we are training. So we need to get rid of them.\n",
    "\n",
    "# Text Preprocessing\n",
    "Tweet texts often consists of other user mentions, hyperlink texts, emoticons and punctuations. In order to use them for learning using a Language Model. We cannot permit those texts for training a model. So we have to clean the text data using various preprocessing and cleansing methods. Let's continue\n",
    "![Data Science Meme](https://miro.medium.com/max/800/1*Xhm9c9qDfXa3ZCQjiOvm_w.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming/ Lematization\n",
    "For grammatical reasons, documents are going to use different forms of a word, such as *write, writing and writes.* Additionally, there are families of derivationally related words with similar meanings. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "\n",
    "Stemming usually refers to a process that chops off the ends of words in the hope of achieving goal correctly most of the time and often includes the removal of derivational affixes. \n",
    "\n",
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base and dictionary form of a word\n",
    "![Stemming and Lematization](https://qph.fs.quoracdn.net/main-qimg-cd7f4bafaa42639deb999b1580bea69f)\n",
    "\n",
    "### Hyperlinks and Mentions\n",
    "Twitter is a social media platform where people can tag and mentions other people's ID and share videos and blogs from internet. So the tweets often contain lots of Hyperlinks and twitter mentions.\n",
    "\n",
    "- Twitter User Mentions - Eg. @arunrk7, @andrewng\n",
    "- Hyperlinks - Eg. https://keras.io, https://tensorflow.org\n",
    "\n",
    "### Stopwords\n",
    "Stopwords are commonly used words in English which have no contextual meaning in an sentence. So therefore we remove them before classification. Some stopwords are...\n",
    "![Stopwords English](https://4.bp.blogspot.com/-yiEr-jCVv38/Wmk10d84DYI/AAAAAAAAk0o/IfgjfjpgrxM5NosUQrGw7PtLvgr6DAG8ACLcBGAs/s1600/Screen%2BShot%2B2018-01-24%2Bat%2B5.41.21%2BPM.png)\n",
    "\n",
    "That looks like a tedious process, isn't?. Don't worry there is always some library in Python to do almost any work. The world is great!!!\n",
    "\n",
    "**NLTK** is a python library which got functions to perform text processing task for NLP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stem=False):\n",
    "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df.text = df['News'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aaww.. It is clean and tidy now. Now let's see some word cloud visualizations of it.**\n",
    "\n",
    "### Positive Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      index        Date                                               News  \\\n",
      "0         0  19 04 2013  PM directs early completion of projects in Bal...   \n",
      "3864   3864  19 12 2019  KMDC Alumni Conference to be held on December ...   \n",
      "3862   3862  19 12 2019  Millennial Olympiad begins at TMUC - The Mille...   \n",
      "3861   3861  19 12 2019  IoBM signs MoU with SWVL - The Institute of Bu...   \n",
      "3860   3860  19 12 2019  Bhandara wishes Christians Happy Christmas - T...   \n",
      "...     ...         ...                                                ...   \n",
      "1841   1841  19 10 2017  Japan trade surplus expands strongly in Septem...   \n",
      "8179   8179  09 11 2013  Allama Iqbal was free from prejudice: Altaf Hu...   \n",
      "8177   8177  10 09 2020  '48pc petrol pumps' measurement scale tempered...   \n",
      "8284   8284  16 02 2015  US port strike pushes up freight rates with sh...   \n",
      "8285   8285  07 12 2015  Brazil's November inflation seen at 12-year hi...   \n",
      "\n",
      "      Polarity  Sentiment    neg    neu    pos  compound  Cum  \n",
      "0            1          1  0.000  0.884  0.116    0.5267    1  \n",
      "3864         1          1  0.000  0.855  0.145    0.7906    1  \n",
      "3862         1          1  0.000  0.904  0.096    0.5106    1  \n",
      "3861         1          1  0.050  0.860  0.090    0.3919    1  \n",
      "3860         1          1  0.049  0.591  0.360    0.9422    1  \n",
      "...        ...        ...    ...    ...    ...       ...  ...  \n",
      "1841         1          1  0.000  0.868  0.132    0.3612    1  \n",
      "8179         0          0  0.104  0.791  0.104    0.0000    0  \n",
      "8177         0          0  0.000  1.000  0.000    0.0000    0  \n",
      "8284         0          0  0.088  0.912  0.000   -0.2500    0  \n",
      "8285         0          0  0.000  1.000  0.000    0.0000    0  \n",
      "\n",
      "[4387 rows x 10 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-08b3fecfeed3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mwc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1600\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m800\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwc\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'bilinear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \"\"\"\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \"\"\"\n\u001b[0;32m    613\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    503\u001b[0m                 result = occupancy.sample_position(box_size[1] + self.margin,\n\u001b[0;32m    504\u001b[0m                                                    \u001b[0mbox_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmargin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m                                                    random_state)\n\u001b[0m\u001b[0;32m    506\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfont_size\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_font_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m                     \u001b[1;31m# either we found a place or font-size went too small\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "plt.figure(figsize = (20,20)) \n",
    "temp = df.sort_values(by=['Polarity'], ascending = False)\n",
    "print(temp[0:count_pos+4])\n",
    "text = ' '.join(temp[\"News\"][0:count_pos])\n",
    "\n",
    "\n",
    "    \n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(text)\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neutral Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20)) \n",
    "temp = df.sort_values(by=['Polarity'], ascending = False)\n",
    "print(temp[count_pos:count_pos + count_ntl])\n",
    "text = ' '.join(temp[\"News\"][count_pos:count_pos + count_ntl])\n",
    "\n",
    "\n",
    "    \n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(text)\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20)) \n",
    "temp = df.sort_values(by=['Polarity'], ascending = False)\n",
    "print(temp[count_pos + count_ntl:])\n",
    "text = ' '.join(temp[\"News\"][count_pos + count_ntl:])\n",
    "\n",
    "\n",
    "    \n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(text)\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "MAX_NB_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data size: 8412\n",
      "Test Data size 2103\n",
      "      index        Date                                               News  \\\n",
      "8991   8991  19 08 2014  PTI, PAT protests: Will there be a people's pa...   \n",
      "270     270  29 05 2014  Super typhoon cools Philippine economy - MANIL...   \n",
      "4087   4087  13 02 2020  Gwadar's new master plan highlights factors be...   \n",
      "191     191  28 04 2014  LSE looses 10.59 points - LAHORE: Lahore Stock...   \n",
      "3613   3613  30 11 2019  Opposition parties to be taken on board in leg...   \n",
      "...     ...         ...                                                ...   \n",
      "2575   2575  31 12 2018  Saudi GDP growth speeds up in Q3, non-oil sect...   \n",
      "1095   1095  25 04 2016  Polish March jobless falls to 10.0pc, as expec...   \n",
      "660     660  02 02 2015  CM for passing benefits of decrease in POL pri...   \n",
      "7449   7449  01 08 2020  Colonialism, foreign occupation: Masood pays t...   \n",
      "1083   1083  20 04 2016  Burundi's inflation eases to 4.3pc in March - ...   \n",
      "\n",
      "      Polarity  Sentiment    neg    neu    pos  compound  Cum  \n",
      "8991         0          0  0.147  0.853  0.000   -0.2263    0  \n",
      "270          1          1  0.000  0.747  0.253    0.8316    1  \n",
      "4087         1          1  0.000  0.940  0.060    0.4019    1  \n",
      "191         -1         -1  0.167  0.833  0.000   -0.4939   -1  \n",
      "3613         1          1  0.000  0.819  0.181    0.6486    1  \n",
      "...        ...        ...    ...    ...    ...       ...  ...  \n",
      "2575         1          1  0.000  0.846  0.154    0.6249    1  \n",
      "1095        -1         -1  0.165  0.835  0.000   -0.5766   -1  \n",
      "660          1          1  0.000  0.839  0.161    0.6369    1  \n",
      "7449         0          0  0.000  1.000  0.000    0.0000    0  \n",
      "1083         1          1  0.057  0.783  0.160    0.4404    1  \n",
      "\n",
      "[2103 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=1-TRAIN_SIZE,\n",
    "                                         random_state=7) # Splits Dataset into Training and Testing set\n",
    "print(\"Train Data size:\", len(train_data))\n",
    "print(\"Test Data size\", len(test_data))\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_test_split` will shuffle the dataset and split it to gives training and testing dataset. It's important to shuffle our dataset before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8412 entries, 3192 to 9412\n",
      "Data columns (total 10 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   index      8412 non-null   int64  \n",
      " 1   Date       8412 non-null   object \n",
      " 2   News       8412 non-null   object \n",
      " 3   Polarity   8412 non-null   int64  \n",
      " 4   Sentiment  8412 non-null   int64  \n",
      " 5   neg        8412 non-null   float64\n",
      " 6   neu        8412 non-null   float64\n",
      " 7   pos        8412 non-null   float64\n",
      " 8   compound   8412 non-null   float64\n",
      " 9   Cum        8412 non-null   int64  \n",
      "dtypes: float64(4), int64(4), object(2)\n",
      "memory usage: 722.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called *tokens* , perhaps at the same time throwing away certain characters, such as punctuation. The process is called **Tokenization.**\n",
    "![Tokenization](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/11/tokenization.png)\n",
    "\n",
    "`tokenizer` create tokens for every word in the data corpus and map them to a index using dictionary.\n",
    "\n",
    "`word_index` contains the index for each word\n",
    "\n",
    "`vocab_size` represents the total number of word in the data corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size : 17774\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['News'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary Size :\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got a `tokenizer` object, which can be used to covert any word into a Key in dictionary (number).\n",
    "\n",
    "Since we are going to build a sequence model. We should feed in a sequence of numbers to it. And also we should ensure there is no variance in input shapes of sequences. It all should be of same lenght. But texts in tweets have different count of words in it. To avoid this, we seek a little help from `pad_sequence` to do our job. It will make all the sequence in one constant length `MAX_SEQUENCE_LENGTH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X Shape: (8412, 100)\n",
      "Testing X Shape: (2103, 100)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(train_data['News']),\n",
    "                        maxlen = MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(test_data['News']),\n",
    "                       maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Training X Shape:\",x_train.shape)\n",
    "print(\"Testing X Shape:\",x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding \n",
    "We are building the model to predict class in enocoded form (0 or 1 as this is a binary classification). We should encode our training labels to encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      index        Date                                               News  \\\n",
      "3192   3192  29 10 2019  Trade deal hopes drive European shares to 21-m...   \n",
      "5335   5335  28 07 2020  Efforts afoot to reduce educational gaps, prom...   \n",
      "733     733  01 04 2015  Italy sees more leeway for growth measures in ...   \n",
      "1417   1417  19 12 2016  CET power line set to energize Pakistan's Sind...   \n",
      "6999   6999  14 09 2020             Senior PTI leader shot dead in Haripur   \n",
      "1900   1900  23 11 2017  RECORDER REPORT: PSX, BRIndex-30 update - KARA...   \n",
      "1497   1497  03 03 2017  Govt allocates Rs27.96bn subsidy on fertilizer...   \n",
      "1066   1066  13 04 2016  PNSC, NIB Bank renew loan agreement - KARACHI:...   \n",
      "7589   7589  11 10 2016  In pictures: Muslims across the world commemor...   \n",
      "6304   6304  09 12 2017  Suspects behind recent child kidnapping in Kar...   \n",
      "\n",
      "      Polarity  Sentiment    neg    neu    pos  compound  Cum  \n",
      "3192         1          1  0.033  0.690  0.277    0.9274    1  \n",
      "5335         1          1  0.000  0.927  0.073    0.3818    1  \n",
      "733          1          1  0.063  0.686  0.251    0.7624    1  \n",
      "1417         1          1  0.000  0.933  0.067    0.4767    1  \n",
      "6999        -1         -1  0.417  0.583  0.000   -0.6486   -1  \n",
      "1900        -1         -1  0.139  0.861  0.000   -0.5719   -1  \n",
      "1497         1          1  0.000  0.921  0.079    0.3400    1  \n",
      "1066         1          1  0.000  0.758  0.242    0.7506    1  \n",
      "7589         0          0  0.000  1.000  0.000    0.0000    0  \n",
      "6304        -1         -1  0.434  0.566  0.000   -0.7845   -1  \n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "train_labels = to_categorical(train_data['Polarity'], num_classes=3)\n",
    "test_labels = to_categorical(test_data['Polarity'], num_classes=3)\n",
    "#labels = np.argmax(labels, axis=1)\n",
    "print(train_data[:10])\n",
    "print(train_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (8412, 3)\n",
      "y_test shape: (2103, 3)\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_train = train_labels\n",
    "y_test = test_labels\n",
    "#y_train = y_train.reshape(-1,1)\n",
    "#y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Emdedding\n",
    "In Language Model, words are represented in a way to intend more meaning and for learning the patterns and contextual meaning behind it. \n",
    "\n",
    "**Word Embedding** is one of the popular representation of document vocabulary.It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n",
    "\n",
    "Basically, it's a feature vector representation of words which are used for other natural language processing applications.\n",
    "\n",
    "We could train the embedding ourselves but that would take a while to train and it wouldn't be effective. So going in the path of Computer Vision, here we use **Transfer Learning**. We download the pre-trained embedding and use it in our model.\n",
    "\n",
    "The pretrained Word Embedding like **GloVe & Word2Vec** gives more insights for a word which can be used for classification. If you want to learn more about the Word Embedding, please refer some links that I left at the end of this notebook.\n",
    "\n",
    "\n",
    "In this notebook, I use **GloVe Embedding from Stanford AI** which can be found [here](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip glove.6B.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_EMB = 'glove.6B.300d.txt'\n",
    "EMBEDDING_DIM = 300\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "MODEL_PATH = 'best_model.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "f = open(GLOVE_EMB, encoding=\"utf8\")\n",
    "for line in f:\n",
    "  values = line.split()\n",
    "  word = value = values[0]\n",
    "  coefs = np.asarray(values[1:], dtype='float32')\n",
    "  embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' %len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "  embedding_vector = embeddings_index.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
    "                                          EMBEDDING_DIM,\n",
    "                                          weights=[embedding_matrix],\n",
    "                                          input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                          trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training - LSTM\n",
    "We are clear to build our Deep Learning model. While developing a DL model, we should keep in mind of key things like Model Architecture, Hyperparmeter Tuning and Performance of the model.\n",
    "\n",
    "As you can see in the word cloud, the some words are predominantly feature in both Positive and Negative tweets. This could be a problem if we are using a Machine Learning model like Naive Bayes, SVD, etc.. That's why we use **Sequence Models**.\n",
    "\n",
    "### Sequence Model\n",
    "![Sequence Model](https://miro.medium.com/max/1458/1*SICYykT7ybua1gVJDNlajw.png)\n",
    "\n",
    "Reccurent Neural Networks can handle a seqence of data and learn a pattern of input seqence to give either sequence or scalar value as output. In our case, the Neural Network outputs a scalar value prediction. \n",
    "\n",
    "For model architecture, we use\n",
    "\n",
    "1) **Embedding Layer** - Generates Embedding Vector for each input sequence.\n",
    "\n",
    "2) **Conv1D Layer** - Its using to convolve data into smaller feature vectors. \n",
    "\n",
    "3) **LSTM** - Long Short Term Memory, its a variant of RNN which has memory state cell to learn the context of words which are at further along the text to carry contextual meaning rather than just neighbouring words as in case of RNN.\n",
    "\n",
    "4) **Dense** - Fully Connected Layers for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((8412, 100), (8412, 3), (2103, 100), (2103, 3))\n",
      "WARNING:tensorflow:From C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Tensor(\"dense_2/Sigmoid:0\", shape=(?, 3), dtype=float32)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 300)          5332200   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 96, 64)            96064     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 5,750,571\n",
      "Trainable params: 418,371\n",
      "Non-trainable params: 5,332,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print((x_train.shape, y_train.shape, x_test.shape, y_test.shape))\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedding_sequences = embedding_layer(sequence_input)\n",
    "#x = Sequential()\n",
    "x = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "x = Conv1D(64, 5)(x)\n",
    "x = Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "outputs = Dense(3, activation='sigmoid')(x)\n",
    "print(outputs)\n",
    "model = tf.keras.Model(sequence_input, outputs)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Algorithm\n",
    "This notebook uses Adam, optimization algorithm for Gradient Descent. You can learn more about Adam [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n",
    "\n",
    "### Callbacks\n",
    "Callbacks are special functions which are called at the end of an epoch. We can use any functions to perform specific operation after each epoch. I used two callbacks here,\n",
    "\n",
    "- **LRScheduler** - It changes a Learning Rate at specfic epoch to achieve more improved result. In this notebook, the learning rate exponentionally decreases after remaining same for first 10 Epoch.\n",
    "\n",
    "- **ModelCheckPoint** - It saves best model while training based on some metrics. Here, it saves the model with minimum Validity Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "ReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n",
    "                                     min_lr = 0.01,\n",
    "                                     monitor = 'val_loss',\n",
    "                                     verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start training... It takes a heck of a time if training in CPU, be sure your GPU turned on... May the CUDA Cores be with you...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training on GPU...\") if tf.test.is_gpu_available() else print(\"Training on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8412 samples, validate on 2103 samples\n",
      "Epoch 1/10\n",
      "8412/8412 [==============================] - 50s 6ms/sample - loss: 0.5462 - acc: 0.7235 - val_loss: 0.4539 - val_acc: 0.7863\n",
      "Epoch 2/10\n",
      "8412/8412 [==============================] - 47s 6ms/sample - loss: 0.4523 - acc: 0.7888 - val_loss: 0.4388 - val_acc: 0.8035\n",
      "Epoch 3/10\n",
      "8412/8412 [==============================] - 49s 6ms/sample - loss: 0.4203 - acc: 0.8084 - val_loss: 0.4199 - val_acc: 0.8092\n",
      "Epoch 4/10\n",
      "8412/8412 [==============================] - 48s 6ms/sample - loss: 0.3837 - acc: 0.8299 - val_loss: 0.4114 - val_acc: 0.8193\n",
      "Epoch 5/10\n",
      "8412/8412 [==============================] - 49s 6ms/sample - loss: 0.3646 - acc: 0.8386 - val_loss: 0.4209 - val_acc: 0.8206\n",
      "Epoch 6/10\n",
      "8412/8412 [==============================] - 52s 6ms/sample - loss: 0.3417 - acc: 0.8496 - val_loss: 0.4234 - val_acc: 0.8212\n",
      "Epoch 7/10\n",
      "8412/8412 [==============================] - 51s 6ms/sample - loss: 0.3139 - acc: 0.8617 - val_loss: 0.4328 - val_acc: 0.8195\n",
      "Epoch 8/10\n",
      "8412/8412 [==============================] - 52s 6ms/sample - loss: 0.2998 - acc: 0.8716 - val_loss: 0.4563 - val_acc: 0.8152\n",
      "Epoch 9/10\n",
      "8412/8412 [==============================] - 51s 6ms/sample - loss: 0.2861 - acc: 0.8785 - val_loss: 0.4380 - val_acc: 0.8090\n",
      "Epoch 10/10\n",
      "8412/8412 [==============================] - 48s 6ms/sample - loss: 0.2804 - acc: 0.8759 - val_loss: 0.4334 - val_acc: 0.8174\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Now that we have trained the model, we can evaluate its performance. We will some evaluation metrics and techniques to test the model.\n",
    "\n",
    "Let's start with the Learning Curve of loss and accuracy of the model on each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12ab2dfe088>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s, (at, al) = plt.subplots(2,1)\n",
    "at.plot(history.history['acc'], c= 'b')\n",
    "at.plot(history.history['val_acc'], c='r')\n",
    "at.set_title('model accuracy')\n",
    "at.set_ylabel('accuracy')\n",
    "at.set_xlabel('epoch')\n",
    "at.legend(['LSTM_train', 'LSTM_val'], loc='upper left')\n",
    "\n",
    "al.plot(history.history['loss'], c='m')\n",
    "al.plot(history.history['val_loss'], c='c')\n",
    "al.set_title('model loss')\n",
    "al.set_ylabel('loss')\n",
    "al.set_xlabel('epoch')\n",
    "al.legend(['train', 'val'], loc = 'upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will output a prediction score between 0 and 1. We can classify two classes by defining a threshold value for it. In our case, I have set 0.5 as THRESHOLD value, if the score above it. Then it will be classified as **POSITIVE** sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2103/2103 [==============================] - 8s 4ms/sample\n",
      "[[0.3653754  0.508828   0.14803734]\n",
      " [0.21849456 0.7244437  0.06948945]\n",
      " [0.20001486 0.77167296 0.01593834]\n",
      " ...\n",
      " [0.00723511 0.99208343 0.00135356]\n",
      " [0.46056703 0.47778586 0.04029682]\n",
      " [0.24568105 0.02614996 0.7211149 ]]\n",
      "0.508828 1\n",
      "---\n",
      "0.7244437 1\n",
      "---\n",
      "0.77167296 1\n",
      "---\n",
      "0.95879984 -1\n",
      "---\n",
      "0.92849123 1\n",
      "---\n",
      "0.61699903 -1\n",
      "---\n",
      "0.7104194 0\n",
      "---\n",
      "0.7072658 0\n",
      "---\n",
      "0.87985337 1\n",
      "---\n",
      "0.68470997 -1\n",
      "---\n",
      "0.91408324 -1\n",
      "---\n",
      "0.6181692 1\n",
      "---\n",
      "0.52552813 1\n",
      "---\n",
      "0.67127323 1\n",
      "---\n",
      "0.87615955 0\n",
      "---\n",
      "0.66042876 0\n",
      "---\n",
      "0.7411982 -1\n",
      "---\n",
      "0.97231865 0\n",
      "---\n",
      "0.96444625 1\n",
      "---\n",
      "0.915461 1\n",
      "---\n",
      "0.5065943 0\n",
      "---\n",
      "0.7792466 1\n",
      "---\n",
      "0.8939904 0\n",
      "---\n",
      "0.7361847 0\n",
      "---\n",
      "0.8595226 1\n",
      "---\n",
      "0.97597945 1\n",
      "---\n",
      "0.9224732 -1\n",
      "---\n",
      "0.87527764 0\n",
      "---\n",
      "0.582941 1\n",
      "---\n",
      "0.93166065 1\n",
      "---\n",
      "0.7385917 -1\n",
      "---\n",
      "0.42612743 1\n",
      "---\n",
      "0.91645885 -1\n",
      "---\n",
      "0.82359624 1\n",
      "---\n",
      "0.9576527 1\n",
      "---\n",
      "0.749525 -1\n",
      "---\n",
      "0.78981483 0\n",
      "---\n",
      "0.9708192 -1\n",
      "---\n",
      "0.6888772 -1\n",
      "---\n",
      "0.77464944 1\n",
      "---\n",
      "0.67989266 1\n",
      "---\n",
      "0.72776484 0\n",
      "---\n",
      "0.98383045 1\n",
      "---\n",
      "0.60810024 0\n",
      "---\n",
      "0.9925178 1\n",
      "---\n",
      "0.5937671 0\n",
      "---\n",
      "0.817315 1\n",
      "---\n",
      "0.868162 1\n",
      "---\n",
      "0.5047383 1\n",
      "---\n",
      "0.9966125 -1\n",
      "---\n",
      "0.5545695 1\n",
      "---\n",
      "0.5227249 0\n",
      "---\n",
      "0.6893671 0\n",
      "---\n",
      "0.9358318 1\n",
      "---\n",
      "0.984378 1\n",
      "---\n",
      "0.5619853 1\n",
      "---\n",
      "0.9785116 -1\n",
      "---\n",
      "0.9508477 -1\n",
      "---\n",
      "0.4563269 -1\n",
      "---\n",
      "0.9997266 1\n",
      "---\n",
      "0.74484605 0\n",
      "---\n",
      "0.53165185 0\n",
      "---\n",
      "0.83594 0\n",
      "---\n",
      "0.5860796 0\n",
      "---\n",
      "0.6081106 -1\n",
      "---\n",
      "0.6480381 0\n",
      "---\n",
      "0.8225225 0\n",
      "---\n",
      "0.88752496 -1\n",
      "---\n",
      "0.89769244 1\n",
      "---\n",
      "0.41131207 0\n",
      "---\n",
      "0.7459626 1\n",
      "---\n",
      "0.9605497 1\n",
      "---\n",
      "0.59928644 0\n",
      "---\n",
      "0.8722866 -1\n",
      "---\n",
      "0.73600197 -1\n",
      "---\n",
      "0.39569193 0\n",
      "---\n",
      "0.9755943 -1\n",
      "---\n",
      "0.4155411 1\n",
      "---\n",
      "0.7408721 -1\n",
      "---\n",
      "0.80538476 1\n",
      "---\n",
      "0.6874282 1\n",
      "---\n",
      "0.42813864 0\n",
      "---\n",
      "0.4993968 1\n",
      "---\n",
      "0.9116696 0\n",
      "---\n",
      "0.61200786 -1\n",
      "---\n",
      "0.6039549 0\n",
      "---\n",
      "0.9631173 1\n",
      "---\n",
      "0.8358563 -1\n",
      "---\n",
      "0.9902464 1\n",
      "---\n",
      "0.45277804 1\n",
      "---\n",
      "0.9970803 0\n",
      "---\n",
      "0.9424475 -1\n",
      "---\n",
      "0.99682 1\n",
      "---\n",
      "0.3933413 1\n",
      "---\n",
      "0.6333448 1\n",
      "---\n",
      "0.6542511 0\n",
      "---\n",
      "0.99969053 1\n",
      "---\n",
      "0.9995084 1\n",
      "---\n",
      "0.99638855 -1\n",
      "---\n",
      "0.95635897 -1\n",
      "---\n",
      "0.83856106 0\n",
      "---\n",
      "0.5455758 0\n",
      "---\n",
      "0.8523031 0\n",
      "---\n",
      "0.95549035 0\n",
      "---\n",
      "0.7798747 0\n",
      "---\n",
      "0.98015803 0\n",
      "---\n",
      "0.9311079 0\n",
      "---\n",
      "0.88817745 0\n",
      "---\n",
      "0.7421123 0\n",
      "---\n",
      "0.71349406 1\n",
      "---\n",
      "0.7337025 1\n",
      "---\n",
      "0.96148735 0\n",
      "---\n",
      "0.82623243 -1\n",
      "---\n",
      "0.99121356 1\n",
      "---\n",
      "0.71891665 1\n",
      "---\n",
      "0.7110954 1\n",
      "---\n",
      "0.5623367 0\n",
      "---\n",
      "0.9686575 0\n",
      "---\n",
      "0.6794012 0\n",
      "---\n",
      "0.50779253 -1\n",
      "---\n",
      "0.9847976 -1\n",
      "---\n",
      "0.901504 1\n",
      "---\n",
      "0.7348553 1\n",
      "---\n",
      "0.49600205 0\n",
      "---\n",
      "0.7942835 1\n",
      "---\n",
      "0.9982636 1\n",
      "---\n",
      "0.7555818 0\n",
      "---\n",
      "0.54159933 1\n",
      "---\n",
      "0.95006436 1\n",
      "---\n",
      "0.936033 1\n",
      "---\n",
      "0.77773404 1\n",
      "---\n",
      "0.8083087 1\n",
      "---\n",
      "0.4203473 -1\n",
      "---\n",
      "0.65607536 -1\n",
      "---\n",
      "0.58339334 -1\n",
      "---\n",
      "0.9177973 0\n",
      "---\n",
      "0.9797194 1\n",
      "---\n",
      "0.6594044 0\n",
      "---\n",
      "0.9159362 0\n",
      "---\n",
      "0.8267766 0\n",
      "---\n",
      "0.5341542 1\n",
      "---\n",
      "0.8296941 0\n",
      "---\n",
      "0.55560976 -1\n",
      "---\n",
      "0.9362463 -1\n",
      "---\n",
      "0.94090825 1\n",
      "---\n",
      "0.98803896 -1\n",
      "---\n",
      "0.9024056 0\n",
      "---\n",
      "0.5380597 1\n",
      "---\n",
      "0.66282344 0\n",
      "---\n",
      "0.9495436 1\n",
      "---\n",
      "0.6496156 0\n",
      "---\n",
      "0.86969584 1\n",
      "---\n",
      "0.979165 -1\n",
      "---\n",
      "0.5671633 0\n",
      "---\n",
      "0.9971391 0\n",
      "---\n",
      "0.94007987 0\n",
      "---\n",
      "0.7212459 1\n",
      "---\n",
      "0.53224826 -1\n",
      "---\n",
      "0.6439003 1\n",
      "---\n",
      "0.96998525 1\n",
      "---\n",
      "0.79767275 -1\n",
      "---\n",
      "0.8764584 -1\n",
      "---\n",
      "0.99044836 1\n",
      "---\n",
      "0.66344184 1\n",
      "---\n",
      "0.9408761 1\n",
      "---\n",
      "0.98225635 1\n",
      "---\n",
      "0.51812375 0\n",
      "---\n",
      "0.67129844 -1\n",
      "---\n",
      "0.8039026 1\n",
      "---\n",
      "0.6061934 1\n",
      "---\n",
      "0.5605958 1\n",
      "---\n",
      "0.97923595 1\n",
      "---\n",
      "0.6423794 0\n",
      "---\n",
      "0.59145796 0\n",
      "---\n",
      "0.6348537 -1\n",
      "---\n",
      "0.8523811 0\n",
      "---\n",
      "0.5839234 1\n",
      "---\n",
      "0.9266937 1\n",
      "---\n",
      "0.5242648 1\n",
      "---\n",
      "0.8875514 0\n",
      "---\n",
      "0.92680097 1\n",
      "---\n",
      "0.9951352 0\n",
      "---\n",
      "0.8264568 -1\n",
      "---\n",
      "0.99986994 1\n",
      "---\n",
      "0.9979187 1\n",
      "---\n",
      "0.5728836 1\n",
      "---\n",
      "0.46784347 1\n",
      "---\n",
      "0.99472195 0\n",
      "---\n",
      "0.7243546 1\n",
      "---\n",
      "0.8516331 0\n",
      "---\n",
      "0.9570266 -1\n",
      "---\n",
      "0.68474996 1\n",
      "---\n",
      "0.97651327 1\n",
      "---\n",
      "0.84644425 0\n",
      "---\n",
      "0.68491226 0\n",
      "---\n",
      "0.8956499 1\n",
      "---\n",
      "0.9785181 1\n",
      "---\n",
      "0.964295 0\n",
      "---\n",
      "0.8295119 0\n",
      "---\n",
      "0.9964007 1\n",
      "---\n",
      "0.945192 1\n",
      "---\n",
      "0.75794196 1\n",
      "---\n",
      "0.9873183 1\n",
      "---\n",
      "0.83299816 0\n",
      "---\n",
      "0.8358058 1\n",
      "---\n",
      "0.97119194 1\n",
      "---\n",
      "0.5405766 1\n",
      "---\n",
      "0.720314 1\n",
      "---\n",
      "0.99682426 0\n",
      "---\n",
      "0.9874607 1\n",
      "---\n",
      "0.6125847 0\n",
      "---\n",
      "0.9744755 1\n",
      "---\n",
      "0.790213 -1\n",
      "---\n",
      "0.8779609 -1\n",
      "---\n",
      "0.6096226 -1\n",
      "---\n",
      "0.47180915 1\n",
      "---\n",
      "0.98549306 1\n",
      "---\n",
      "0.4936779 1\n",
      "---\n",
      "0.9620454 -1\n",
      "---\n",
      "0.99418795 1\n",
      "---\n",
      "0.97903883 0\n",
      "---\n",
      "0.47385854 0\n",
      "---\n",
      "0.8318263 -1\n",
      "---\n",
      "0.6303282 0\n",
      "---\n",
      "0.7126374 0\n",
      "---\n",
      "0.58895046 1\n",
      "---\n",
      "0.9951018 -1\n",
      "---\n",
      "0.90122616 -1\n",
      "---\n",
      "0.74291646 0\n",
      "---\n",
      "0.5343666 0\n",
      "---\n",
      "0.4444268 1\n",
      "---\n",
      "0.6460968 1\n",
      "---\n",
      "0.9018157 1\n",
      "---\n",
      "0.44420105 0\n",
      "---\n",
      "0.65248656 1\n",
      "---\n",
      "0.9128655 1\n",
      "---\n",
      "0.9310114 0\n",
      "---\n",
      "0.9038715 -1\n",
      "---\n",
      "0.7713833 -1\n",
      "---\n",
      "0.65860003 1\n",
      "---\n",
      "0.7191129 0\n",
      "---\n",
      "0.70831627 0\n",
      "---\n",
      "0.9498515 1\n",
      "---\n",
      "0.9541332 -1\n",
      "---\n",
      "0.9946839 1\n",
      "---\n",
      "0.94314015 1\n",
      "---\n",
      "0.6089083 0\n",
      "---\n",
      "0.69355625 1\n",
      "---\n",
      "0.9655695 1\n",
      "---\n",
      "0.7837407 0\n",
      "---\n",
      "0.9690425 -1\n",
      "---\n",
      "0.96440136 1\n",
      "---\n",
      "0.6098932 -1\n",
      "---\n",
      "0.8462264 -1\n",
      "---\n",
      "0.9690888 1\n",
      "---\n",
      "0.8833835 -1\n",
      "---\n",
      "0.6395891 0\n",
      "---\n",
      "0.9044702 -1\n",
      "---\n",
      "0.95621574 0\n",
      "---\n",
      "0.978783 1\n",
      "---\n",
      "0.86316633 -1\n",
      "---\n",
      "0.96196604 0\n",
      "---\n",
      "0.7516083 1\n",
      "---\n",
      "0.39268488 1\n",
      "---\n",
      "0.9618783 1\n",
      "---\n",
      "0.4947423 0\n",
      "---\n",
      "0.7738228 -1\n",
      "---\n",
      "0.9951089 1\n",
      "---\n",
      "0.98926836 1\n",
      "---\n",
      "0.8933028 1\n",
      "---\n",
      "0.41756842 1\n",
      "---\n",
      "0.6298572 1\n",
      "---\n",
      "0.5468091 -1\n",
      "---\n",
      "0.5746453 1\n",
      "---\n",
      "0.8854779 0\n",
      "---\n",
      "0.9981464 1\n",
      "---\n",
      "0.7111033 0\n",
      "---\n",
      "0.8823771 1\n",
      "---\n",
      "0.9220831 -1\n",
      "---\n",
      "0.8201212 0\n",
      "---\n",
      "0.8959776 0\n",
      "---\n",
      "0.9889181 -1\n",
      "---\n",
      "0.9982327 -1\n",
      "---\n",
      "0.828058 1\n",
      "---\n",
      "0.931507 0\n",
      "---\n",
      "0.8285008 -1\n",
      "---\n",
      "0.5867755 1\n",
      "---\n",
      "0.99772835 0\n",
      "---\n",
      "0.99303806 1\n",
      "---\n",
      "0.99569714 1\n",
      "---\n",
      "0.599245 -1\n",
      "---\n",
      "0.6444839 0\n",
      "---\n",
      "0.9847043 -1\n",
      "---\n",
      "0.6021533 1\n",
      "---\n",
      "0.43531048 -1\n",
      "---\n",
      "0.98324037 1\n",
      "---\n",
      "0.52679956 0\n",
      "---\n",
      "0.7603426 0\n",
      "---\n",
      "0.9674823 -1\n",
      "---\n",
      "0.99682236 1\n",
      "---\n",
      "0.9716326 1\n",
      "---\n",
      "0.85686874 0\n",
      "---\n",
      "0.46715823 1\n",
      "---\n",
      "0.99605846 0\n",
      "---\n",
      "0.67351747 0\n",
      "---\n",
      "0.7159869 -1\n",
      "---\n",
      "0.77688825 1\n",
      "---\n",
      "0.93453306 -1\n",
      "---\n",
      "0.5334461 1\n",
      "---\n",
      "0.4544227 1\n",
      "---\n",
      "0.8718804 0\n",
      "---\n",
      "0.9927958 -1\n",
      "---\n",
      "0.75176877 -1\n",
      "---\n",
      "0.59227717 1\n",
      "---\n",
      "0.97032344 1\n",
      "---\n",
      "0.4986453 -1\n",
      "---\n",
      "0.69541526 1\n",
      "---\n",
      "0.8748212 -1\n",
      "---\n",
      "0.43485004 -1\n",
      "---\n",
      "0.66445166 1\n",
      "---\n",
      "0.5909377 -1\n",
      "---\n",
      "0.96114576 1\n",
      "---\n",
      "0.95250463 0\n",
      "---\n",
      "0.7147083 -1\n",
      "---\n",
      "0.9147085 1\n",
      "---\n",
      "0.8393406 1\n",
      "---\n",
      "0.61426455 0\n",
      "---\n",
      "0.9644881 -1\n",
      "---\n",
      "0.99600434 1\n",
      "---\n",
      "0.49353394 1\n",
      "---\n",
      "0.9933405 -1\n",
      "---\n",
      "0.8897828 -1\n",
      "---\n",
      "0.5953771 1\n",
      "---\n",
      "0.5329445 1\n",
      "---\n",
      "0.7050218 0\n",
      "---\n",
      "0.64459616 -1\n",
      "---\n",
      "0.45422864 0\n",
      "---\n",
      "0.8837316 1\n",
      "---\n",
      "0.99656063 -1\n",
      "---\n",
      "0.9942689 1\n",
      "---\n",
      "0.98773944 -1\n",
      "---\n",
      "0.7878775 1\n",
      "---\n",
      "0.8384871 1\n",
      "---\n",
      "0.42128637 0\n",
      "---\n",
      "0.5855786 1\n",
      "---\n",
      "0.9343736 -1\n",
      "---\n",
      "0.95694196 0\n",
      "---\n",
      "0.811786 1\n",
      "---\n",
      "0.59113264 1\n",
      "---\n",
      "0.5063287 0\n",
      "---\n",
      "0.8802179 -1\n",
      "---\n",
      "0.8272114 0\n",
      "---\n",
      "0.91935694 1\n",
      "---\n",
      "0.95212215 0\n",
      "---\n",
      "0.99458665 1\n",
      "---\n",
      "0.99819434 1\n",
      "---\n",
      "0.8597269 -1\n",
      "---\n",
      "0.63377297 1\n",
      "---\n",
      "0.92029107 0\n",
      "---\n",
      "0.9770627 1\n",
      "---\n",
      "0.99385935 0\n",
      "---\n",
      "0.9506036 0\n",
      "---\n",
      "0.503677 1\n",
      "---\n",
      "0.9948317 1\n",
      "---\n",
      "0.9524714 -1\n",
      "---\n",
      "0.9988245 1\n",
      "---\n",
      "0.9988368 1\n",
      "---\n",
      "0.9356117 1\n",
      "---\n",
      "0.9501296 1\n",
      "---\n",
      "0.9997429 1\n",
      "---\n",
      "0.91297144 1\n",
      "---\n",
      "0.7477186 1\n",
      "---\n",
      "0.6583097 -1\n",
      "---\n",
      "0.99947315 1\n",
      "---\n",
      "0.4818644 1\n",
      "---\n",
      "0.8898508 1\n",
      "---\n",
      "0.63668936 0\n",
      "---\n",
      "0.8880289 -1\n",
      "---\n",
      "0.8852561 0\n",
      "---\n",
      "0.815722 0\n",
      "---\n",
      "0.9450656 0\n",
      "---\n",
      "0.84182054 1\n",
      "---\n",
      "0.45903045 0\n",
      "---\n",
      "0.81228405 -1\n",
      "---\n",
      "0.9695211 1\n",
      "---\n",
      "0.9343029 0\n",
      "---\n",
      "0.57110405 0\n",
      "---\n",
      "0.953669 -1\n",
      "---\n",
      "0.6118445 1\n",
      "---\n",
      "0.79650956 1\n",
      "---\n",
      "0.9650554 1\n",
      "---\n",
      "0.9298377 1\n",
      "---\n",
      "0.9754624 -1\n",
      "---\n",
      "0.99295354 1\n",
      "---\n",
      "0.92212415 0\n",
      "---\n",
      "0.49679688 0\n",
      "---\n",
      "0.90147305 -1\n",
      "---\n",
      "0.9013089 0\n",
      "---\n",
      "0.85229766 0\n",
      "---\n",
      "0.8236809 0\n",
      "---\n",
      "0.8232641 1\n",
      "---\n",
      "0.84303355 0\n",
      "---\n",
      "0.9292791 -1\n",
      "---\n",
      "0.9987292 1\n",
      "---\n",
      "0.5095281 0\n",
      "---\n",
      "0.9634542 -1\n",
      "---\n",
      "0.9583032 0\n",
      "---\n",
      "0.66441584 0\n",
      "---\n",
      "0.57096756 0\n",
      "---\n",
      "0.97865295 -1\n",
      "---\n",
      "0.5148511 1\n",
      "---\n",
      "0.46396112 0\n",
      "---\n",
      "0.7325113 0\n",
      "---\n",
      "0.47270602 1\n",
      "---\n",
      "0.99847615 1\n",
      "---\n",
      "0.92967224 0\n",
      "---\n",
      "0.8464962 1\n",
      "---\n",
      "0.6499391 1\n",
      "---\n",
      "0.8775754 1\n",
      "---\n",
      "0.5138328 1\n",
      "---\n",
      "0.9919641 0\n",
      "---\n",
      "0.6552259 0\n",
      "---\n",
      "0.8701477 1\n",
      "---\n",
      "0.91121435 1\n",
      "---\n",
      "0.83832765 1\n",
      "---\n",
      "0.67094487 1\n",
      "---\n",
      "0.99855566 -1\n",
      "---\n",
      "0.74662924 0\n",
      "---\n",
      "0.71396524 0\n",
      "---\n",
      "0.65302837 0\n",
      "---\n",
      "0.6277392 0\n",
      "---\n",
      "0.87676144 1\n",
      "---\n",
      "0.98000634 1\n",
      "---\n",
      "0.98296523 0\n",
      "---\n",
      "0.99634296 -1\n",
      "---\n",
      "0.4560217 1\n",
      "---\n",
      "0.5578379 0\n",
      "---\n",
      "0.9991152 1\n",
      "---\n",
      "0.84413517 -1\n",
      "---\n",
      "0.6894697 1\n",
      "---\n",
      "0.9140283 1\n",
      "---\n",
      "0.9596572 0\n",
      "---\n",
      "0.9394873 0\n",
      "---\n",
      "0.6593696 0\n",
      "---\n",
      "0.9208088 1\n",
      "---\n",
      "0.7566068 1\n",
      "---\n",
      "0.76316005 -1\n",
      "---\n",
      "0.6201512 -1\n",
      "---\n",
      "0.708606 0\n",
      "---\n",
      "0.8048287 0\n",
      "---\n",
      "0.9038713 1\n",
      "---\n",
      "0.9743805 0\n",
      "---\n",
      "0.7891879 1\n",
      "---\n",
      "0.45494115 -1\n",
      "---\n",
      "0.9781375 1\n",
      "---\n",
      "0.987343 1\n",
      "---\n",
      "0.60299885 -1\n",
      "---\n",
      "0.7463951 1\n",
      "---\n",
      "0.8410665 -1\n",
      "---\n",
      "0.9759025 1\n",
      "---\n",
      "0.56564337 0\n",
      "---\n",
      "0.9334825 0\n",
      "---\n",
      "0.9727349 0\n",
      "---\n",
      "0.76224804 -1\n",
      "---\n",
      "0.9236382 1\n",
      "---\n",
      "0.5343576 0\n",
      "---\n",
      "0.8410073 -1\n",
      "---\n",
      "0.9557718 1\n",
      "---\n",
      "0.9540057 1\n",
      "---\n",
      "0.98519033 -1\n",
      "---\n",
      "0.9972968 1\n",
      "---\n",
      "0.83021104 1\n",
      "---\n",
      "0.7821015 -1\n",
      "---\n",
      "0.93138045 1\n",
      "---\n",
      "0.6643322 -1\n",
      "---\n",
      "0.9950875 0\n",
      "---\n",
      "0.82639396 0\n",
      "---\n",
      "0.7240505 -1\n",
      "---\n",
      "0.9746506 0\n",
      "---\n",
      "0.67706066 1\n",
      "---\n",
      "0.94899064 0\n",
      "---\n",
      "0.9711271 -1\n",
      "---\n",
      "0.8940361 1\n",
      "---\n",
      "0.95651186 0\n",
      "---\n",
      "0.9926166 -1\n",
      "---\n",
      "0.8074218 0\n",
      "---\n",
      "0.9802828 1\n",
      "---\n",
      "0.79966176 1\n",
      "---\n",
      "0.6485385 0\n",
      "---\n",
      "0.9643989 -1\n",
      "---\n",
      "0.96926665 -1\n",
      "---\n",
      "0.9610029 -1\n",
      "---\n",
      "0.95493454 1\n",
      "---\n",
      "0.9588176 -1\n",
      "---\n",
      "0.68309724 1\n",
      "---\n",
      "0.73521185 0\n",
      "---\n",
      "0.59050626 1\n",
      "---\n",
      "0.9081371 -1\n",
      "---\n",
      "0.9945533 -1\n",
      "---\n",
      "0.9315154 -1\n",
      "---\n",
      "0.4711337 -1\n",
      "---\n",
      "0.63471705 0\n",
      "---\n",
      "0.5362853 -1\n",
      "---\n",
      "0.89225924 0\n",
      "---\n",
      "0.98328364 -1\n",
      "---\n",
      "0.9768497 1\n",
      "---\n",
      "0.4809391 0\n",
      "---\n",
      "0.97704715 1\n",
      "---\n",
      "0.9929118 -1\n",
      "---\n",
      "0.7539006 1\n",
      "---\n",
      "0.9476244 -1\n",
      "---\n",
      "0.54266125 1\n",
      "---\n",
      "0.628226 1\n",
      "---\n",
      "0.64071393 1\n",
      "---\n",
      "0.537689 0\n",
      "---\n",
      "0.8429012 0\n",
      "---\n",
      "0.98281395 1\n",
      "---\n",
      "0.6401942 1\n",
      "---\n",
      "0.99947786 1\n",
      "---\n",
      "0.5822613 0\n",
      "---\n",
      "0.69706446 -1\n",
      "---\n",
      "0.72207254 1\n",
      "---\n",
      "0.8364525 1\n",
      "---\n",
      "0.92727304 1\n",
      "---\n",
      "0.4933582 1\n",
      "---\n",
      "0.6722385 0\n",
      "---\n",
      "0.8230268 -1\n",
      "---\n",
      "0.9689142 -1\n",
      "---\n",
      "0.84653616 0\n",
      "---\n",
      "0.94583696 -1\n",
      "---\n",
      "0.73888123 0\n",
      "---\n",
      "0.83319974 1\n",
      "---\n",
      "0.6668794 0\n",
      "---\n",
      "0.96549344 -1\n",
      "---\n",
      "0.63952756 1\n",
      "---\n",
      "0.9856318 1\n",
      "---\n",
      "0.998196 1\n",
      "---\n",
      "0.85161316 0\n",
      "---\n",
      "0.6437398 1\n",
      "---\n",
      "0.8082667 1\n",
      "---\n",
      "0.87576926 0\n",
      "---\n",
      "0.9936086 -1\n",
      "---\n",
      "0.90318143 0\n",
      "---\n",
      "0.9658292 0\n",
      "---\n",
      "0.80667347 1\n",
      "---\n",
      "0.99132097 1\n",
      "---\n",
      "0.5618431 0\n",
      "---\n",
      "0.9880718 0\n",
      "---\n",
      "0.99125075 1\n",
      "---\n",
      "0.9936054 1\n",
      "---\n",
      "0.6442804 1\n",
      "---\n",
      "0.5548829 1\n",
      "---\n",
      "0.9935111 1\n",
      "---\n",
      "0.9452002 0\n",
      "---\n",
      "0.5512258 0\n",
      "---\n",
      "0.88539505 0\n",
      "---\n",
      "0.77717745 1\n",
      "---\n",
      "0.995937 1\n",
      "---\n",
      "0.61359483 1\n",
      "---\n",
      "0.40403706 0\n",
      "---\n",
      "0.47449738 1\n",
      "---\n",
      "0.87069017 0\n",
      "---\n",
      "0.57638997 -1\n",
      "---\n",
      "0.9974574 0\n",
      "---\n",
      "0.9773406 0\n",
      "---\n",
      "0.9799047 -1\n",
      "---\n",
      "0.91657454 1\n",
      "---\n",
      "0.6040624 1\n",
      "---\n",
      "0.9964268 0\n",
      "---\n",
      "0.9993048 1\n",
      "---\n",
      "0.6042705 -1\n",
      "---\n",
      "0.89680684 0\n",
      "---\n",
      "0.9918869 -1\n",
      "---\n",
      "0.96102726 -1\n",
      "---\n",
      "0.9296325 -1\n",
      "---\n",
      "0.929565 0\n",
      "---\n",
      "0.5047872 1\n",
      "---\n",
      "0.9820615 1\n",
      "---\n",
      "0.7984177 1\n",
      "---\n",
      "0.94580334 -1\n",
      "---\n",
      "0.6386501 0\n",
      "---\n",
      "0.9373015 -1\n",
      "---\n",
      "0.5313482 1\n",
      "---\n",
      "0.924856 0\n",
      "---\n",
      "0.91408616 0\n",
      "---\n",
      "0.99855655 1\n",
      "---\n",
      "0.7559637 0\n",
      "---\n",
      "0.4523606 -1\n",
      "---\n",
      "0.9854218 1\n",
      "---\n",
      "0.99889636 1\n",
      "---\n",
      "0.97588766 -1\n",
      "---\n",
      "0.6273144 0\n",
      "---\n",
      "0.97613025 1\n",
      "---\n",
      "0.64111274 1\n",
      "---\n",
      "0.94941235 1\n",
      "---\n",
      "0.96864426 0\n",
      "---\n",
      "0.9836112 1\n",
      "---\n",
      "0.45288688 -1\n",
      "---\n",
      "0.7845703 1\n",
      "---\n",
      "0.70011246 0\n",
      "---\n",
      "0.9589029 0\n",
      "---\n",
      "0.995509 1\n",
      "---\n",
      "0.98467094 0\n",
      "---\n",
      "0.6043168 0\n",
      "---\n",
      "0.46741694 -1\n",
      "---\n",
      "0.3896011 0\n",
      "---\n",
      "0.608827 -1\n",
      "---\n",
      "0.9873531 0\n",
      "---\n",
      "0.9340322 1\n",
      "---\n",
      "0.6604573 -1\n",
      "---\n",
      "0.7019048 0\n",
      "---\n",
      "0.98527384 -1\n",
      "---\n",
      "0.95232576 -1\n",
      "---\n",
      "0.80976933 -1\n",
      "---\n",
      "0.8737316 1\n",
      "---\n",
      "0.49083647 -1\n",
      "---\n",
      "0.9204127 1\n",
      "---\n",
      "0.9715465 1\n",
      "---\n",
      "0.9851389 -1\n",
      "---\n",
      "0.982068 -1\n",
      "---\n",
      "0.52502906 1\n",
      "---\n",
      "0.9943188 1\n",
      "---\n",
      "0.4799865 -1\n",
      "---\n",
      "0.9152423 1\n",
      "---\n",
      "0.93483555 1\n",
      "---\n",
      "0.51738155 0\n",
      "---\n",
      "0.8597808 1\n",
      "---\n",
      "0.54816836 1\n",
      "---\n",
      "0.9511573 -1\n",
      "---\n",
      "0.97734493 1\n",
      "---\n",
      "0.8148056 -1\n",
      "---\n",
      "0.9829556 -1\n",
      "---\n",
      "0.76585287 0\n",
      "---\n",
      "0.9841013 1\n",
      "---\n",
      "0.5893535 1\n",
      "---\n",
      "0.756381 1\n",
      "---\n",
      "0.93975437 1\n",
      "---\n",
      "0.6871519 0\n",
      "---\n",
      "0.7198931 -1\n",
      "---\n",
      "0.9164412 0\n",
      "---\n",
      "0.6213636 -1\n",
      "---\n",
      "0.82347214 0\n",
      "---\n",
      "0.9908921 -1\n",
      "---\n",
      "0.39677238 -1\n",
      "---\n",
      "0.5867574 1\n",
      "---\n",
      "0.9566028 -1\n",
      "---\n",
      "0.98316514 1\n",
      "---\n",
      "0.81979454 1\n",
      "---\n",
      "0.790413 -1\n",
      "---\n",
      "0.9900625 0\n",
      "---\n",
      "0.995021 1\n",
      "---\n",
      "0.8650649 0\n",
      "---\n",
      "0.91361547 0\n",
      "---\n",
      "0.98278576 -1\n",
      "---\n",
      "0.9783815 -1\n",
      "---\n",
      "0.9967476 0\n",
      "---\n",
      "0.8922583 0\n",
      "---\n",
      "0.82896554 0\n",
      "---\n",
      "0.8223967 0\n",
      "---\n",
      "0.686549 0\n",
      "---\n",
      "0.87582666 1\n",
      "---\n",
      "0.9789947 0\n",
      "---\n",
      "0.555774 1\n",
      "---\n",
      "0.9117892 -1\n",
      "---\n",
      "0.9603955 -1\n",
      "---\n",
      "0.9963738 1\n",
      "---\n",
      "0.9574688 0\n",
      "---\n",
      "0.876118 0\n",
      "---\n",
      "0.7595157 -1\n",
      "---\n",
      "0.87891173 1\n",
      "---\n",
      "0.8987769 1\n",
      "---\n",
      "0.9998401 1\n",
      "---\n",
      "0.97361016 1\n",
      "---\n",
      "0.87880516 0\n",
      "---\n",
      "0.6302065 0\n",
      "---\n",
      "0.8753447 1\n",
      "---\n",
      "0.88218385 -1\n",
      "---\n",
      "0.57752097 -1\n",
      "---\n",
      "0.80660534 0\n",
      "---\n",
      "0.99892104 -1\n",
      "---\n",
      "0.8460446 0\n",
      "---\n",
      "0.80329776 1\n",
      "---\n",
      "0.47891158 -1\n",
      "---\n",
      "0.63789445 1\n",
      "---\n",
      "0.56112957 1\n",
      "---\n",
      "0.48825204 0\n",
      "---\n",
      "0.9459085 0\n",
      "---\n",
      "0.687676 1\n",
      "---\n",
      "0.965067 1\n",
      "---\n",
      "0.9992442 1\n",
      "---\n",
      "0.8010813 -1\n",
      "---\n",
      "0.9525912 0\n",
      "---\n",
      "0.9061153 1\n",
      "---\n",
      "0.92189205 1\n",
      "---\n",
      "0.97814196 -1\n",
      "---\n",
      "0.9213085 1\n",
      "---\n",
      "0.99693155 1\n",
      "---\n",
      "0.9475719 1\n",
      "---\n",
      "0.9852475 1\n",
      "---\n",
      "0.9675794 -1\n",
      "---\n",
      "0.9833145 -1\n",
      "---\n",
      "0.78989965 -1\n",
      "---\n",
      "0.74444723 1\n",
      "---\n",
      "0.86524665 0\n",
      "---\n",
      "0.9194815 1\n",
      "---\n",
      "0.961609 1\n",
      "---\n",
      "0.379232 0\n",
      "---\n",
      "0.71979165 -1\n",
      "---\n",
      "0.6545192 1\n",
      "---\n",
      "0.9987527 1\n",
      "---\n",
      "0.533226 0\n",
      "---\n",
      "0.6512896 0\n",
      "---\n",
      "0.9929184 1\n",
      "---\n",
      "0.51841074 1\n",
      "---\n",
      "0.92941695 -1\n",
      "---\n",
      "0.7487538 0\n",
      "---\n",
      "0.6074385 1\n",
      "---\n",
      "0.9909713 1\n",
      "---\n",
      "0.7878987 1\n",
      "---\n",
      "0.52306205 0\n",
      "---\n",
      "0.9008311 0\n",
      "---\n",
      "0.5028972 1\n",
      "---\n",
      "0.9481658 0\n",
      "---\n",
      "0.96558243 1\n",
      "---\n",
      "0.90967155 0\n",
      "---\n",
      "0.6426811 -1\n",
      "---\n",
      "0.98281187 -1\n",
      "---\n",
      "0.84461415 1\n",
      "---\n",
      "0.98262876 0\n",
      "---\n",
      "0.90167916 1\n",
      "---\n",
      "0.5374747 1\n",
      "---\n",
      "0.96332896 0\n",
      "---\n",
      "0.9017523 -1\n",
      "---\n",
      "0.4996825 1\n",
      "---\n",
      "0.6715569 -1\n",
      "---\n",
      "0.98168445 1\n",
      "---\n",
      "0.7514389 0\n",
      "---\n",
      "0.99453485 1\n",
      "---\n",
      "0.45772186 0\n",
      "---\n",
      "0.4284405 -1\n",
      "---\n",
      "0.8476743 -1\n",
      "---\n",
      "0.7418689 0\n",
      "---\n",
      "0.6689832 0\n",
      "---\n",
      "0.5501776 1\n",
      "---\n",
      "0.92948014 1\n",
      "---\n",
      "0.61758864 0\n",
      "---\n",
      "0.5782599 1\n",
      "---\n",
      "0.4937628 1\n",
      "---\n",
      "0.55237794 1\n",
      "---\n",
      "0.66197425 0\n",
      "---\n",
      "0.9098388 -1\n",
      "---\n",
      "0.75703317 1\n",
      "---\n",
      "0.9848317 1\n",
      "---\n",
      "0.60657036 1\n",
      "---\n",
      "0.8904865 1\n",
      "---\n",
      "0.9243492 1\n",
      "---\n",
      "0.9887183 0\n",
      "---\n",
      "0.47859195 -1\n",
      "---\n",
      "0.50160974 1\n",
      "---\n",
      "0.9692 0\n",
      "---\n",
      "0.94893014 0\n",
      "---\n",
      "0.9165355 1\n",
      "---\n",
      "0.9117111 1\n",
      "---\n",
      "0.9992163 1\n",
      "---\n",
      "0.6956633 -1\n",
      "---\n",
      "0.9568424 -1\n",
      "---\n",
      "0.72735643 1\n",
      "---\n",
      "0.83256793 -1\n",
      "---\n",
      "0.89105093 -1\n",
      "---\n",
      "0.8490016 1\n",
      "---\n",
      "0.5332847 0\n",
      "---\n",
      "0.8010758 -1\n",
      "---\n",
      "0.60958654 -1\n",
      "---\n",
      "0.4636225 1\n",
      "---\n",
      "0.7578374 1\n",
      "---\n",
      "0.8575035 1\n",
      "---\n",
      "0.6696476 -1\n",
      "---\n",
      "0.9120569 1\n",
      "---\n",
      "0.5049418 0\n",
      "---\n",
      "0.8731561 0\n",
      "---\n",
      "0.78269947 0\n",
      "---\n",
      "0.9970561 1\n",
      "---\n",
      "0.71235144 -1\n",
      "---\n",
      "0.7171356 0\n",
      "---\n",
      "0.93202066 1\n",
      "---\n",
      "0.50985897 0\n",
      "---\n",
      "0.5141798 0\n",
      "---\n",
      "0.81649494 1\n",
      "---\n",
      "0.9824399 -1\n",
      "---\n",
      "0.98421234 1\n",
      "---\n",
      "0.8015888 0\n",
      "---\n",
      "0.99933344 1\n",
      "---\n",
      "0.94911915 -1\n",
      "---\n",
      "0.99468887 0\n",
      "---\n",
      "0.70147014 0\n",
      "---\n",
      "0.5799666 0\n",
      "---\n",
      "0.95246613 1\n",
      "---\n",
      "0.94710505 -1\n",
      "---\n",
      "0.70312464 0\n",
      "---\n",
      "0.50722444 0\n",
      "---\n",
      "0.77036273 1\n",
      "---\n",
      "0.9538375 1\n",
      "---\n",
      "0.7301144 1\n",
      "---\n",
      "0.86336666 0\n",
      "---\n",
      "0.99069387 1\n",
      "---\n",
      "0.9781256 0\n",
      "---\n",
      "0.5979775 -1\n",
      "---\n",
      "0.9161775 0\n",
      "---\n",
      "0.8319917 -1\n",
      "---\n",
      "0.89647657 1\n",
      "---\n",
      "0.92126846 1\n",
      "---\n",
      "0.5290683 0\n",
      "---\n",
      "0.5460556 0\n",
      "---\n",
      "0.712849 0\n",
      "---\n",
      "0.99983776 1\n",
      "---\n",
      "0.99755716 -1\n",
      "---\n",
      "0.56800115 -1\n",
      "---\n",
      "0.6735263 0\n",
      "---\n",
      "0.97066873 1\n",
      "---\n",
      "0.81791675 0\n",
      "---\n",
      "0.4780303 -1\n",
      "---\n",
      "0.92737883 -1\n",
      "---\n",
      "0.99748045 1\n",
      "---\n",
      "0.9496585 1\n",
      "---\n",
      "0.9393619 0\n",
      "---\n",
      "0.93213415 1\n",
      "---\n",
      "0.9256685 0\n",
      "---\n",
      "0.7235122 1\n",
      "---\n",
      "0.78823924 -1\n",
      "---\n",
      "0.6573398 1\n",
      "---\n",
      "0.61109066 0\n",
      "---\n",
      "0.9707184 -1\n",
      "---\n",
      "0.78657585 -1\n",
      "---\n",
      "0.6633749 1\n",
      "---\n",
      "0.8286539 0\n",
      "---\n",
      "0.99402654 0\n",
      "---\n",
      "0.9149398 1\n",
      "---\n",
      "0.99635625 0\n",
      "---\n",
      "0.9863564 -1\n",
      "---\n",
      "0.8771852 0\n",
      "---\n",
      "0.9820411 1\n",
      "---\n",
      "0.9647865 1\n",
      "---\n",
      "0.84450954 -1\n",
      "---\n",
      "0.9849136 -1\n",
      "---\n",
      "0.92339337 1\n",
      "---\n",
      "0.97043276 1\n",
      "---\n",
      "0.96325076 0\n",
      "---\n",
      "0.958975 1\n",
      "---\n",
      "0.92970073 1\n",
      "---\n",
      "0.4914042 1\n",
      "---\n",
      "0.40822455 1\n",
      "---\n",
      "0.9940663 1\n",
      "---\n",
      "0.9513514 0\n",
      "---\n",
      "0.9521526 0\n",
      "---\n",
      "0.96264005 1\n",
      "---\n",
      "0.7519125 1\n",
      "---\n",
      "0.80430955 1\n",
      "---\n",
      "0.56010485 -1\n",
      "---\n",
      "0.99446833 1\n",
      "---\n",
      "0.61578107 -1\n",
      "---\n",
      "0.96868896 0\n",
      "---\n",
      "0.99706125 1\n",
      "---\n",
      "0.8261609 -1\n",
      "---\n",
      "0.8218969 1\n",
      "---\n",
      "0.99053234 -1\n",
      "---\n",
      "0.99396646 1\n",
      "---\n",
      "0.8761536 0\n",
      "---\n",
      "0.7828795 1\n",
      "---\n",
      "0.6558272 1\n",
      "---\n",
      "0.9107543 0\n",
      "---\n",
      "0.9976722 1\n",
      "---\n",
      "0.6683365 -1\n",
      "---\n",
      "0.8598027 1\n",
      "---\n",
      "0.82045877 0\n",
      "---\n",
      "0.9574622 1\n",
      "---\n",
      "0.653709 1\n",
      "---\n",
      "0.9943557 1\n",
      "---\n",
      "0.9557249 -1\n",
      "---\n",
      "0.4964506 1\n",
      "---\n",
      "0.39482513 -1\n",
      "---\n",
      "0.95694566 1\n",
      "---\n",
      "0.60274243 1\n",
      "---\n",
      "0.74911284 -1\n",
      "---\n",
      "0.9291793 0\n",
      "---\n",
      "0.976628 1\n",
      "---\n",
      "0.8461085 1\n",
      "---\n",
      "0.48734882 1\n",
      "---\n",
      "0.9304758 0\n",
      "---\n",
      "0.5310374 0\n",
      "---\n",
      "0.6704794 1\n",
      "---\n",
      "0.5967388 -1\n",
      "---\n",
      "0.61699814 -1\n",
      "---\n",
      "0.95194364 1\n",
      "---\n",
      "0.9888153 1\n",
      "---\n",
      "0.9208477 -1\n",
      "---\n",
      "0.9764347 1\n",
      "---\n",
      "0.543995 1\n",
      "---\n",
      "0.90229666 1\n",
      "---\n",
      "0.66047037 0\n",
      "---\n",
      "0.9788711 1\n",
      "---\n",
      "0.9487056 -1\n",
      "---\n",
      "0.60303575 1\n",
      "---\n",
      "0.9408672 1\n",
      "---\n",
      "0.5075109 -1\n",
      "---\n",
      "0.9802584 -1\n",
      "---\n",
      "0.90500796 1\n",
      "---\n",
      "0.985646 -1\n",
      "---\n",
      "0.9965146 -1\n",
      "---\n",
      "0.7758521 1\n",
      "---\n",
      "0.50817734 0\n",
      "---\n",
      "0.9883952 1\n",
      "---\n",
      "0.7889544 1\n",
      "---\n",
      "0.42439976 -1\n",
      "---\n",
      "0.65607387 0\n",
      "---\n",
      "0.61779535 0\n",
      "---\n",
      "0.5022543 -1\n",
      "---\n",
      "0.8775338 0\n",
      "---\n",
      "0.98188084 -1\n",
      "---\n",
      "0.8449954 1\n",
      "---\n",
      "0.5542898 -1\n",
      "---\n",
      "0.98330295 -1\n",
      "---\n",
      "0.9944807 1\n",
      "---\n",
      "0.92385507 0\n",
      "---\n",
      "0.97106385 -1\n",
      "---\n",
      "0.37936723 1\n",
      "---\n",
      "0.6147337 0\n",
      "---\n",
      "0.9585724 0\n",
      "---\n",
      "0.9348372 0\n",
      "---\n",
      "0.54422516 1\n",
      "---\n",
      "0.7122148 -1\n",
      "---\n",
      "0.6822314 -1\n",
      "---\n",
      "0.7718744 0\n",
      "---\n",
      "0.43923527 -1\n",
      "---\n",
      "0.61811805 1\n",
      "---\n",
      "0.5633749 1\n",
      "---\n",
      "0.87885857 -1\n",
      "---\n",
      "0.98622394 -1\n",
      "---\n",
      "0.70059806 1\n",
      "---\n",
      "0.62373316 1\n",
      "---\n",
      "0.75656813 0\n",
      "---\n",
      "0.94922876 1\n",
      "---\n",
      "0.6829729 0\n",
      "---\n",
      "0.46881902 0\n",
      "---\n",
      "0.73915863 0\n",
      "---\n",
      "0.9921545 -1\n",
      "---\n",
      "0.9757736 1\n",
      "---\n",
      "0.8393428 -1\n",
      "---\n",
      "0.87194717 0\n",
      "---\n",
      "0.92213047 0\n",
      "---\n",
      "0.50605583 -1\n",
      "---\n",
      "0.51599234 1\n",
      "---\n",
      "0.51041526 1\n",
      "---\n",
      "0.7768495 1\n",
      "---\n",
      "0.64490294 0\n",
      "---\n",
      "0.59690815 1\n",
      "---\n",
      "0.9000844 1\n",
      "---\n",
      "0.6972613 1\n",
      "---\n",
      "0.9393412 1\n",
      "---\n",
      "0.603598 0\n",
      "---\n",
      "0.9892972 -1\n",
      "---\n",
      "0.50531423 0\n",
      "---\n",
      "0.7946986 1\n",
      "---\n",
      "0.6078651 1\n",
      "---\n",
      "0.6901907 1\n",
      "---\n",
      "0.46181276 0\n",
      "---\n",
      "0.4149831 0\n",
      "---\n",
      "0.9996934 1\n",
      "---\n",
      "0.9940203 1\n",
      "---\n",
      "0.9089802 1\n",
      "---\n",
      "0.9769787 1\n",
      "---\n",
      "0.95674396 -1\n",
      "---\n",
      "0.5387461 -1\n",
      "---\n",
      "0.9339353 -1\n",
      "---\n",
      "0.82742107 1\n",
      "---\n",
      "0.8440738 -1\n",
      "---\n",
      "0.8956299 -1\n",
      "---\n",
      "0.96813786 1\n",
      "---\n",
      "0.6042831 0\n",
      "---\n",
      "0.87792027 1\n",
      "---\n",
      "0.8861132 -1\n",
      "---\n",
      "0.7079985 1\n",
      "---\n",
      "0.98270947 1\n",
      "---\n",
      "0.96955925 0\n",
      "---\n",
      "0.4212993 -1\n",
      "---\n",
      "0.8891201 0\n",
      "---\n",
      "0.7558632 1\n",
      "---\n",
      "0.8626078 1\n",
      "---\n",
      "0.67902315 1\n",
      "---\n",
      "0.9995946 1\n",
      "---\n",
      "0.80784833 0\n",
      "---\n",
      "0.7408415 -1\n",
      "---\n",
      "0.98765373 -1\n",
      "---\n",
      "0.76604885 0\n",
      "---\n",
      "0.7306224 0\n",
      "---\n",
      "0.7202013 1\n",
      "---\n",
      "0.87630904 1\n",
      "---\n",
      "0.5489643 1\n",
      "---\n",
      "0.9408606 0\n",
      "---\n",
      "0.9681242 0\n",
      "---\n",
      "0.73102605 1\n",
      "---\n",
      "0.7723532 1\n",
      "---\n",
      "0.84739923 1\n",
      "---\n",
      "0.99915034 1\n",
      "---\n",
      "0.6794547 0\n",
      "---\n",
      "0.8829483 1\n",
      "---\n",
      "0.9099891 0\n",
      "---\n",
      "0.9055235 1\n",
      "---\n",
      "0.7487654 0\n",
      "---\n",
      "0.57424676 1\n",
      "---\n",
      "0.62528014 1\n",
      "---\n",
      "0.51964605 1\n",
      "---\n",
      "0.94556403 0\n",
      "---\n",
      "0.71459246 -1\n",
      "---\n",
      "0.7462005 1\n",
      "---\n",
      "0.7452146 0\n",
      "---\n",
      "0.5267754 0\n",
      "---\n",
      "0.6325923 0\n",
      "---\n",
      "0.74432737 1\n",
      "---\n",
      "0.9952636 1\n",
      "---\n",
      "0.47280145 1\n",
      "---\n",
      "0.50978386 1\n",
      "---\n",
      "0.7304813 -1\n",
      "---\n",
      "0.5319823 1\n",
      "---\n",
      "0.96452963 -1\n",
      "---\n",
      "0.9949734 -1\n",
      "---\n",
      "0.59885496 1\n",
      "---\n",
      "0.9814824 0\n",
      "---\n",
      "0.9006626 1\n",
      "---\n",
      "0.7017721 0\n",
      "---\n",
      "0.690928 0\n",
      "---\n",
      "0.99585426 0\n",
      "---\n",
      "0.9465395 1\n",
      "---\n",
      "0.7851907 0\n",
      "---\n",
      "0.7727752 0\n",
      "---\n",
      "0.9953246 1\n",
      "---\n",
      "0.76926136 1\n",
      "---\n",
      "0.9658812 0\n",
      "---\n",
      "0.6841556 0\n",
      "---\n",
      "0.9638406 1\n",
      "---\n",
      "0.58702457 -1\n",
      "---\n",
      "0.9574282 0\n",
      "---\n",
      "0.49982917 -1\n",
      "---\n",
      "0.4864126 0\n",
      "---\n",
      "0.5322514 0\n",
      "---\n",
      "0.52130556 1\n",
      "---\n",
      "0.46884486 -1\n",
      "---\n",
      "0.9994519 1\n",
      "---\n",
      "0.9621992 -1\n",
      "---\n",
      "0.98665226 -1\n",
      "---\n",
      "0.99966645 1\n",
      "---\n",
      "0.78415954 0\n",
      "---\n",
      "0.9893053 1\n",
      "---\n",
      "0.9571276 -1\n",
      "---\n",
      "0.87966436 1\n",
      "---\n",
      "0.8760886 1\n",
      "---\n",
      "0.5115036 1\n",
      "---\n",
      "0.91112614 0\n",
      "---\n",
      "0.43443406 1\n",
      "---\n",
      "0.5385402 0\n",
      "---\n",
      "0.89476234 -1\n",
      "---\n",
      "0.9600148 -1\n",
      "---\n",
      "0.81118643 -1\n",
      "---\n",
      "0.77555263 0\n",
      "---\n",
      "0.5263094 0\n",
      "---\n",
      "0.56191075 0\n",
      "---\n",
      "0.9958161 1\n",
      "---\n",
      "0.9175 -1\n",
      "---\n",
      "0.7619173 1\n",
      "---\n",
      "0.9977418 1\n",
      "---\n",
      "0.8411591 1\n",
      "---\n",
      "0.88572943 -1\n",
      "---\n",
      "0.81016076 0\n",
      "---\n",
      "0.98947906 1\n",
      "---\n",
      "0.5226456 1\n",
      "---\n",
      "0.67867565 0\n",
      "---\n",
      "0.928946 1\n",
      "---\n",
      "0.853963 -1\n",
      "---\n",
      "0.461611 -1\n",
      "---\n",
      "0.81116074 1\n",
      "---\n",
      "0.86859787 0\n",
      "---\n",
      "0.97943413 -1\n",
      "---\n",
      "0.99513614 -1\n",
      "---\n",
      "0.50067884 0\n",
      "---\n",
      "0.95634687 0\n",
      "---\n",
      "0.9838182 -1\n",
      "---\n",
      "0.99687773 1\n",
      "---\n",
      "0.6966972 1\n",
      "---\n",
      "0.89655226 0\n",
      "---\n",
      "0.96790135 -1\n",
      "---\n",
      "0.9760088 0\n",
      "---\n",
      "0.58207136 1\n",
      "---\n",
      "0.9845364 1\n",
      "---\n",
      "0.9713076 1\n",
      "---\n",
      "0.61760986 0\n",
      "---\n",
      "0.8103541 0\n",
      "---\n",
      "0.65209997 1\n",
      "---\n",
      "0.91451484 -1\n",
      "---\n",
      "0.978429 1\n",
      "---\n",
      "0.70169836 1\n",
      "---\n",
      "0.95321745 1\n",
      "---\n",
      "0.4574811 0\n",
      "---\n",
      "0.5211696 1\n",
      "---\n",
      "0.4510685 1\n",
      "---\n",
      "0.6882047 1\n",
      "---\n",
      "0.9994548 1\n",
      "---\n",
      "0.9766886 -1\n",
      "---\n",
      "0.92027247 -1\n",
      "---\n",
      "0.7528566 -1\n",
      "---\n",
      "0.78631985 0\n",
      "---\n",
      "0.8390113 1\n",
      "---\n",
      "0.77417433 -1\n",
      "---\n",
      "0.9651665 0\n",
      "---\n",
      "0.8132273 1\n",
      "---\n",
      "0.81079817 1\n",
      "---\n",
      "0.79244435 0\n",
      "---\n",
      "0.6154712 0\n",
      "---\n",
      "0.9700145 1\n",
      "---\n",
      "0.99910027 -1\n",
      "---\n",
      "0.99361664 1\n",
      "---\n",
      "0.96643126 0\n",
      "---\n",
      "0.98237586 1\n",
      "---\n",
      "0.64166033 -1\n",
      "---\n",
      "0.9167587 -1\n",
      "---\n",
      "0.62170786 -1\n",
      "---\n",
      "0.6897655 1\n",
      "---\n",
      "0.965258 -1\n",
      "---\n",
      "0.5798973 1\n",
      "---\n",
      "0.996313 1\n",
      "---\n",
      "0.5996177 1\n",
      "---\n",
      "0.5611699 1\n",
      "---\n",
      "0.8803859 1\n",
      "---\n",
      "0.99490047 1\n",
      "---\n",
      "0.7663667 1\n",
      "---\n",
      "0.5840265 -1\n",
      "---\n",
      "0.99509597 -1\n",
      "---\n",
      "0.6871018 1\n",
      "---\n",
      "0.95121455 1\n",
      "---\n",
      "0.58278215 1\n",
      "---\n",
      "0.8478296 -1\n",
      "---\n",
      "0.8923107 -1\n",
      "---\n",
      "0.9908221 1\n",
      "---\n",
      "0.80039 1\n",
      "---\n",
      "0.9866642 1\n",
      "---\n",
      "0.81202716 1\n",
      "---\n",
      "0.8629522 0\n",
      "---\n",
      "0.9917711 1\n",
      "---\n",
      "0.48607415 -1\n",
      "---\n",
      "0.8927054 -1\n",
      "---\n",
      "0.6219876 0\n",
      "---\n",
      "0.8310226 -1\n",
      "---\n",
      "0.5908225 1\n",
      "---\n",
      "0.6240795 1\n",
      "---\n",
      "0.4517372 1\n",
      "---\n",
      "0.9962789 1\n",
      "---\n",
      "0.96482086 0\n",
      "---\n",
      "0.88731587 0\n",
      "---\n",
      "0.8768927 0\n",
      "---\n",
      "0.95424026 -1\n",
      "---\n",
      "0.5400131 1\n",
      "---\n",
      "0.83386993 1\n",
      "---\n",
      "0.8525375 0\n",
      "---\n",
      "0.8657847 1\n",
      "---\n",
      "0.91244173 1\n",
      "---\n",
      "0.49767524 0\n",
      "---\n",
      "0.95272696 1\n",
      "---\n",
      "0.85475355 0\n",
      "---\n",
      "0.9772124 1\n",
      "---\n",
      "0.833235 1\n",
      "---\n",
      "0.8999298 1\n",
      "---\n",
      "0.99049836 1\n",
      "---\n",
      "0.6133284 1\n",
      "---\n",
      "0.648705 0\n",
      "---\n",
      "0.5853096 1\n",
      "---\n",
      "0.89668155 1\n",
      "---\n",
      "0.66088843 0\n",
      "---\n",
      "0.95405865 1\n",
      "---\n",
      "0.9244188 1\n",
      "---\n",
      "0.912363 1\n",
      "---\n",
      "0.9213742 1\n",
      "---\n",
      "0.9796189 1\n",
      "---\n",
      "0.9939475 1\n",
      "---\n",
      "0.50855607 1\n",
      "---\n",
      "0.739831 -1\n",
      "---\n",
      "0.932377 0\n",
      "---\n",
      "0.9964118 1\n",
      "---\n",
      "0.984942 1\n",
      "---\n",
      "0.9937842 1\n",
      "---\n",
      "0.7329539 -1\n",
      "---\n",
      "0.4529809 1\n",
      "---\n",
      "0.9003418 -1\n",
      "---\n",
      "0.3831377 1\n",
      "---\n",
      "0.79912543 1\n",
      "---\n",
      "0.97615623 1\n",
      "---\n",
      "0.6253024 1\n",
      "---\n",
      "0.9992584 1\n",
      "---\n",
      "0.99497926 1\n",
      "---\n",
      "0.5595231 0\n",
      "---\n",
      "0.99119556 1\n",
      "---\n",
      "0.9875076 0\n",
      "---\n",
      "0.9744771 1\n",
      "---\n",
      "0.8865247 0\n",
      "---\n",
      "0.99058974 1\n",
      "---\n",
      "0.9893857 -1\n",
      "---\n",
      "0.9980792 1\n",
      "---\n",
      "0.9901355 1\n",
      "---\n",
      "0.997569 -1\n",
      "---\n",
      "0.9070146 1\n",
      "---\n",
      "0.94394064 0\n",
      "---\n",
      "0.9807893 1\n",
      "---\n",
      "0.96895695 1\n",
      "---\n",
      "0.80645376 1\n",
      "---\n",
      "0.97392833 -1\n",
      "---\n",
      "0.9818475 0\n",
      "---\n",
      "0.9126837 1\n",
      "---\n",
      "0.6073474 1\n",
      "---\n",
      "0.8655118 1\n",
      "---\n",
      "0.60898346 1\n",
      "---\n",
      "0.7532496 1\n",
      "---\n",
      "0.9913597 0\n",
      "---\n",
      "0.99475276 -1\n",
      "---\n",
      "0.9115378 0\n",
      "---\n",
      "0.95169723 1\n",
      "---\n",
      "0.99418306 -1\n",
      "---\n",
      "0.83354545 1\n",
      "---\n",
      "0.4040875 -1\n",
      "---\n",
      "0.4658244 1\n",
      "---\n",
      "0.8779692 1\n",
      "---\n",
      "0.9375738 0\n",
      "---\n",
      "0.82473624 1\n",
      "---\n",
      "0.88022524 1\n",
      "---\n",
      "0.75950646 1\n",
      "---\n",
      "0.8795359 1\n",
      "---\n",
      "0.62867796 1\n",
      "---\n",
      "0.9627162 0\n",
      "---\n",
      "0.98879313 -1\n",
      "---\n",
      "0.9627391 0\n",
      "---\n",
      "0.9723903 -1\n",
      "---\n",
      "0.860834 1\n",
      "---\n",
      "0.6544059 0\n",
      "---\n",
      "0.441719 0\n",
      "---\n",
      "0.46822608 0\n",
      "---\n",
      "0.9974824 0\n",
      "---\n",
      "0.4263197 0\n",
      "---\n",
      "0.49471793 1\n",
      "---\n",
      "0.9636514 0\n",
      "---\n",
      "0.95479953 -1\n",
      "---\n",
      "0.98140156 -1\n",
      "---\n",
      "0.9091501 0\n",
      "---\n",
      "0.97460246 -1\n",
      "---\n",
      "0.9085382 0\n",
      "---\n",
      "0.8926599 -1\n",
      "---\n",
      "0.99969614 1\n",
      "---\n",
      "0.9912158 -1\n",
      "---\n",
      "0.9352014 0\n",
      "---\n",
      "0.5011496 1\n",
      "---\n",
      "0.59337974 0\n",
      "---\n",
      "0.96232456 0\n",
      "---\n",
      "0.74173254 -1\n",
      "---\n",
      "0.98798347 1\n",
      "---\n",
      "0.9919098 -1\n",
      "---\n",
      "0.9640914 1\n",
      "---\n",
      "0.6194353 0\n",
      "---\n",
      "0.92121416 1\n",
      "---\n",
      "0.51483804 0\n",
      "---\n",
      "0.7409173 1\n",
      "---\n",
      "0.53650594 0\n",
      "---\n",
      "0.92665565 -1\n",
      "---\n",
      "0.8835534 -1\n",
      "---\n",
      "0.68049306 0\n",
      "---\n",
      "0.5494848 0\n",
      "---\n",
      "0.42427173 0\n",
      "---\n",
      "0.7928469 1\n",
      "---\n",
      "0.93051875 1\n",
      "---\n",
      "0.7558757 1\n",
      "---\n",
      "0.64300406 0\n",
      "---\n",
      "0.6591195 -1\n",
      "---\n",
      "0.9821106 -1\n",
      "---\n",
      "0.9888859 -1\n",
      "---\n",
      "0.80399 1\n",
      "---\n",
      "0.76988935 1\n",
      "---\n",
      "0.7267587 1\n",
      "---\n",
      "0.97945654 -1\n",
      "---\n",
      "0.724529 0\n",
      "---\n",
      "0.773309 1\n",
      "---\n",
      "0.55026907 0\n",
      "---\n",
      "0.9531456 1\n",
      "---\n",
      "0.77346843 1\n",
      "---\n",
      "0.9739049 0\n",
      "---\n",
      "0.9014282 0\n",
      "---\n",
      "0.6119304 -1\n",
      "---\n",
      "0.97992665 0\n",
      "---\n",
      "0.9973364 0\n",
      "---\n",
      "0.84093976 1\n",
      "---\n",
      "0.9680042 1\n",
      "---\n",
      "0.9741819 1\n",
      "---\n",
      "0.595577 0\n",
      "---\n",
      "0.9850567 -1\n",
      "---\n",
      "0.86632556 -1\n",
      "---\n",
      "0.9315172 1\n",
      "---\n",
      "0.9867667 0\n",
      "---\n",
      "0.82814074 -1\n",
      "---\n",
      "0.9128053 1\n",
      "---\n",
      "0.80446774 -1\n",
      "---\n",
      "0.8989954 0\n",
      "---\n",
      "0.56617886 0\n",
      "---\n",
      "0.692853 1\n",
      "---\n",
      "0.99406606 1\n",
      "---\n",
      "0.55721647 1\n",
      "---\n",
      "0.9892925 0\n",
      "---\n",
      "0.76323915 -1\n",
      "---\n",
      "0.5544826 -1\n",
      "---\n",
      "0.98537445 1\n",
      "---\n",
      "0.9855623 -1\n",
      "---\n",
      "0.6960268 0\n",
      "---\n",
      "0.9714159 0\n",
      "---\n",
      "0.79652 0\n",
      "---\n",
      "0.6476836 1\n",
      "---\n",
      "0.9311854 -1\n",
      "---\n",
      "0.86523867 1\n",
      "---\n",
      "0.524997 1\n",
      "---\n",
      "0.99195755 1\n",
      "---\n",
      "0.45899594 -1\n",
      "---\n",
      "0.98640954 -1\n",
      "---\n",
      "0.49606198 1\n",
      "---\n",
      "0.9032934 1\n",
      "---\n",
      "0.75520223 1\n",
      "---\n",
      "0.7888193 1\n",
      "---\n",
      "0.52637005 -1\n",
      "---\n",
      "0.9112979 -1\n",
      "---\n",
      "0.65685827 1\n",
      "---\n",
      "0.5405828 0\n",
      "---\n",
      "0.5455817 1\n",
      "---\n",
      "0.5593883 1\n",
      "---\n",
      "0.9987587 1\n",
      "---\n",
      "0.8853038 0\n",
      "---\n",
      "0.42992795 0\n",
      "---\n",
      "0.9659911 1\n",
      "---\n",
      "0.8725759 0\n",
      "---\n",
      "0.93462425 1\n",
      "---\n",
      "0.578714 0\n",
      "---\n",
      "0.9748759 1\n",
      "---\n",
      "0.86751246 -1\n",
      "---\n",
      "0.7232647 1\n",
      "---\n",
      "0.4514096 1\n",
      "---\n",
      "0.44123015 -1\n",
      "---\n",
      "0.9796406 1\n",
      "---\n",
      "0.8498436 1\n",
      "---\n",
      "0.96565235 -1\n",
      "---\n",
      "0.96631974 -1\n",
      "---\n",
      "0.89738476 0\n",
      "---\n",
      "0.5971846 1\n",
      "---\n",
      "0.89182585 -1\n",
      "---\n",
      "0.5926682 1\n",
      "---\n",
      "0.64214504 1\n",
      "---\n",
      "0.8462753 0\n",
      "---\n",
      "0.8052305 -1\n",
      "---\n",
      "0.7025309 1\n",
      "---\n",
      "0.9812794 -1\n",
      "---\n",
      "0.6561527 1\n",
      "---\n",
      "0.51834625 1\n",
      "---\n",
      "0.8612288 1\n",
      "---\n",
      "0.910337 1\n",
      "---\n",
      "0.89213645 0\n",
      "---\n",
      "0.9627356 -1\n",
      "---\n",
      "0.6292246 0\n",
      "---\n",
      "0.39673018 0\n",
      "---\n",
      "0.4955327 0\n",
      "---\n",
      "0.89545286 1\n",
      "---\n",
      "0.5029287 1\n",
      "---\n",
      "0.9858198 1\n",
      "---\n",
      "0.98350567 0\n",
      "---\n",
      "0.66358614 1\n",
      "---\n",
      "0.7096983 1\n",
      "---\n",
      "0.9788866 0\n",
      "---\n",
      "0.5072672 1\n",
      "---\n",
      "0.5253704 1\n",
      "---\n",
      "0.99515486 1\n",
      "---\n",
      "0.8612058 0\n",
      "---\n",
      "0.549569 0\n",
      "---\n",
      "0.99867976 0\n",
      "---\n",
      "0.91933966 0\n",
      "---\n",
      "0.5153351 -1\n",
      "---\n",
      "0.71837115 0\n",
      "---\n",
      "0.8070946 0\n",
      "---\n",
      "0.9035529 1\n",
      "---\n",
      "0.8871469 0\n",
      "---\n",
      "0.50496995 1\n",
      "---\n",
      "0.5816093 -1\n",
      "---\n",
      "0.98258114 1\n",
      "---\n",
      "0.7831288 1\n",
      "---\n",
      "0.49673867 1\n",
      "---\n",
      "0.82263404 1\n",
      "---\n",
      "0.95590246 0\n",
      "---\n",
      "0.9839943 1\n",
      "---\n",
      "0.46441758 1\n",
      "---\n",
      "0.99208784 1\n",
      "---\n",
      "0.61028826 0\n",
      "---\n",
      "0.9777405 1\n",
      "---\n",
      "0.9797883 1\n",
      "---\n",
      "0.58263063 0\n",
      "---\n",
      "0.9008029 -1\n",
      "---\n",
      "0.8987528 0\n",
      "---\n",
      "0.9821714 -1\n",
      "---\n",
      "0.99305356 1\n",
      "---\n",
      "0.9864126 1\n",
      "---\n",
      "0.67248315 0\n",
      "---\n",
      "0.8092545 1\n",
      "---\n",
      "0.9361198 0\n",
      "---\n",
      "0.92916703 0\n",
      "---\n",
      "0.8980012 -1\n",
      "---\n",
      "0.99332476 -1\n",
      "---\n",
      "0.82346976 0\n",
      "---\n",
      "0.8767499 0\n",
      "---\n",
      "0.95242804 1\n",
      "---\n",
      "0.86464983 0\n",
      "---\n",
      "0.5316132 1\n",
      "---\n",
      "0.9979091 -1\n",
      "---\n",
      "0.87586343 0\n",
      "---\n",
      "0.50469416 1\n",
      "---\n",
      "0.9652303 1\n",
      "---\n",
      "0.99930066 1\n",
      "---\n",
      "0.54370713 1\n",
      "---\n",
      "0.99509203 1\n",
      "---\n",
      "0.97485375 1\n",
      "---\n",
      "0.9800171 -1\n",
      "---\n",
      "0.9789358 -1\n",
      "---\n",
      "0.4639032 1\n",
      "---\n",
      "0.87404895 0\n",
      "---\n",
      "0.62583256 0\n",
      "---\n",
      "0.5254072 1\n",
      "---\n",
      "0.5541535 0\n",
      "---\n",
      "0.96375287 1\n",
      "---\n",
      "0.98782384 1\n",
      "---\n",
      "0.7924516 1\n",
      "---\n",
      "0.9377191 1\n",
      "---\n",
      "0.87575114 1\n",
      "---\n",
      "0.5327698 1\n",
      "---\n",
      "0.9215609 0\n",
      "---\n",
      "0.98868346 1\n",
      "---\n",
      "0.98193604 0\n",
      "---\n",
      "0.8623983 0\n",
      "---\n",
      "0.9904492 0\n",
      "---\n",
      "0.42729953 0\n",
      "---\n",
      "0.74637204 0\n",
      "---\n",
      "0.5300193 -1\n",
      "---\n",
      "0.69168687 1\n",
      "---\n",
      "0.99817276 1\n",
      "---\n",
      "0.88115835 1\n",
      "---\n",
      "0.8467566 1\n",
      "---\n",
      "0.9897344 -1\n",
      "---\n",
      "0.9368057 1\n",
      "---\n",
      "0.6952907 1\n",
      "---\n",
      "0.5442342 1\n",
      "---\n",
      "0.5701306 0\n",
      "---\n",
      "0.9818567 1\n",
      "---\n",
      "0.55695385 0\n",
      "---\n",
      "0.85443676 1\n",
      "---\n",
      "0.5586604 1\n",
      "---\n",
      "0.69220984 0\n",
      "---\n",
      "0.7419279 0\n",
      "---\n",
      "0.50290066 1\n",
      "---\n",
      "0.9651636 -1\n",
      "---\n",
      "0.74804604 1\n",
      "---\n",
      "0.5870632 1\n",
      "---\n",
      "0.98025686 1\n",
      "---\n",
      "0.52343553 0\n",
      "---\n",
      "0.97822344 1\n",
      "---\n",
      "0.82285416 -1\n",
      "---\n",
      "0.56633115 1\n",
      "---\n",
      "0.99720156 1\n",
      "---\n",
      "0.7091866 -1\n",
      "---\n",
      "0.76220375 1\n",
      "---\n",
      "0.9496306 -1\n",
      "---\n",
      "0.9981267 1\n",
      "---\n",
      "0.92951506 0\n",
      "---\n",
      "0.83165973 1\n",
      "---\n",
      "0.5701465 1\n",
      "---\n",
      "0.5895802 0\n",
      "---\n",
      "0.9896109 1\n",
      "---\n",
      "0.5915215 1\n",
      "---\n",
      "0.42687768 0\n",
      "---\n",
      "0.9771979 1\n",
      "---\n",
      "0.6245505 0\n",
      "---\n",
      "0.89823735 -1\n",
      "---\n",
      "0.71340513 1\n",
      "---\n",
      "0.52972835 1\n",
      "---\n",
      "0.40626994 1\n",
      "---\n",
      "0.9767007 -1\n",
      "---\n",
      "0.49825838 1\n",
      "---\n",
      "0.9985061 1\n",
      "---\n",
      "0.99578273 1\n",
      "---\n",
      "0.8915417 -1\n",
      "---\n",
      "0.42595035 -1\n",
      "---\n",
      "0.97268695 1\n",
      "---\n",
      "0.9915508 1\n",
      "---\n",
      "0.96062243 -1\n",
      "---\n",
      "0.87916565 0\n",
      "---\n",
      "0.89921993 0\n",
      "---\n",
      "0.57255197 1\n",
      "---\n",
      "0.9515756 -1\n",
      "---\n",
      "0.7888328 0\n",
      "---\n",
      "0.918447 -1\n",
      "---\n",
      "0.885718 0\n",
      "---\n",
      "0.86767095 -1\n",
      "---\n",
      "0.7135659 0\n",
      "---\n",
      "0.7094807 1\n",
      "---\n",
      "0.72969234 0\n",
      "---\n",
      "0.95990956 1\n",
      "---\n",
      "0.97411495 1\n",
      "---\n",
      "0.997113 1\n",
      "---\n",
      "0.8712127 0\n",
      "---\n",
      "0.9992764 1\n",
      "---\n",
      "0.47546628 1\n",
      "---\n",
      "0.8601173 0\n",
      "---\n",
      "0.85646904 1\n",
      "---\n",
      "0.55837923 -1\n",
      "---\n",
      "0.53170884 0\n",
      "---\n",
      "0.9167891 -1\n",
      "---\n",
      "0.86691236 0\n",
      "---\n",
      "0.9872277 1\n",
      "---\n",
      "0.9716297 1\n",
      "---\n",
      "0.8492116 0\n",
      "---\n",
      "0.89113665 -1\n",
      "---\n",
      "0.80908453 -1\n",
      "---\n",
      "0.93896866 1\n",
      "---\n",
      "0.8837867 1\n",
      "---\n",
      "0.9805615 1\n",
      "---\n",
      "0.99618864 0\n",
      "---\n",
      "0.978683 0\n",
      "---\n",
      "0.972558 0\n",
      "---\n",
      "0.9975023 -1\n",
      "---\n",
      "0.78373516 1\n",
      "---\n",
      "0.9908784 -1\n",
      "---\n",
      "0.54085785 1\n",
      "---\n",
      "0.5023499 0\n",
      "---\n",
      "0.8522849 1\n",
      "---\n",
      "0.52401114 0\n",
      "---\n",
      "0.9991995 1\n",
      "---\n",
      "0.90608144 0\n",
      "---\n",
      "0.99390244 1\n",
      "---\n",
      "0.99846333 -1\n",
      "---\n",
      "0.52331823 -1\n",
      "---\n",
      "0.9501676 -1\n",
      "---\n",
      "0.91496646 0\n",
      "---\n",
      "0.7965367 -1\n",
      "---\n",
      "0.5384595 0\n",
      "---\n",
      "0.52198267 0\n",
      "---\n",
      "0.5231468 0\n",
      "---\n",
      "0.7571199 1\n",
      "---\n",
      "0.5998086 1\n",
      "---\n",
      "0.95553327 -1\n",
      "---\n",
      "0.5949024 1\n",
      "---\n",
      "0.848475 0\n",
      "---\n",
      "0.9880977 -1\n",
      "---\n",
      "0.58447146 0\n",
      "---\n",
      "0.7469169 0\n",
      "---\n",
      "0.89399207 0\n",
      "---\n",
      "0.7443059 1\n",
      "---\n",
      "0.90395117 1\n",
      "---\n",
      "0.47544047 1\n",
      "---\n",
      "0.6651539 0\n",
      "---\n",
      "0.7799163 1\n",
      "---\n",
      "0.6505957 -1\n",
      "---\n",
      "0.8339641 0\n",
      "---\n",
      "0.3883763 -1\n",
      "---\n",
      "0.6850218 1\n",
      "---\n",
      "0.9606325 0\n",
      "---\n",
      "0.64886 -1\n",
      "---\n",
      "0.93000996 0\n",
      "---\n",
      "0.6806766 1\n",
      "---\n",
      "0.5500812 1\n",
      "---\n",
      "0.99463326 1\n",
      "---\n",
      "0.71356755 -1\n",
      "---\n",
      "0.71061856 1\n",
      "---\n",
      "0.6197218 0\n",
      "---\n",
      "0.86206555 1\n",
      "---\n",
      "0.7727282 -1\n",
      "---\n",
      "0.9978643 0\n",
      "---\n",
      "0.9303855 0\n",
      "---\n",
      "0.9900119 -1\n",
      "---\n",
      "0.98344874 1\n",
      "---\n",
      "0.9951956 0\n",
      "---\n",
      "0.9916945 1\n",
      "---\n",
      "0.9922434 1\n",
      "---\n",
      "0.56911415 1\n",
      "---\n",
      "0.97165245 1\n",
      "---\n",
      "0.76583827 0\n",
      "---\n",
      "0.9944154 1\n",
      "---\n",
      "0.6906343 1\n",
      "---\n",
      "0.94891334 0\n",
      "---\n",
      "0.6526414 0\n",
      "---\n",
      "0.7920929 1\n",
      "---\n",
      "0.8965366 1\n",
      "---\n",
      "0.90882784 -1\n",
      "---\n",
      "0.72438866 1\n",
      "---\n",
      "0.97902274 1\n",
      "---\n",
      "0.99220973 1\n",
      "---\n",
      "0.90077746 1\n",
      "---\n",
      "0.8493836 0\n",
      "---\n",
      "0.991213 -1\n",
      "---\n",
      "0.88705933 1\n",
      "---\n",
      "0.5772143 0\n",
      "---\n",
      "0.6664094 1\n",
      "---\n",
      "0.99826676 -1\n",
      "---\n",
      "0.96955925 0\n",
      "---\n",
      "0.99222016 0\n",
      "---\n",
      "0.660029 1\n",
      "---\n",
      "0.87089086 -1\n",
      "---\n",
      "0.5138423 0\n",
      "---\n",
      "0.53535664 0\n",
      "---\n",
      "0.9561899 1\n",
      "---\n",
      "0.8370662 0\n",
      "---\n",
      "0.6742538 1\n",
      "---\n",
      "0.9756751 -1\n",
      "---\n",
      "0.57143694 0\n",
      "---\n",
      "0.4113679 1\n",
      "---\n",
      "0.94926345 0\n",
      "---\n",
      "0.9944812 1\n",
      "---\n",
      "0.98994935 1\n",
      "---\n",
      "0.824992 -1\n",
      "---\n",
      "0.9772254 -1\n",
      "---\n",
      "0.97827256 -1\n",
      "---\n",
      "0.7522895 1\n",
      "---\n",
      "0.98517174 -1\n",
      "---\n",
      "0.99978137 1\n",
      "---\n",
      "0.50236475 -1\n",
      "---\n",
      "0.755602 1\n",
      "---\n",
      "0.6227145 1\n",
      "---\n",
      "0.9349647 1\n",
      "---\n",
      "0.93175006 1\n",
      "---\n",
      "0.550274 0\n",
      "---\n",
      "0.9682249 -1\n",
      "---\n",
      "0.94648457 -1\n",
      "---\n",
      "0.98208046 1\n",
      "---\n",
      "0.9070262 1\n",
      "---\n",
      "0.5354739 1\n",
      "---\n",
      "0.860414 0\n",
      "---\n",
      "0.9462214 1\n",
      "---\n",
      "0.9940466 0\n",
      "---\n",
      "0.93798226 1\n",
      "---\n",
      "0.9912039 -1\n",
      "---\n",
      "0.62140954 0\n",
      "---\n",
      "0.9917544 -1\n",
      "---\n",
      "0.7710873 0\n",
      "---\n",
      "0.6985146 0\n",
      "---\n",
      "0.5038587 1\n",
      "---\n",
      "0.82791066 0\n",
      "---\n",
      "0.91440713 -1\n",
      "---\n",
      "0.99507135 -1\n",
      "---\n",
      "0.6737445 -1\n",
      "---\n",
      "0.52099556 1\n",
      "---\n",
      "0.98706937 1\n",
      "---\n",
      "0.9972062 1\n",
      "---\n",
      "0.6932885 1\n",
      "---\n",
      "0.5539948 1\n",
      "---\n",
      "0.87471163 1\n",
      "---\n",
      "0.99268734 1\n",
      "---\n",
      "0.38366508 0\n",
      "---\n",
      "0.6862869 1\n",
      "---\n",
      "0.96395516 1\n",
      "---\n",
      "0.98027813 -1\n",
      "---\n",
      "0.9997009 1\n",
      "---\n",
      "0.7264332 -1\n",
      "---\n",
      "0.52994204 0\n",
      "---\n",
      "0.9604046 0\n",
      "---\n",
      "0.97259986 -1\n",
      "---\n",
      "0.5571803 1\n",
      "---\n",
      "0.9722712 1\n",
      "---\n",
      "0.97586584 0\n",
      "---\n",
      "0.96570677 1\n",
      "---\n",
      "0.5108885 0\n",
      "---\n",
      "0.91733015 1\n",
      "---\n",
      "0.9872957 0\n",
      "---\n",
      "0.9212378 1\n",
      "---\n",
      "0.6516718 0\n",
      "---\n",
      "0.9983914 -1\n",
      "---\n",
      "0.88464725 1\n",
      "---\n",
      "0.49637684 -1\n",
      "---\n",
      "0.97580236 0\n",
      "---\n",
      "0.523394 -1\n",
      "---\n",
      "0.955809 1\n",
      "---\n",
      "0.9897944 -1\n",
      "---\n",
      "0.7592537 -1\n",
      "---\n",
      "0.898008 1\n",
      "---\n",
      "0.72982347 1\n",
      "---\n",
      "0.5602367 1\n",
      "---\n",
      "0.93066794 -1\n",
      "---\n",
      "0.8759819 -1\n",
      "---\n",
      "0.97244143 1\n",
      "---\n",
      "0.95969933 -1\n",
      "---\n",
      "0.5103035 0\n",
      "---\n",
      "0.82885975 -1\n",
      "---\n",
      "0.9967768 1\n",
      "---\n",
      "0.99157774 1\n",
      "---\n",
      "0.9355424 0\n",
      "---\n",
      "0.9632653 1\n",
      "---\n",
      "0.49907827 1\n",
      "---\n",
      "0.98140335 -1\n",
      "---\n",
      "0.9416188 -1\n",
      "---\n",
      "0.79329944 0\n",
      "---\n",
      "0.95130587 1\n",
      "---\n",
      "0.58244586 0\n",
      "---\n",
      "0.8510578 -1\n",
      "---\n",
      "0.9995235 1\n",
      "---\n",
      "0.5917483 1\n",
      "---\n",
      "0.9556576 0\n",
      "---\n",
      "0.8274402 1\n",
      "---\n",
      "0.77220523 0\n",
      "---\n",
      "0.9444841 0\n",
      "---\n",
      "0.95322406 0\n",
      "---\n",
      "0.8671385 1\n",
      "---\n",
      "0.9702542 0\n",
      "---\n",
      "0.9737879 1\n",
      "---\n",
      "0.4537052 -1\n",
      "---\n",
      "0.59408516 0\n",
      "---\n",
      "0.99944323 -1\n",
      "---\n",
      "0.834764 1\n",
      "---\n",
      "0.8024259 0\n",
      "---\n",
      "0.9855019 -1\n",
      "---\n",
      "0.64276 1\n",
      "---\n",
      "0.83811593 -1\n",
      "---\n",
      "0.99363106 -1\n",
      "---\n",
      "0.98758686 1\n",
      "---\n",
      "0.7561042 0\n",
      "---\n",
      "0.94096977 1\n",
      "---\n",
      "0.9420791 0\n",
      "---\n",
      "0.7505337 -1\n",
      "---\n",
      "0.9367895 1\n",
      "---\n",
      "0.6908344 0\n",
      "---\n",
      "0.98296034 1\n",
      "---\n",
      "0.4043302 -1\n",
      "---\n",
      "0.796551 0\n",
      "---\n",
      "0.5414723 0\n",
      "---\n",
      "0.679416 0\n",
      "---\n",
      "0.67978954 0\n",
      "---\n",
      "0.9963447 1\n",
      "---\n",
      "0.7449107 1\n",
      "---\n",
      "0.5767582 1\n",
      "---\n",
      "0.9799317 -1\n",
      "---\n",
      "0.4408043 1\n",
      "---\n",
      "0.7022958 -1\n",
      "---\n",
      "0.9826766 -1\n",
      "---\n",
      "0.97382087 1\n",
      "---\n",
      "0.99734193 1\n",
      "---\n",
      "0.6045585 1\n",
      "---\n",
      "0.8914392 1\n",
      "---\n",
      "0.7701688 0\n",
      "---\n",
      "0.67417663 0\n",
      "---\n",
      "0.83490956 1\n",
      "---\n",
      "0.9784672 1\n",
      "---\n",
      "0.95649 0\n",
      "---\n",
      "0.82814384 0\n",
      "---\n",
      "0.9628936 -1\n",
      "---\n",
      "0.5467553 0\n",
      "---\n",
      "0.7278346 0\n",
      "---\n",
      "0.99192905 -1\n",
      "---\n",
      "0.6641394 1\n",
      "---\n",
      "0.9198738 0\n",
      "---\n",
      "0.9706764 1\n",
      "---\n",
      "0.5504353 -1\n",
      "---\n",
      "0.47507173 0\n",
      "---\n",
      "0.995435 1\n",
      "---\n",
      "0.96130043 0\n",
      "---\n",
      "0.49889532 -1\n",
      "---\n",
      "0.9302014 0\n",
      "---\n",
      "0.98488605 1\n",
      "---\n",
      "0.9986112 1\n",
      "---\n",
      "0.5763856 -1\n",
      "---\n",
      "0.583068 -1\n",
      "---\n",
      "0.49168324 -1\n",
      "---\n",
      "0.4754368 0\n",
      "---\n",
      "0.6095317 0\n",
      "---\n",
      "0.7201143 1\n",
      "---\n",
      "0.5355282 1\n",
      "---\n",
      "0.77574086 0\n",
      "---\n",
      "0.4456569 0\n",
      "---\n",
      "0.5062749 -1\n",
      "---\n",
      "0.9997587 1\n",
      "---\n",
      "0.5267523 1\n",
      "---\n",
      "0.72986513 1\n",
      "---\n",
      "0.61155266 1\n",
      "---\n",
      "0.7504481 0\n",
      "---\n",
      "0.99356425 1\n",
      "---\n",
      "0.6781882 1\n",
      "---\n",
      "0.8590834 0\n",
      "---\n",
      "0.983845 1\n",
      "---\n",
      "0.99546206 1\n",
      "---\n",
      "0.99941564 1\n",
      "---\n",
      "0.9391194 0\n",
      "---\n",
      "0.6010226 0\n",
      "---\n",
      "0.595471 -1\n",
      "---\n",
      "0.9441232 1\n",
      "---\n",
      "0.9975284 1\n",
      "---\n",
      "0.91352767 1\n",
      "---\n",
      "0.99343324 -1\n",
      "---\n",
      "0.738745 1\n",
      "---\n",
      "0.44141316 -1\n",
      "---\n",
      "0.9544251 1\n",
      "---\n",
      "0.96241915 -1\n",
      "---\n",
      "0.9810142 1\n",
      "---\n",
      "0.8335897 0\n",
      "---\n",
      "0.93677235 1\n",
      "---\n",
      "0.91933346 0\n",
      "---\n",
      "0.9756141 0\n",
      "---\n",
      "0.42125848 -1\n",
      "---\n",
      "0.9606707 0\n",
      "---\n",
      "0.8995669 -1\n",
      "---\n",
      "0.9069308 1\n",
      "---\n",
      "0.9897926 1\n",
      "---\n",
      "0.99030054 1\n",
      "---\n",
      "0.98777205 1\n",
      "---\n",
      "0.8469105 1\n",
      "---\n",
      "0.99855334 -1\n",
      "---\n",
      "0.93477905 0\n",
      "---\n",
      "0.5043478 1\n",
      "---\n",
      "0.5664006 0\n",
      "---\n",
      "0.67742985 -1\n",
      "---\n",
      "0.48372242 0\n",
      "---\n",
      "0.98757356 -1\n",
      "---\n",
      "0.9882196 1\n",
      "---\n",
      "0.81172 1\n",
      "---\n",
      "0.51406723 1\n",
      "---\n",
      "0.7639457 1\n",
      "---\n",
      "0.6408444 1\n",
      "---\n",
      "0.5927402 -1\n",
      "---\n",
      "0.87972426 1\n",
      "---\n",
      "0.45158285 1\n",
      "---\n",
      "0.5114025 1\n",
      "---\n",
      "0.55465645 1\n",
      "---\n",
      "0.9887172 1\n",
      "---\n",
      "0.91550535 1\n",
      "---\n",
      "0.8964884 -1\n",
      "---\n",
      "0.5141622 0\n",
      "---\n",
      "0.6272057 -1\n",
      "---\n",
      "0.9475831 -1\n",
      "---\n",
      "0.9875252 0\n",
      "---\n",
      "0.8041651 0\n",
      "---\n",
      "0.81965613 0\n",
      "---\n",
      "0.6072838 0\n",
      "---\n",
      "0.9510556 -1\n",
      "---\n",
      "0.5370419 1\n",
      "---\n",
      "0.9356003 1\n",
      "---\n",
      "0.9892547 1\n",
      "---\n",
      "0.9973597 -1\n",
      "---\n",
      "0.7246707 -1\n",
      "---\n",
      "0.64080375 0\n",
      "---\n",
      "0.7858064 0\n",
      "---\n",
      "0.9329262 -1\n",
      "---\n",
      "0.7096095 1\n",
      "---\n",
      "0.9861268 1\n",
      "---\n",
      "0.9919467 1\n",
      "---\n",
      "0.8177253 0\n",
      "---\n",
      "0.97439814 1\n",
      "---\n",
      "0.5621115 1\n",
      "---\n",
      "0.6825468 1\n",
      "---\n",
      "0.70518386 0\n",
      "---\n",
      "0.43430343 -1\n",
      "---\n",
      "0.9389791 1\n",
      "---\n",
      "0.8053678 0\n",
      "---\n",
      "0.5534806 -1\n",
      "---\n",
      "0.92289877 1\n",
      "---\n",
      "0.88991356 0\n",
      "---\n",
      "0.8540101 1\n",
      "---\n",
      "0.9130864 -1\n",
      "---\n",
      "0.68195814 0\n",
      "---\n",
      "0.50447136 1\n",
      "---\n",
      "0.7674731 0\n",
      "---\n",
      "0.94743466 0\n",
      "---\n",
      "0.8975806 1\n",
      "---\n",
      "0.48297694 1\n",
      "---\n",
      "0.50025284 1\n",
      "---\n",
      "0.53937566 -1\n",
      "---\n",
      "0.6174796 1\n",
      "---\n",
      "0.9798096 0\n",
      "---\n",
      "0.8729978 0\n",
      "---\n",
      "0.9986563 1\n",
      "---\n",
      "0.71452427 1\n",
      "---\n",
      "0.9542954 0\n",
      "---\n",
      "0.97398853 0\n",
      "---\n",
      "0.9790358 -1\n",
      "---\n",
      "0.99931437 1\n",
      "---\n",
      "0.9838084 1\n",
      "---\n",
      "0.77189595 0\n",
      "---\n",
      "0.43885136 0\n",
      "---\n",
      "0.8230821 0\n",
      "---\n",
      "0.985306 0\n",
      "---\n",
      "0.5804932 1\n",
      "---\n",
      "0.9007746 0\n",
      "---\n",
      "0.96484524 1\n",
      "---\n",
      "0.97212493 -1\n",
      "---\n",
      "0.47339466 0\n",
      "---\n",
      "0.89200205 -1\n",
      "---\n",
      "0.8718394 -1\n",
      "---\n",
      "0.98752683 1\n",
      "---\n",
      "0.869123 1\n",
      "---\n",
      "0.9641558 1\n",
      "---\n",
      "0.96322083 0\n",
      "---\n",
      "0.94187105 1\n",
      "---\n",
      "0.9651595 1\n",
      "---\n",
      "0.9915748 1\n",
      "---\n",
      "0.4551162 -1\n",
      "---\n",
      "0.84585106 1\n",
      "---\n",
      "0.8131989 1\n",
      "---\n",
      "0.97901106 -1\n",
      "---\n",
      "0.9703548 -1\n",
      "---\n",
      "0.59828126 1\n",
      "---\n",
      "0.998961 1\n",
      "---\n",
      "0.82215655 0\n",
      "---\n",
      "0.66079634 1\n",
      "---\n",
      "0.9131427 0\n",
      "---\n",
      "0.76289344 0\n",
      "---\n",
      "0.5036578 0\n",
      "---\n",
      "0.9666443 1\n",
      "---\n",
      "0.4729142 -1\n",
      "---\n",
      "0.69822174 0\n",
      "---\n",
      "0.9806256 -1\n",
      "---\n",
      "0.9620611 1\n",
      "---\n",
      "0.9084609 0\n",
      "---\n",
      "0.7804395 1\n",
      "---\n",
      "0.9679959 1\n",
      "---\n",
      "0.8123595 1\n",
      "---\n",
      "0.98312455 1\n",
      "---\n",
      "0.9711685 -1\n",
      "---\n",
      "0.99736893 1\n",
      "---\n",
      "0.9409733 0\n",
      "---\n",
      "0.98411953 1\n",
      "---\n",
      "0.8829996 1\n",
      "---\n",
      "0.46789786 1\n",
      "---\n",
      "0.98097205 1\n",
      "---\n",
      "0.5807952 1\n",
      "---\n",
      "0.6483092 0\n",
      "---\n",
      "0.9867914 1\n",
      "---\n",
      "0.6030921 0\n",
      "---\n",
      "0.99926794 1\n",
      "---\n",
      "0.9594201 1\n",
      "---\n",
      "0.81658375 1\n",
      "---\n",
      "0.9420079 1\n",
      "---\n",
      "0.9355067 1\n",
      "---\n",
      "0.823424 0\n",
      "---\n",
      "0.95641613 0\n",
      "---\n",
      "0.4862327 1\n",
      "---\n",
      "0.99945337 -1\n",
      "---\n",
      "0.4776309 1\n",
      "---\n",
      "0.98226416 -1\n",
      "---\n",
      "0.90039456 -1\n",
      "---\n",
      "0.7097701 0\n",
      "---\n",
      "0.9476975 1\n",
      "---\n",
      "0.6724644 -1\n",
      "---\n",
      "0.97093713 0\n",
      "---\n",
      "0.9647414 0\n",
      "---\n",
      "0.8322573 -1\n",
      "---\n",
      "0.8460194 1\n",
      "---\n",
      "0.513056 0\n",
      "---\n",
      "0.9981724 1\n",
      "---\n",
      "0.5126723 0\n",
      "---\n",
      "0.93073535 -1\n",
      "---\n",
      "0.9046966 1\n",
      "---\n",
      "0.54494625 0\n",
      "---\n",
      "0.9397749 1\n",
      "---\n",
      "0.6218054 0\n",
      "---\n",
      "0.67543566 0\n",
      "---\n",
      "0.846583 -1\n",
      "---\n",
      "0.86976475 1\n",
      "---\n",
      "0.49809134 1\n",
      "---\n",
      "0.9898851 1\n",
      "---\n",
      "0.7281517 -1\n",
      "---\n",
      "0.71565187 -1\n",
      "---\n",
      "0.7102327 1\n",
      "---\n",
      "0.3529867 1\n",
      "---\n",
      "0.6706453 1\n",
      "---\n",
      "0.99617946 0\n",
      "---\n",
      "0.8161273 -1\n",
      "---\n",
      "0.98410416 1\n",
      "---\n",
      "0.5235658 1\n",
      "---\n",
      "0.9731875 0\n",
      "---\n",
      "0.9887494 1\n",
      "---\n",
      "0.94981885 1\n",
      "---\n",
      "0.8709769 0\n",
      "---\n",
      "0.9183261 0\n",
      "---\n",
      "0.9922578 1\n",
      "---\n",
      "0.48940623 1\n",
      "---\n",
      "0.9068176 0\n",
      "---\n",
      "0.9534318 1\n",
      "---\n",
      "0.4883591 1\n",
      "---\n",
      "0.75725627 -1\n",
      "---\n",
      "0.8853648 -1\n",
      "---\n",
      "0.5363039 0\n",
      "---\n",
      "0.90763533 1\n",
      "---\n",
      "0.52926993 1\n",
      "---\n",
      "0.9410622 -1\n",
      "---\n",
      "0.89367515 1\n",
      "---\n",
      "0.7265743 0\n",
      "---\n",
      "0.76889634 0\n",
      "---\n",
      "0.8592528 1\n",
      "---\n",
      "0.48950303 1\n",
      "---\n",
      "0.78675765 0\n",
      "---\n",
      "0.7294369 0\n",
      "---\n",
      "0.957768 -1\n",
      "---\n",
      "0.81077707 -1\n",
      "---\n",
      "0.7547043 1\n",
      "---\n",
      "0.7770753 1\n",
      "---\n",
      "0.99282837 0\n",
      "---\n",
      "0.5393386 0\n",
      "---\n",
      "0.9879446 1\n",
      "---\n",
      "0.9353199 0\n",
      "---\n",
      "0.91940343 1\n",
      "---\n",
      "0.86046946 1\n",
      "---\n",
      "0.45509425 0\n",
      "---\n",
      "0.98691845 1\n",
      "---\n",
      "0.82089525 1\n",
      "---\n",
      "0.7796837 1\n",
      "---\n",
      "0.993726 1\n",
      "---\n",
      "0.87865925 0\n",
      "---\n",
      "0.9709293 1\n",
      "---\n",
      "0.97225296 -1\n",
      "---\n",
      "0.6612409 1\n",
      "---\n",
      "0.9346843 -1\n",
      "---\n",
      "0.9060309 0\n",
      "---\n",
      "0.9880933 -1\n",
      "---\n",
      "0.9993855 1\n",
      "---\n",
      "0.83671093 1\n",
      "---\n",
      "0.9805325 -1\n",
      "---\n",
      "0.7092221 1\n",
      "---\n",
      "0.5670052 0\n",
      "---\n",
      "0.8761343 1\n",
      "---\n",
      "0.5070465 -1\n",
      "---\n",
      "0.9625704 -1\n",
      "---\n",
      "0.98110163 -1\n",
      "---\n",
      "0.48981747 1\n",
      "---\n",
      "0.98173803 1\n",
      "---\n",
      "0.99651265 0\n",
      "---\n",
      "0.99929595 1\n",
      "---\n",
      "0.4007619 0\n",
      "---\n",
      "0.8928219 0\n",
      "---\n",
      "0.8236067 -1\n",
      "---\n",
      "0.7944318 1\n",
      "---\n",
      "0.97693205 1\n",
      "---\n",
      "0.5737819 1\n",
      "---\n",
      "0.9891752 0\n",
      "---\n",
      "0.99704546 1\n",
      "---\n",
      "0.99805903 1\n",
      "---\n",
      "0.88044256 0\n",
      "---\n",
      "0.9722669 0\n",
      "---\n",
      "0.9889405 0\n",
      "---\n",
      "0.7564862 0\n",
      "---\n",
      "0.9704267 0\n",
      "---\n",
      "0.96031237 0\n",
      "---\n",
      "0.4801648 -1\n",
      "---\n",
      "0.9743804 0\n",
      "---\n",
      "0.5765823 -1\n",
      "---\n",
      "0.6471702 0\n",
      "---\n",
      "0.9354745 1\n",
      "---\n",
      "0.93749475 1\n",
      "---\n",
      "0.92530245 0\n",
      "---\n",
      "0.9848833 1\n",
      "---\n",
      "0.9734882 0\n",
      "---\n",
      "0.9578955 0\n",
      "---\n",
      "0.5023364 0\n",
      "---\n",
      "0.9705268 1\n",
      "---\n",
      "0.5277804 0\n",
      "---\n",
      "0.953436 1\n",
      "---\n",
      "0.9648216 -1\n",
      "---\n",
      "0.99806654 1\n",
      "---\n",
      "0.9603534 -1\n",
      "---\n",
      "0.54416645 1\n",
      "---\n",
      "0.91527903 1\n",
      "---\n",
      "0.81607574 0\n",
      "---\n",
      "0.6723476 1\n",
      "---\n",
      "0.9684257 1\n",
      "---\n",
      "0.53617686 -1\n",
      "---\n",
      "0.85014164 0\n",
      "---\n",
      "0.999849 1\n",
      "---\n",
      "0.72646236 -1\n",
      "---\n",
      "0.97565675 -1\n",
      "---\n",
      "0.3988139 -1\n",
      "---\n",
      "0.99696183 1\n",
      "---\n",
      "0.9131202 0\n",
      "---\n",
      "0.83997446 -1\n",
      "---\n",
      "0.99658585 0\n",
      "---\n",
      "0.9254657 -1\n",
      "---\n",
      "0.99490285 0\n",
      "---\n",
      "0.54310983 1\n",
      "---\n",
      "0.8097626 -1\n",
      "---\n",
      "0.6062005 1\n",
      "---\n",
      "0.5080715 1\n",
      "---\n",
      "0.9838928 -1\n",
      "---\n",
      "0.9682457 1\n",
      "---\n",
      "0.78023005 0\n",
      "---\n",
      "0.95727336 1\n",
      "---\n",
      "0.9250334 1\n",
      "---\n",
      "0.98738897 0\n",
      "---\n",
      "0.8699605 0\n",
      "---\n",
      "0.5661307 -1\n",
      "---\n",
      "0.7793472 1\n",
      "---\n",
      "0.46641412 0\n",
      "---\n",
      "0.8605702 1\n",
      "---\n",
      "0.97312105 -1\n",
      "---\n",
      "0.83293515 0\n",
      "---\n",
      "0.99299324 1\n",
      "---\n",
      "0.97730494 -1\n",
      "---\n",
      "0.99208343 1\n",
      "---\n",
      "0.47778586 1\n",
      "---\n",
      "0.7211149 -1\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def decode_sentiment(score):\n",
    "    if score[0] > score[1] and score[0] > score[2]:\n",
    "        print(score[0], '0\\n---')\n",
    "        return 0\n",
    "    elif score[1] > score[0] and score[1] > score[2]:\n",
    "        print(score[1], '1\\n---')\n",
    "        return 1\n",
    "    elif score[2] > score[0] and score[2] > score[1]:\n",
    "        print(score[2], '-1\\n---')\n",
    "        return -1\n",
    "\n",
    "scores = model.predict(x_test, verbose=1, batch_size=10)\n",
    "print(scores)\n",
    "y_pred_1d = [decode_sentiment(score) for score in scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "Confusion Matrix provide a nice overlook at the model's performance in classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=13)\n",
    "    plt.yticks(tick_marks, classes, fontsize=13)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label', fontsize=17)\n",
    "    plt.xlabel('Predicted label', fontsize=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix(test_data['Polarity'].to_list(), y_pred_1d)\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_confusion_matrix(cnf_matrix, classes=test_data['Polarity'].unique(), title=\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.68      0.70       530\n",
      "           0       0.69      0.63      0.66       697\n",
      "           1       0.73      0.82      0.77       876\n",
      "\n",
      "    accuracy                           0.72      2103\n",
      "   macro avg       0.72      0.71      0.71      2103\n",
      "weighted avg       0.72      0.72      0.72      2103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(list(test_data['Polarity']), y_pred_1d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a pretty good model we trained here in terms of NLP. Around 80% accuracy is good enough considering the baseline human accuracy also pretty low in these tasks. Also, you may go on and explore the dataset, some tweets might have other languages than English. So our Embedding and Tokenizing wont have effect on them. But on practical scenario, this model is good for handling most tasks for Sentiment Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Some of the resource and people who help me learn some concepts</h3>\n",
    "<font color='#008080'>\n",
    "    <ul>\n",
    "        <li> <b>Andrew NG's Seqence Model Course</b> at <a href=\"https://www.coursera.org/learn/nlp-sequence-models\"> Coursera</a> </li>\n",
    "    \n",
    "<li> <b>Andrej Karpathy's Blog</b> on <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">Effectiveness of RNN</a></li>\n",
    "\n",
    "<li> <b>Intuitive Understanding of GloVe Embedding</b> on <a href=\"https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010\">TDS</a></li>\n",
    "\n",
    "<li> <b>Keras tutorial on Word Embedding</b> <a href=\"https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\"> here</a></li>\n",
    "\n",
    "</ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='#696969'>I got to say like you, I am still at learning phase in terms of NLP. I have got lot to learn in future. I found that writing this notebook even though it is done by lot of people before me helps me with a deeper and complete understanding our the concepts that I am learning. Kaggle has been a amazing place to learn from and contribute to community of Data Science Aspirants.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color='red'> If you find this notebook usefull kindly UPVOTE this notebook. I am new to writting notebooks hope that would really encourage me to write and learn more.</font></h2>\n",
    "\n",
    "<h5>Thanks in Advance. Have a nice day. Learn more and Happy Kaggle</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-124bb67c79cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# specify the color palettes and make subplots for each preddeg and highdeg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mc1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolor_palette\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolor_palette\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sb' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize the number of categories to plot\n",
    "N = 3\n",
    "\n",
    "# separate out the counts for preddeg and highdeg\n",
    "#preddeg_counts = df.preddeg.value_counts(sort = False).values\n",
    "#highdeg_counts = df.highdeg.value_counts(sort = False).values\n",
    "\n",
    "preddeg_counts = [72, 70, 73]\n",
    "highdeg_counts = [28, 30, 27]\n",
    "\n",
    "# get the locations to plot bars at and specify the width of each bar\n",
    "ind = np.arange(N)\n",
    "width = 0.35\n",
    "\n",
    "# specify the color palettes and make subplots for each preddeg and highdeg\n",
    "c1, c2 = sb.color_palette()[0], sb.color_palette()[1]\n",
    "fig, ax = plt.subplots(figsize = (8,5))\n",
    "\n",
    "# plot\n",
    "rects1 = ax.bar(ind, preddeg_counts, width, color = c1)\n",
    "rects2 = ax.bar(ind + width, highdeg_counts, width, color = c2)\n",
    "\n",
    "# specify the tick locations\n",
    "ax.set_xticks(ind + width / 2)\n",
    "\n",
    "# label\n",
    "ax.set_xticklabels((\"Certificate\", \"Associate's\", \"Bachelor's\", \"Graduate's\"), fontsize = 11)\n",
    "ax.legend( (rects1[0], rects2[0]), ('Predominant Award', 'Highest Award'), fontsize = 12 )\n",
    "plt.title('Distribution of Degrees', fontsize = 14)\n",
    "plt.xlabel('Award', fontsize = 12)\n",
    "plt.ylabel('Count', fontsize = 12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
