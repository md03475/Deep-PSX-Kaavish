{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Mechanism "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all relevant libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs as tfdocs\n",
    "from keras.optimizers import adam\n",
    "from keras.utils import plot_model\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, save_model, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, GRU\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A file is chosen as a sample. The code is generalized for all such files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5153</th>\n",
       "      <td>2020-10-02</td>\n",
       "      <td>1840.0</td>\n",
       "      <td>1840.0</td>\n",
       "      <td>1689.01</td>\n",
       "      <td>1689.51</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154</th>\n",
       "      <td>2020-10-05</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>1660.0</td>\n",
       "      <td>1630.00</td>\n",
       "      <td>1650.00</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5155</th>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>1634.0</td>\n",
       "      <td>1600.00</td>\n",
       "      <td>1631.60</td>\n",
       "      <td>620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5156</th>\n",
       "      <td>2020-10-07</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>1638.0</td>\n",
       "      <td>1630.00</td>\n",
       "      <td>1636.50</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5157</th>\n",
       "      <td>2020-10-08</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>1625.00</td>\n",
       "      <td>1625.00</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    Open    High      Low    Close  Volume\n",
       "5153  2020-10-02  1840.0  1840.0  1689.01  1689.51     400\n",
       "5154  2020-10-05  1650.0  1660.0  1630.00  1650.00     320\n",
       "5155  2020-10-06  1625.0  1634.0  1600.00  1631.60     620\n",
       "5156  2020-10-07  1636.0  1638.0  1630.00  1636.50     560\n",
       "5157  2020-10-08  1625.0  1625.0  1625.00  1625.00      20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abl = pd.read_csv('BATA PA Equity.csv') \n",
    "abl.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDEAAAIuCAYAAACmWpP5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeZhcVZ3/8c+ppbuzrx0IWcjCEsKWQARklYCAwAA6LqjjPqDCT51BR9EZFddBxxW3EVdcRkFAAQERAVllSQhLEgKEJGSFLJ2k03st5/dH3Vt969attatr6Xq/nidPqm7dunWqusnD+dT3fI+x1goAAAAAAKDehWo9AAAAAAAAgGIQYgAAAAAAgIZAiAEAAAAAABoCIQYAAAAAAGgIhBgAAAAAAKAhEGIAAAAAAICGUPMQwxgTNsasMMb82bk/1xjzmDHmRWPM9caYFud4q3N/rfP4HM81Pu0cf94Yc3Zt3gkAAAAAABhOkVoPQNLHJD0nabxz/2uSvm2t/b0x5n8lfUDSj5y/d1trDzLGXOyc9zZjzEJJF0s6XNIBkv5mjDnEWpvI9YJTp061c+bMGbY3BAAAAAAAyrN8+fKd1tr2oMdqGmIYY2ZKOk/SVyRdYYwxkpZKeodzynWSrlIqxLjQuS1JN0r6vnP+hZJ+b63tl7TeGLNW0nGS/pHrdefMmaNly5ZV/P0AAAAAAIChMca8nOuxWi8n+Y6kT0pKOvenSNpjrY079zdLmuHcniFpkyQ5j+91zk8fD3gOAAAAAAAYIWoWYhhjzpe03Vq73Hs44FRb4LF8z/G+3qXGmGXGmGU7duwoebwAAAAAAKC2almJcZKkC4wxGyT9XqllJN+RNNEY4y5zmSlpq3N7s6RZkuQ8PkFSh/d4wHPSrLXXWmuXWGuXtLcHLq0BAAAAAAB1rGYhhrX209bamdbaOUo15rzXWvtOSfdJerNz2nsk3eLcvtW5L+fxe6211jl+sbN7yVxJB0t6vEpvAwAAAAAAVEk97E7i9ylJvzfGfFnSCkk/c47/TNKvncadHUoFH7LWrjLG3CBptaS4pMvz7UwCAAAAAAAak0kVMzSXJUuWWHYnAQAAAACg/hhjlltrlwQ9VuvdSQAAAAAAAIpCiAEAAAAAABoCIQYAAAAAAGgIhBgAAAAAAKAhEGIAAAAAAICGQIgBAAAAAAAaAiEGAAAAAABoCIQYAAAAAACgIRBiAAAAAACAhkCIAQAAAAAAGgIhBgAAAAAAaAiEGAAAAAAAoCEQYgAAAAAAgIZAiAEAAAAAOXT1x/W6/7lPT23aU+uhABAhBgAAAADktGLjbm3Y1aNv3PV8rYcCQIQYAAAAAFCQla31EACIEAMAAAAAADQIQgwAAAAAKMDI1HoIAESIAQAAAAAFsZwEqA+EGAAAAACQAxUYQH0hxAAAAAAAAA2BEAMAAAAAADQEQgwAAAAAKMDSEgOoC4QYAAAAAACgIRBiAAAAAACAhkCIAQAAAAA5GDYnAeoKIQYAAAAA5EAvDKC+EGIAAAAAAICGQIgBAAAAADmwnASoL4QYAAAAAFAAy0qA+kCIAQAAAAAAGgIhBgAAAAAAaAiEGAAAAACQAy0xgPpCiAEAAAAAABoCIQYAAAAABOiLJfTZW1bWehgAPAgxAAAAACDA6m2demlHtyTJiu1JgHpAiAEAAAAAAZJJggug3hBiAAAAAECAuCfEMLT4BOoCIQYAAAAABIgnBkMMlpMA9YEQAwAAAAACxJLJWg8BgA8hBgAAAAAEiMUJMYB6Q4gBAAAAAAHiNPYE6g4hBgAAAAAEiCUGKzEseQZQFwgxAAAAACDA/c/vqPUQAPgQYgAAAABAgC17ems9BAA+hBgAAAAAEGAgQWNPoN4QYgAAAABAgL4YIQZQbwgxAAAAACBAfyxR6yEA8CHEAAAAAACfXV39Wrezu9bDAOBTsxDDGNNmjHncGPO0MWaVMeYLzvFfGmPWG2Oecv4sco4bY8w1xpi1xphnjDHHeK71HmPMi86f99TqPQEAAAAYGW59emvG/SljW2o0EgBekRq+dr+kpdbaLmNMVNJDxpg7ncf+w1p7o+/8N0g62PlzvKQfSTreGDNZ0uclLZFkJS03xtxqrd1dlXcBAAAAYMSJJ2zG/faxrTUaCQCvmlVi2JQu527U+WPzPOVCSb9ynveopInGmOmSzpZ0t7W2wwku7pZ0znCOHQAAAMDIlrSZU5N8ExUA1VPTnhjGmLAx5ilJ25UKIh5zHvqKs2Tk28YYN/KcIWmT5+mbnWO5jgMAAABAWZLWf58YA6gHNQ0xrLUJa+0iSTMlHWeMOULSpyUtkPQaSZMlfco53QRdIs/xDMaYS40xy4wxy3bs2FGR8QMAAAAYmbIqMcgwgLpQF7uTWGv3SPq7pHOstducJSP9kn4h6TjntM2SZnmeNlPS1jzH/a9xrbV2ibV2SXt7+zC8CwAAAAAjhfWlFv7KDAC1UcvdSdqNMROd26MknSlpjdPnQsYYI+kiSSudp9wq6d3OLiUnSNprrd0m6S5JZxljJhljJkk6yzkGAAAAAGXJDi1IMYB6UMvdSaZLus4YE1YqTLnBWvtnY8y9xph2pZaJPCXpQ875d0g6V9JaST2S3idJ1toOY8yXJD3hnPdFa21HFd8HAAAAgBGG5SRAfapZiGGtfUbS4oDjS3OcbyVdnuOxn0v6eUUHCAAAAKBp0dgTqE910RMDAAAAAOqJtVYh471fu7EAGESIAQAAAAA+SWsVMsZzv4aDAZBGiAEAAAAAPkmrjBDD0tgTqAuEGAAAAADgk7RWIc9sKZG0+tlD69U7kKjdoADUdHcSAAAAAKhL1leJccez23TLU1u1dU+vPnv+whqODGhuVGIAAAAAgE8ymdkTI5ZILSfZ2xur1ZAAiBADAAAAALIkreTJMADUCUIMAAAAAPDx705Sir5YQg+v3VnhEQGQCDEAAAAAIIu1VqGADMMWsUnJ525ZqXf+9DGt3b6v8gMDmhwhBgAAAAD4JJxKjI8uPajk577wapckaW9vvNLDApoeIQYAAAAA+KR6YhhdcdahOmS/sWVdg54aQOURYgAAAACAj3c5Sam9MYpYcQKgTJFaDwAAAAAAai2ZtLr2wXXa3TMgSVqxcU86vFjzSv7eFv3xhG5cvlljWyO6cNGMdOMMCjGAyiPEAAAAAND01u/q1tV3rlE0bNLhxSkHtweee8MTm/TJm57RR5YepI+fdaieWL9b//nHlZKkE+dPrdqYgWZEiAEAAACgaa3aulfnXfOQ/vdfjpEkfe/ti3XOEdMLPkeSnncqNAYSifRjA4nkMI0UgERPDAAAAABN7MblmyVJd616VZIUCRWeInX1p0KLpLNsxLvtajJp0z0xDJ09gYqjEgMAAABA04s5FRSRcP7goaO7X/c9v0OSlEim4oqkJ8TwBhpEGEDlUYkBAAAAoOm5IUY0nH+K5AYYkpRwAoukJ7lIWPYmAYYTIQYAAACAprR2e5d+8fAGSVLcSSQioez6iU+dsyDr2Pz2MUomA5aTEGIAw4oQAwAAAEDTeXLjbp35rfvT9932FZGASoxLT52XdawlEk4vJ7Ge4MJamw41iDOAyiPEAAAAANB0rrj+qYz7MacSIxrQEyOgOEMtYZNeOuLtiZFISm5rz0SSGAOoNBp7AgAAAGg6m3f3Ztxfv7NbUvDuJEG7jETDocHlJJ6ai2/89Xmt3NIpSbrukQ1atXWvQsbovCOna9KYloqNH2hWhBgAAAAAmk7IGHkXfGzbmwo19p/QVtTzWyIh9cbcrVYHj9+9+tX07Vuf3qpbn94qSeruj+uDp80f4qgBsJwEAAAAQPPxFVfEk1YtkZAmF1ktEfFWYuRo5nnKwVP1xH+eKUnqiyXLHyuANEIMAAAAAE3H3+fCWqmlwPaqXmEzWIGRa0OSCaOiah/XKklKJAkxgEogxAAAAADQdEIBfS4iAU09cwmHTLpxZ65tVd1QJBIabAIKYGgIMQAAAAA0naC4IqipZy4hY9LhRa5NSKJOiBEOGcXZqQSoCEIMAAAAAE1lT8+AugcSWcd3dvUXfQ1vJUaunhijWsLpc5OEGEBFEGIAAAAAI8S6HV269oGXaj2Muve357YP+RqhkNG2vX2ScvfEuOKsQyRRiQFUEiEGAAAAMEL8y08f01fvWKPOvlith9IwNlx9nkY7FROl6OyNqas/roF4MrAnxkWLDtD4tqikzKoNAENDiAEAAACMEP3x1A4Y/WznmVdLJHMaFA5o8lnI8XMnS5L64gkFxRPexqERQgygYggxAAAAgBGi1Zmc98Wy+z1gkH971XARu5IcPWtixv3xo1JVFv2xzEqMyWNa1BYN6ewj9ve8HiEGUCmRWg8AAAAAQGW0RlPLInoJMfLyBwoRJ9X4lxNm53zOHz98ov7rlpX6v8c26gsXHJ4OjAYSyfTuJKNbwvrGW47S6w6ZplCISgxgOBBiAAAAACNES5hKjGK4gcKfP3KypMGlH/mWlYRCJr0ta8gMLkkZiCfTu5M88MnTNXVsa9Zzw2FCDKBSWE4CAAAAjDDMl/NzdwqZ4CwJcSsxQv51JrkYo5ZwquolFWKkDodyhCAhY3Tzii2ES0AFEGIAAAAAI4Q7h+Zb//ySzucTdkILtydGrhDC5f1U26KpqdSHfrM83RMj17PbneqMtdu7yhwxABfLSQAAAIARJmjLz2ZmrdXVf1mjZzbtlZXVzq4BSYMhRiQUyrhfiJF0wrwpioSMtu3tTVe+5ApBPnTafP3rr5aJHwswdIQYAAAAwAhDJUamWMLqx/evyzruhhZudlGwEsPzsY5pjeiDp83T/96/Lt0Tw+Soc3cyEiVIMYAhYzkJAAAAMMJQiZEpnkwGHncbeQ5WYhR3PTfrGBUNK5G06o+nrp8rAnHDEX4uwNARYgAAAAAjTI45e9OKJYLDA7cXRro3RoFKjMyuGFKbs6XtP17aJSl/Y09psBcHgPIRYgAAAAAjDMsWMq3YuDvwuBtauMs9Cu1O4n6s7mar89rHyBjpobU71T6uVa2R4OmVG5KQYQBDR08MAAAAYIThG/9B+/pieu8vnkjfH98WUWdfXKOiYbU4oYMbShSqxPjE2YdqIJHUGxfPkCQtXbCfVn3hbCWSVq2RsCI51qOwawxQOYQYAAAAwAhD74VB+/riGfcvO/0gnXzQVE0d26qoL3QoVIkxdWyrvvXWRRnHRrcUnlK54Yjl5wIMGSEGAAAAMMLwjf+g3lgi435bJKQjZkzIODaqJdXbItdykKEKsZwEqBhCDAAAAGCEMOyCkaV3IDPECFry8eWLjtCyDbt13pHTh2UMboEHvUqAoSPEAAAAAEaYRJPtTtIzENf7f/mEBuJJbdjVo47uAf3ukhP09p88mnVuNJy9ZOSQ/cbpkP3GDdv42GIVqBxCDAAAAGCEabbJ8pbdvXp0XUfGsWvueTHrvDMP20+nHtJerWGlscUqUDmEGAAAAMAI02whhrtM40fvPEYf/u2TqWNOYHDRogO0ePYkvfu1B6aX21QbW6wClUOIAQAAAIwQ7hS92Rp7xhOp9+vdXSSeTK2peetrZunE+VNrMi4XW6wClTM87XeLYIxpM8Y8box52hizyhjzBef4XGPMY8aYF40x1xtjWpzjrc79tc7jczzX+rRz/HljzNm1eUcAAABAfWi2ybJbeRL2VFo4uUbWNqq14FZisMUqMHS1/C+6X9JSa+3RkhZJOscYc4Kkr0n6trX2YEm7JX3AOf8DknZbaw+S9G3nPBljFkq6WNLhks6R9ENjTLiq7wQAAACoA23R1P/ed/fHazyS6nJDm7CnaWfCqcSIhGqzhMTL7YnB7iTA0NUsxLApXc7dqPPHSloq6Ubn+HWSLnJuX+jcl/P4GSa1qO1CSb+31vZba9dLWivpuCq8BQAAAKCujB8VlSR19MRqPJLqSocYnkoMd4lJPVRiDO5OUuOBACNATXtiOBUTyyUdJOkHkl6StMda60bHmyXNcG7PkLRJkqy1cWPMXklTnOPevZO8zwEAAACaTldfc1ZieKsu0scCtlStNndY3777hXSFyMHTxumIGRNqOCqgMdU0xLDWJiQtMsZMlPRHSYcFneb8HfSvj81zPIMx5lJJl0rS7NmzyxovAAAAUM/ciXvT7U6SzG7sORhs1L4SY+LoFknS+p3d+vfrn5YkzZg4Sg9fubSWwwIaUu3/i5Zkrd0j6e+STpA00RjjhiszJW11bm+WNEuSnMcnSOrwHg94jvc1rrXWLrHWLmlvr/7e0AAAAMBwc7OLZmvs6faa8FZirNvZLUlqjdR+yjN5TIvGtaamONe8fbEuXHSAemOJGo8KaEy13J2k3anAkDFmlKQzJT0n6T5Jb3ZOe4+kW5zbtzr35Tx+r021971V0sXO7iVzJR0s6fHqvAsAAACgflCJkVmk/fHXH6KZk0bVYkhZJo9NVWPMmNimiaOiTRc0AZVSy+Uk0yVd5/TFCEm6wVr7Z2PMakm/N8Z8WdIKST9zzv+ZpF8bY9YqVYFxsSRZa1cZY26QtFpSXNLlzjIVAAAAoKm44UWzhhhhY7Rw+nit3tYpSfrAKXNlTO17YkiDVTIhYxQKmab7GQGVUrMQw1r7jKTFAcfXKWB3EWttn6S35LjWVyR9pdJjBAAAABqJOzFutm/50yFGyOgX73uNjv/qPZIGdwWpJyFjFDJGySb7GQGVUvsFYgAAAAAqIpnuiVHbcVSbG96EQ0bTxrWmj0dC9RNiuHlKOGQUDpl0Hw8ApSHEAAAAAEYItyLBNtkEOe7ZYtW7fCRcRyGGa7ASo9YjARoTIQYAAAAwQrjhRbN9y5+rsWe99MOQBntipCoxmq9vCVAphBgAAADACJFo9p4YdRRa5BIOpaoxmi1oAiqFEAMAAAAYIdwlCs32Lb+3sWe9M85yEmubb9kPUAmEGAAAAEAD+uJtq3XtAy9lHGu23UlueGKT5lx5u/piCUmNEWKEjUmPs0l+TEBFEWIAAAAADejnD6/XV+9Yk3HMDTGGMjn+x0u79KcVW4YytKr5+l2p99/RHZNUX7uR5GKM5A6zWcImoJIitR4AAAAAgMpwJ8XJIUyO3/6TRyVJFy2eUZExVYPbX8Lf2LNehdKVGIQYQKmoxAAAAABGCDe7aLamkbFEqhlIIzT2tHZwnFRiAKUjxAAAAAAajLch5Oqtnfrb6lf18q7udG+Ijbt69NSmPbUaXtX1x5wQI5wKB+o9ywhTiQGUjeUkAAAAwBB09ce1eXePFuw/vmqv6Z37nnvNg1mPr9vZratuXaU/XX5S2a+xeXePZk4aXfbzq6kv7jT2dNKLR65cqp37Bmo5pJyMSe1QIg3uJgOgeFRiAAAAAENw9rcf0DnfyQ4ShlO+b/DfcuxMLV0wTf3xoc2Q3/GTx4b0/GpKV2I4FQ7TJ4zSkTMn1HJIWca1pb4/DhmTbkD6h+WbajkkoCERYgAAAABDsGVPb9VfM1/Pi0jYqC0aUjxReojhfc7Gjp6yxlZN7seQrsSo48aeP3n3En3m3AWaOWmUli6YJkla88q+Go8KaDyEGAAAAECD8WYYR/sqDkLGKBwKldU0spH6TCaSVru6U0tGnljfIam+G3seMHGULj11vowxmjV5tGZPHk1jT6AMhBgAAABABQxlW9NSeSe/P/yXYzMeC4dSyxXiZYUYjTOp/umD69K3t+/rl9Q4W6xKKvtnBDQ7QgwAAACgAqo5IfWGDf7qA7fnQjnLSRopxNi2t6/WQxiScMgoQWdPoGSEGAAAAEAFVDMA8OYl/uKDcMgoEi63EmOIA6ui7v64pk9oq/UwyhYOGcUSDfSBA3WCEAMAAACogKpWYnhey7+EIrWcJDSil5Os2Lhbf1i+WaNbwrUeStkiYUNPDKAMhBgAAABABSSq+K16vuUk4ZBRuMzlJLZBVjfcsCy1NekZh+1X45GUr9ygCWh2hBgAAABABeTb9rTS3Lnv6Ye2K+QPMZyeGJ198TKu2xiT6q7+hOZMGa3PnHtY+tjnzl9YwxGVLkJPDKAshBgAAABABcSrOCF1w4YzF+6nkO//6EMhk15i8ui6XWVdV5KmjWsd2iCH0WPrdmWFN+8/eW6NRlOeVLVMY4RGQD0hxAAAAAAqoJr9DdywIWxMYCXGBUcfIEl6eVd3Sdf1VpO0RutzqtDRPaDt+/q1bmdp763e0BMDKE99/ssEAAAANJjqhhipv0Mm1f/CqycW15ypYyRJe3tjJV3Xu5qkXlc6dPenlslcckpjVV74hYzRspd313oYQMMhxAAAAAAqoKohhvNaxiirEmN394DGtIQVMlJnb2l9MbzLSaq5PKYU7hgPmz5eknTi/Cm1HA6AKovUegAAAADASFDVLVbd5SQhI18hhiTJGKPRLRH1xhIlXjf1dzRsVMbmJlXhfs5uBcov33dcye+zHhw9c6IeXruz1sMAGg4hBgAAAFABySqGGG7VR9ByEreYoi0aVs9AiSGGc91IKFS3O2ckPe9dkloiIbVEGq/APBwySlrJWitjApIoAIEa7792AAAAoA5VtxIj9XcoZHJOgEe1hPTEhg7ZErZNtRmVGPXZdDJhMysxGpU7/nr9nIF6RYgBAAAAVEA1J6NuMJFvHp9MSmu3d+mFV7uKvq4bEETDobqdXCd8lRiNKh1ilBAyASDEAAAAACqimpP+hC08kf/kOYdKkjr7it+hJOkNMep0cu2uchkplRh1umoHqFuEGAAAAEAFPLVpT9Vey534BoUYbvSw3/g2SVIsXvws2a3wiEbqfzlJpNFDDOdnV6+7wAD1ihADAAAAqIDP37pKe3uKr3ool7U2XTHhzuP/89zD1D6uNeO8aDj1v/qxEsKIwd1J6nk5SWrSH2rwECNEJQZQFkIMAAAAoEI27e4Z9te48qZndf73HpI0WIlxyanz9B9np5aPuKtAWtwQw6nE6OqPa1+BpSXp5SShUHrnjHry5MbdeujFXZIGKxkaVdgZfr0u2wHqFVusAgAAABXSHy9tS9NyXL9sU/q2ty+Ef0ofjaSOxBKpEOOC7z+kdTu6tXj2RL39NbP11tfMyrr2lt29Gc9NJK0i4foJC970w0fSt0MN/nVs2AmZ6rXiBahXDf6fPgAAAFA/YonqTkiDihGsBptzStKAE2Ks29EtSXrx1S79ZdUrgdfr6B6QJE0dm1qaUs1tY0vV+JUYbLEKlINKDAAAAKBC4lUOMfLtThJ1ShXWvLJPF0paPHuixrZG1Nkbyzlxdo/PmTJG0o708pJ61Pi7k6T+ZjkJUBoqMQAAAIAKiQ2xS+N1j2zQ+37xeNHnZywn8QUabqPP7v64JCmZtAoZo1DI5Awn3CagrdHUNKGuKzEaPMRwA6hkHX/GQD0ixAAAAAAqJGg704fX7tRxX/mb1rzSWfD5n791le57fkfRr+fNLc4/arretHiGrnzDAknSqJawJo9pSTf6TFircMgobHJvn/r3NdslSa1OmcBdK4OXnZTCWquVW/YO+Tp+jR5iuL1G4kmr7v64Ogs0XQWQQogBAAAAVEhQ5cJtT2/V9n39WrFxT8Vfz9sXoi0a1rfetkjTxrWlj4WMSY8pkUxN/HNVYnT2xXSPE2LMnzZWkvQfNz6jy367XMs2dJQ9xt88tlHnf+8hPfBC8eFMMfItpWkEIU9PjOO/eo+OuuqvNR4R0BgIMQAAAIAKcXcCqZZQgWqESMiklyskkkmFjVHISEGrXgY8VSRnLdxfl546T5J0x7Ov6KYnt5Q9xjXbUhUoL3cMbftZ/7KLRq/EcMeftFZdzpIfAIURYgAAAAAlevDFHfrcLSv12LpdGcevuefFrMn2cPZtLDSPD4e8lRjOcpKQCWwm6a3OCIeM2iKDUwU7hDdRqbfv7zfS6CFGJMTuJEA5CDEAAACAEv34/nX61T9e1tuufTTj+Es7uvWbx14OfM5wTLkLLakIe5aOJG2qciOUoyeGN6eIOMtOXJWYaH/2TytLOn/t9i6d9j/36Ud/f0n3PPdq1s4vjR5iuD+7v5fQAwUAW6wCAAAAJcu39eiWPb0Z920ZtQjJpC24VEQqLsTIqMQwqWNBlRXe9xRyGoC6hrJLSblRw3PbOvXyrh597S9rJEkrPvv6jMfDDd4Tww1h3PcHoDhUYgAAAAAlyru6Isdjpcy5c23V2hdLZNwvqhLDE2KkKzECQ4zM+94QJZZI6pG1O6u6g0bvQOZ77fW994avxGjw8QO1QogBAAAAlChfJYZfOe0k/EsnXJt3ZzbHDBX4v/mwMYo7gUjSWqexp1FQ/9F8jTNf2tGtd/z0MX3pttVFjD5TuTUc3QOZzS79IUajhwCRBh8/UCssJwEAAABKVM7E3JSwsCJo+caLr+7T476tTse05P/f+XBoMLAYbOyZHVhIg2HLN99ytKTMpqF7ewYkScs37i72LRT08NqdenbLXr3/pLnq7Ivp+/eu1bi2iF4zZ7JOPaRdf1yRuSNKT7+vEqPRl5M0+PiBWiHEAAAAAErQ1R/X4+s7cj5eib0m4gGlEmd954GMqo4Pv26+5kwdk/c6qRAjdS3v7iRBlSTuMbe6w7tUZevePkmVbU76vl8+oYF4UksOnKSfP7xedzz7iiRpwf7jdNzcyXpm896M8x/17QRTqAql3hVbSeIGTtWoPHm1s0879vXriBkThv21gHI1+H/6AAAAQHW9sre38ElDdOEPHs5qvunPHT502vyC1wmFjBJW+sZdz2tX94DCISOTsyeGM1l2wougnhOV3Ax0IJ4KV7oHEursHVw6MpBIqqt/8P4nzzlUkrS3N7MfR6TBU4xieno8s3mPFnzuLzriqru0qaOn4PlD9fEbntb533so4/MH6k1j/5cPAAAAVFmpPS7c07fs6Q3cFSTI5t296eqHILd/9GRNGBUteJ2IU4lx9+pXJUkXLjpAYWMCl5O4h0yeEGM4+Bt4JpJW3Z5JtLvsYmdXv8a2RrKON6piPt+NHT0aiCfVM5DQtjy/D5Xy0NqdkqoT1AHlIsQAAAAASlAohsgVVHz3nhf1kwfXFf28zt7cO4EUGzCEjVEiaZWwVuceub+OPXCywiGjDbt6FPMtWbHpSgt39CgAACAASURBVAw5fwe8RiVLMRz+HVcSSav3/eKJ9H33ve7Y16/JY1rSxxu8EKOoECbhCZuKDcAqIZajsSxQDxr8P30AAABg6H73+EZd8qtlRZ0b1E/ivSfOKeq5j7y0K+dj/uII/+Teq9idLVJbrKb6KrihxPi2VDXDw8637v7Xz7ecZDj0BFRi7HYaif74Xcemx/HwSzs1yRNiNPoWq0Hjf3TdLt20fLO++7cXZa3NDDGqODZ/wAXUk5qFGMaYWcaY+4wxzxljVhljPuYcv8oYs8UY85Tz51zPcz5tjFlrjHneGHO25/g5zrG1xpgra/F+AAAA0Lg+ffOz6SUXhSQD5ncHTGwr6rmJgGUc6ev6wpG+WO6JZLjIMoRwKLXFasLa9KT58tMPkiS99xdP6Irrn8p6fXduPZzLNbyTZH9/jkTSykp692sP1NmH758ed18sqTMXTNOXLjpCr503RW2R8LCNrxqmjm3NCjIuvvZRffwPT+vbf3tBa7d3ZexSU8q2vkNFiIF6VstKjLikj1trD5N0gqTLjTELnce+ba1d5Py5Q5Kcxy6WdLikcyT90BgTNsaEJf1A0hskLZT0ds91AAAAgIqyAd+Jh4xRMXP+fPNQf8Dx19Wv5Dy32IAh7DT2jCcGQ4xIeHAKcPOKLenXdSfJbk+MYnbD+J+71uiPKzYXNRYvb/VFMmkzPrtEMlWB4K8IOe2Qdn3kjIP1rhMO1O8uPaEqu3UMp/0ntOmZz5+lNV86R28+dmbW490DiczeJVUsxRiIs5wE9atmIYa1dpu19knn9j5Jz0makecpF0r6vbW231q7XtJaScc5f9Zaa9dZawck/d45FwAAAKi4oCDCGJNe4uF/3Hs/kbSy1gb2N/Af+sXDG3KOIRwuIcRIJpW0Nh18+AMQ91t361tOMm1cqyTpsOnjc17/B/e9pH+//um8Y9ixrz/rmHepTNLazM/IWiWTg6GLO95oeOSthB/TGlFbNJz+rL16BxIZVSosJwFS6uJfAmPMHEmLJT3mHPp/xphnjDE/N8ZMco7NkLTJ87TNzrFcx/2vcakxZpkxZtmOHTsq/A4AAADQLIJCjGjYpLf8tJI2dfSkKxy8lRtJa3XRDx7WQf95Z9Y1/Msq/vXkuTnHUEpPjEQyFZ64oYB/JcqAM2H1Lyc59ZB2PfaZM/THy05Mn1vORNq/TOf/HtuoD/1mefr+F25bnd4VQ5ISiVQjUvc9uuNujdTF1GVYvOmY7EqMvngiozqH5SRASs3/JTDGjJV0k6R/s9Z2SvqRpPmSFknaJumb7qkBT7d5jmcesPZaa+0Sa+2S9vb2iowdAAAAzccfNkipKgF30r2xo0enfP0+ffvuF7LOS1qrpzfvDeyN4Z+k5mtcGbhzSIDU7iSpSoyQLxRwxeJuiJF97f3Gt6ktOrTeE/uNH6wysNbqpic3a+32Li2ePTF9fIqnYWfCaWjpjneMs63qzMmjhjSOenbQtLFZW+aueHm3b3eS4Xntrv64bli2SdbadIBFiIF6VtMQwxgTVSrA+K219mZJsta+aq1NWGuTkn6i1HIRKVVhMcvz9JmStuY5DgAAAFTcnSu3ZR2LhEx6icemjh5J0uPrO7LOy9PXU9Y3b8zXBLToSoywSfeYcJdl+AOQeFZPjNzX6x3IvWNKLotmDYYV1krxRFLHzJ6k3/7r8enjV11weMZ4vOM987D9dN37j9NHlh5c8ms3kqhviVB/PFmV3Um+eNsqffLGZ/T4+o7078YAW6yijtVydxIj6WeSnrPWfstzfLrntDdKWuncvlXSxcaYVmPMXEkHS3pc0hOSDjbGzDXGtCjV/PPWarwHAAAANJ8f378u61iqEiP1v9ZrXtknSXp8gxNieOaDy1/enfO6/goP784U/fHM8KDonhjGE2LkqMQYiLs9MdzlJLmv/UpnX1Gv6+WfiMcSVtGwyXgd71KRRNIqaQfH2RIJ6bRD2jXWqcgYqSK+dT69sYSvEmN4goWdXantbPf1xdPVL251DlCPalmJcZKkd0la6ttO9evGmGeNMc9IOl3Sv0uStXaVpBskrZb0F0mXOxUbcUn/T9JdSjUHvcE5FwAAAChJoYliPEeZfTQcKrlngz+Y8C8n8d6/6tbVGY8VuztJJGTSyzP8jTJdsUTu5SSuliH0o/BOxG95aotWb+tUJBTKDDE8S1bc8/MtpxmJ/O83q7HnMBVHuK8bTybTy0ke9vQoAepNLXcnechaa6y1R3m3U7XWvstae6Rz/AJr7TbPc75irZ1vrT3UWnun5/gd1tpDnMe+Upt3BAAAgEaXbwmHJPXl+IY6EjYa3VJa74g7ns1cluIPMeJJqzWvdCqWSGrllr0ZjxU7wQ+FTLpR5mBjz8znnv2dBzQQT6aXwQRd+q5/O1XTJ7RJkp7atKeo1/a+D9cVN6R2Mrn/hR0Z7yEoAGq2EMO/nGTr3l51OFUSUvDWvpV83bhnW9ubV2wZltcCKqHmjT0BAACAehHUtNPLuzWoVzRsdPbh+w/ptZO+fOSZzXt0znce1PfvXZsVcBTdE8MYZ8vS3MtEYgmrjR3d6YBhdMCyjblTx6jfCXC+f+/aol7bFbSrRm8skRGWBL2fYpuXjhQR3xayD6/dpZ8+tD59f7gqMdxlLI+t6yi6wgeoJUIMAAAAwOEPEvxyNbaMhkO64vWHBD4WydG/4vZntmnV1sEKC/9kf+e+1Lfwy1/ene5b4Sq2SiEcNnq1s18DiaTCAf/n/1/nHSZJ6uiOSZLecfxsHT1zQuC13GUn/l00ConnaBJpPBNmEzB5LjaoGSnc9/upcxbojo+eosMPGJ/xeIEiobJYa3XPc6ktcH/96MtZVTpAPSLEAAAAAByFKjFybT0ZDYcCJ4B9sYRWb+vU1LEtWY/97bntOu+ahwZfOxncE8PK6sXtXRmPBU36g3i/WT/poKlZj7vNMt2Q5IR5U3Je2w1OSv2yvtASHe+1vZptQu2GXYtmTdTCA8Zr8pjM35nhaOx505Nb1O0J5pptCQ8aEyEGAAAA4EgU2Foy13w8V+PLD/56uVZu6VTSFq4syA4xUn8PZe7qTkrboiGdOD87xHCXMLhNRvMtJ3DH776NZJGlAYWCIe81vfw9Ika6sLOsI9cuMsOxmuR7976Yvn3B0QeouT5xNKqRvU8RAAAAUIJCE+6g/g6S1BK0VkOpBpZSaleTaDikeDJ4OYqU2QDT+1rWSkfPnKCJo1vS1ytWrh1JXG5Q4Pa7yPE2JA32qDDOVDfos1q1da/Gt0U1a/Lo9LGgSozj507OuvYv3vsazZ4yWvc+t12dfTGdc8TQeow0mlHR1IfvVlz4f2aVLsRYsXG3Xt7Vk74/uiWc/pkGVQ4B9YIQAwAAAHAUWvqQ6/FC26taZW+p6tqxr187u/rTIcEP33mMvnX3C1rrLCFJWqtYwpZVmRDJ8a3+4OOZlRj5lqm4b90NV/yfxfbOPp13zUNqiYT0wpffkD4e9Jl95+JFGffDIaPTF0yTJM1vH5tzDCPZR884WAPx53XQtNT7z6rEqHCKsa8v7ru+92dFTQbqFyEGAAAA4Ojuj6t9XGvOx4Mm5EfOmJCeeOZkcy9FOeObf1dnX1y3f/RkSanJ676+mPepiieT6cChFKEcIcbX33yUFuw/Tlv39Eka7ImRbznJ/PYxGWGL/7PY1x/PuJYr6DObNDrzm/5m24kkyInzp+rmywaX/Az3chL/9axs+mc1HP03gEqhJwYAAACanjtf3NnVn/c873KSBz95upb/15m67SMnpysYbr7sxMDn5ZsSdjrfiLu7eERCRpecMi/jybGEVbRAtUeQXJUYb10yS0fNnJiu7vjrqlcDz/P68buOlZTqryEFLH/JkdL4Q4w/f+RktUXDGcfyLWNpVv7GpsOdK3grMXItmwLqAf9cAAAAoOmNaUkVKHf1x/Oe552Qt0RCmjI2s2pjlG9y7ipmUuiGApFwSBM9lQpWVrFEUtEydo5wKxxyVTps3dMrSbpnzfbUeXleY+LoFrWPa9WAE7b4Q4tc/US8n9k5h++ftXVovvE1M39VTKWDBX+1hfW8xnBs5wpUCiEGAAAAmp47eY+XsDtJUNVCrh1I+uPBW7N6xZ3tWyMho9Etg2FI0qbGFRlCT4xc5kwdk3G/UE7SEg6lt5n1V2Lk6hfiDTc+908LA/tuEGJk8/9+feR3KzTnytuHbakHlRhoFIQYAAAAaHruhPHy/3tS2/b25jzPO7kLCghyNcYs1DBU8u4QYjRxdDR9fPnLu/VKZ58i4ZDe89oDNduz80ch+SorJOnoWRMz7ufriSGldjNxQwz/e8r1HuMJq4sWHaDHPnOGDpg4KvCcfMtYmlWuzyRWIGgrV9LaweatlGKgjhFiAAAAoOm5E8b+eFJfvv25nOd5J+r+vg7e6wR55MqlecfQM5BayhIJGR08bVzW49ZafeHCI/TAJ0/Pex2vQpUY/scLhR5RTyWGf/lIzkqMpFVLJKT9xrflvC6FGNlyBUrxZOGqnmL4f1reyhoyDNQzQgwAAAA0vUIVCC7vN9RB26rmywByVSG4egZS25xGwqHA7VTHtUWzjhVSqMLBv4yj0LKOaDikgbizxaqvIsBbpeJd8pCwVuEcO6ucc/j+kqTRLWya6OcGSv4fYcUqMXyXiXmWPLGcBPWMEAMAAABNL2Oyn2f+5q0+qHRvh+7+wUqMoPBhTBkT/ULj8VdiFNolJBrJV4nhvW0zbue67tffcpT+dPlJmjymJfiEJuZ+Zv6tdeOJSlViZP78BpzrhkyqIunJjbsr8jpApRFiAAAAoOkV25OhUG+LQssx8tndE5OUWqYSDZj1l7MNqdsMNNew/e+7UOjREjYaiLs9MTIn097PJu4LMfwTcdf4tqgW+fpyIMX9CAd8oYW/oWqlbNjZLSlVCSRJb/rhI8PyOsBQEWIAAACg6RUbYrjFBx874+Dg6xQIAS45ZW7Ox7519wuSpLZoqKSmofm4Sw92dvUHPu6/ZqHPIaMnhq8gwLsEwXs7kbTsPlKGkw+aGnh8uEKMdU6I0epJy/b1xYbltYChIMQAAABA0/OGBv4yey+32uCMw6YFPl4oC5k9ZUz+E5SqxAgKE8oJAo6Znapy8G7Zmk8xPTHcEGNvb+YE11uJsacnpr5YQk9u3J2qxChje9hml2sXmkotJ+mPBV9nn7OsSZJ+cN9LFXktoJIIMQAAAND0RrcO9pu449lXcp7n9oHINdkvp1rCry0aDrxOOZducZqPjmsr3E/joGljNX1C7h1EJKexp1Pd8dYf/yPjMW+PjFVbO/X5W1bpTT98RL2xBJUYZcgV/Ny58pV0/5ShWLFpT/r2aYe0B55Dg0/UI9oAAwAAoOnZIidrbrVBrglmruUY/3H2oZJSPSUKGRWwdatUuMojSFBvjVz+dsVpBc9piRjFEsn0drDS4Hv27txyya+W6aiZE9L3C231imy5liZdfecaXX3nGk0d26LRLRH93yXHa+ak4KqNfLxbBOcKr/ipoR5RiQEAAICm5//G+fv3vqg5V96edZ67lCJXOBA0Vz/90HZdfvpBkqQ3Lp6ppQuCl6JI0ncvXpQzCJk6tjXn83Jxw4NKfaHuLidZt6M7fcwNK/y9GrxhzFAanjaroM9swf7j0rcXz56kjR09enlXT1nX9zZm9b5WS8DWwUA94TcUAAAATc+30Ya+8dcXAs9zG2VGc+y2ETTxPHLm4O4bLZGQPnPuYTnHceGiGTkfe+Pi3I/lUkolRjFawiHF4pkflhuQJH0hxmPrO9K3504tvVKg2fmrV+ZOHaM7PnpK+v4HT50nqfCOObl4W2t4qz4+dNr8sq4HVAvLSQAAANC0PvunldrV3Z+zlae1NqM/hdtUMRoJriwI6v3g38mk2KKEOVNGa4PnW/Zy+m24y17yPXXymBbNytFE0i8aGeyJ4XLvJXKUe0we06ILjy49gGl2/t8lYzJDMvf3ody+Fd5KDG/1z7hWpoiob/yGAgAAoGn9+tGXJWWW6XtZmxkAuMtJIrkqMQLCAv/ykGK3c63EEgxTRFeD+z7xOrUUWbHREg6pP57IPOhMooMqAt574hx9/p8WVqThabPx913xhxrpXiRlhhje5T/e38mo93X5saEOEWIAAACg6eWaCCatVcgzk3OXk+Sa9HsnmucdNV1He5pbuoKWeMycNErvfu2BOa81VPmuNWFUtOjrjGoJqy+WyPi83Llw0Gc4ri1CgFEmf2PPUw9O7SAydWyLZkwclQ7M/EuhiuUNnbxLV6KenhjFhGBAtRFiAAAAoOnlaivgP56uxMixy4g3LPjBO44JPMdtnGiM9PV/PkqbOnp0xVmHBlyr0KgL2298qz5w8ly9dcmsoV9M0uhoWLGEVc/AYDWGldVAPKl4IvtDLLbCA9m8lTgPfvL09A4i93z8dYqGTbq5aq5lPIV4KzFCGZUY/MxQ3wgxAAAA0PRyVWKs3d6llojRQdNSy03ciV+uiV4xS0XcEMNa6S15woVKVGIYY/TZ8xcO+Toud7L7pT+vTh9buaVTh/zXnYHns9NF+bzVEd6eJW7ljPv7Uez2wH4JT+jkrfpo5WeGOsdvKAAAAJqetdLxcydnHT/3mgd15rceSN8fiLtbrOaqxCj8WsVWJ3zhgsN18LSxRZ1bLWNaUtumrtramfOcz5y7IH2bEKN8hXqiuIFZoszlJEE9MY6ZPZFKDNQ9fkMBAADQlPpig0siktZq+oQ2HVQgNIgnk4qETM4+D+7x9500J+c1ig0xjp83RXdfcVpR51bLxcfNljQY4rRFs9/LRYtmpN//mBYKv8vl74nhl+6JUcHdSQ7df1xGiGFz7tsD1A7/qgAAAKAp7emJpW8nrVXImHTPi1xiCVvwm+r1/31u3sdL3XVkzpTROn3BtJKeM1zaomEdNn28ntuWqsRYPGuS/rFul6aMadGu7gFJqff3768/RK2RsC5azNaq5RrdEtbblszS6w5tD3w8VMTuJNZa3bXqVb1+4X5ZS528lRjuEpJ4wmrGxFHp44mAPidArRFiAAAAoCl5l4QkElYyg8tFcoklkjmberqK2Y3jvSfO0VkL9ytqnPd94nV1tcNHi+f9f/HCwzV94ij98uH1+sZfX5CU6uUwvi2qK9+wINclUARjjL725qNyPu72xMgXYvz5mW36yO9W6DPnLtClp87PeMy7O8nYttS0cCCR1GHTB7cbjufqeAvUECEGAAAAmpJ3ftYfTypkTMbELkgskazIjhtXXXB40efWU4AhSRHP+2+JhDS2NZJRnVJqpQnK4y43ybfF6o59/ZKkrXv6sh7zBhTuVqptkbCMMZo8pkUd3QOKl7t/KzCM6IkBAACApuT9BrsvllDISNe9/7i8z4knbMFKjJEuErAdpzfYKNTLAZXhfszFbLEatIOJN7A79sBJOmBCm954TGr5j/szDto2F6g1QgwAAAA0JW+I0RtLKGSMDps+Pu9zBhLJpt+9wfv+3UDHu8SkmG1mMXTu51zuFqveSoxD9x+nRz59hk6YN0WS9M23Hi1Jmtc+ZoijBCqvuf8FBgAAQNPyrhxJ2sLLNl7a0aXVWzsJMTyBRUtAJUaISoyqcD/nfL1o8/0oEnmWihw1c6IkKRxq7t911Cd+KwEAANCUkr7+F24BwZmHBTfcPOOb92vNK/sK9s0Y6aY7u1eMb4totLOF6pQxLenHqcSoDjdfyNfYM99PIt9SEfdHWG6VBzCcaOwJAACApuSf/LnfbF/7rmN13T826Au3rQ58XoezlWiz+vKFR+ijSw/WuLaIWpytOU89ZHAbUEKM6ihmdxJX0Bn5wrjBKg9CDNQfQgwAAAA0Jf/8zJ17h0JGo1vCeZ7X3BO7UMho/wltGcdaIxR4V5vbQPW5bftynpNviZTbE2PRrInZ1w65AclQRggMD/61AQAAQFPyf8vsnfD5J38rt+xN3272ECNIvW0D2wzGtqW+j97XFyvr+Ymk1dIF0/Sny0/Kesz9cfK7jnpEJQYAAACayq6ufr3c0aNxrZn/K7y7Z3CZiL855fnfeyh9m2+nUQ+i4ZCWHDhJf35mm/78zO2SpO+/Y7HOP+qA9Dnur/Hj6zuynh9P2pxNWN0qD3/fGKAeUIkBAACApnLhDx7Wm374iBK+b5mPOGBC+na+tg40O0S9mDAqmnH/2gfWZdx3d9IZ15b93XUimVQkxy96uicGv+sF3bR8s87+9gO1HkZToRIDAAAATWXz7l5Jkn+HybZocduE8uU06kVbNLN3y1EzJ2Tcjzv7rwb9zsaTVuFwjhCDnhhF+/gfnq71EJpOySGGMWaMpHdIOljSFGXv3GOttR+owNgAAACAYZN3a8o8lRj0CUC92LGvP+O+u+Wty23eGfQ7m0zanJUYUqoaiaqj4llr6Q1TJSWFGMaY4yTdrlR4kYuVRIgBAACAujaQyCzFmDZ+cMeNfLttMK9DvXh8Q3avC694wg0xAh5L2rzb4YaMYYvVEiSSVpEclS2orFIrMb4lKSrprZLutdbm/68GAAAAqFNv+uEjGffPWrhf+vbrDp2W83l82Rps0ayJeSfFqLzWSEj98cEwzv/pu5UYQRUViUKVGCHDcpIS8FlVT6mNPY+V9E1r7Y0EGAAAABgpWiOhjFLwtmhYnzpngSRp7tQxGeeGSTEC3fih1+r6S0+o9TCaymfPX5hx3185kUi6PTGyZ9ipSozc08GQYelUKfisqqfUEKNT0q7hGAgAAABQK0Fl8x86bZ7WffVcTRqduQNEiGqDQJFwSJEwmx9W02mHtGfcj/t+j2POcpKVWzq1auteLX95d/qxQpUYYWPYYrUEZBjVU+q/MjdLOns4BgIAAADUyuQxLVnHjDEKhUx6m0rXfuNbqzUsIC9/D4aYr8+LN5w775qH9M8/eiS9tCSeSBbuicHMvGh8VtVTaojxKUnTjDHfM8bMN7RfBQAAQIP77PkLdeOHTsz5uD/E+N0lLJlAffCHEL9/YlNG9UTMv4+wpK/e8Zy+9pc16uyL563E2Ncf121Pb6vcYEc4lpNUT6mNPfcotfvIcZIukxS0jYy11pa8dSsAAABQC+88frbaouGcj0d933bPnDR6uIcEFCXi62mRSFpt2NWtee1jU/cT2RPrnzy4Pn07XGA3jZ1d/XkfxyCbnRdhmJQaNvxKqRBjyIwxs5zr7S8pKelaa+13jTGTJV0vaY6kDZLeaq3d7VR9fFfSuZJ6JL3XWvukc633SPov59JfttZeV4kxAgAAYGR5eVd31rFCtcXsuIF65f5uRkJG17x9sS777ZMZWwf7e2T45avEcF116yp94OS5mjWZ8C4fKjGqp6QQw1r73gq+dlzSx621Txpjxklaboy5W9J7Jd1jrb3aGHOlpCuVWsbyBkkHO3+Ol/QjScc7ocfnJS1RKmBZboy51Vq7O+sVAQAA0NQ2dvRkHQsVSDFYQY165YYQ1nM77qm+iAcsJ/HKtzuJ65ePbNCzW/bqpg/nXnIFemJUU83aB1trt7mVFNbafZKekzRD0oWS3EqK6yRd5Ny+UNKvbMqjkiYaY6Yr1Wj0bmtthxNc3C3pnCq+FQAAADSIrr541rFCIQaFGKhXbiWGtTbd5NNbfREPWE7iVUwlBoqz5Mt/04ad2ZVeqLySQwxjTNgY825jzG+MMXcbYxY7xyc5x2eUcc05khZLekzSftbabVIq6JA0zTlthqRNnqdtdo7lOu5/jUuNMcuMMct27NhR6hABAAAwAgwksr+ZLjSP84YcbVG2EEX9CGdUYqR+Ny/6wcP666pXJBVeTnL6odPyPu5/HeR36a+X1XoITaGkf4WNMaMl3S/pl0pVRiyVNMl5uFPS1ZI+XOI1x0q6SdK/WWs7850acMzmOZ55wNprrbVLrLVL2tvbA54CAACAkS4W8M10oeUi3hBjbGu04mMCyhUJGV166jz96bKTMqoq7nt+u6TUNqq5fPSMg3XkzAlFvU5nb2xoA20SHd0DtR5CUyg1Sr5Kqd4Tb5Q0T54AwVqbkHSzUss7imKMiSoVYPzWWnuzc/hVZ5mInL+3O8c3S5rlefpMSVvzHAcAAAAyxPJM6nLxZhzj2tiED/XDGKPPnHuYjp41URHPVsBuK4x40mre1DEZz/nV+4/TD995jK54/SF5r/1/lxyfvk2IURz/bjEYHqV+ym9RaheRW5TaUcRvrVK7ihTk7DbyM0nPWWu/5XnoVknvcW6/R9ItnuPvNiknSNrrLDe5S9JZznKWSZLOco4BAAAAGcoJMS45ZV769ufOX1jJ4QAV413y4S6bSiQHe2W4Tj2kXeceOb3g9Q6aNjZ9+4T5Uyo0ypGNZTfVUWqUfICkp/M83iNpXJHXOknSuyQ9a4x5yjn2GaWWpNxgjPmApI1KBSeSdIdS26uudV7nfZJkre0wxnxJ0hPOeV+01nYUOQYAAAA0kaDlJIUcPWti+vbpC4rrIQBUW9QTVvTHE7LWKpawRe1AEsRbVTBz4qghj68ZsJFRdZQaYuxSQNNMj8NV5FIOa+1DCu5nIUlnBJxvJV2e41o/l/TzYl4XAAAAzcuyDSJGqIxKjHhSR131V+3rj+uoIvte5LteoQahSJkytrXWQ2gKpYYY90h6nzHmG/4HjDFzJb1f0q8rMTAAAACg0sgwMFKNbhmc2m3s6NG+/tR2wtZKf/7IyVq5Za9eM3dy0dfzhhgJQgzUkVJri76g1G4kTyi1C4mVdI4x5r8lPSmpX9J/V3SEAAAAQIUky0wxzjyMZSSob+3jBqsAXni1K3372S17dcSMCbr4uNma3z426KmBIlRilOzpTXt0x7Pbaj2MEa+kEMNau1appR5xSV9UajnIJyR9StImSWdYazdVepAAAABAJSTKDDF++p7XaMPV51V4NEDljGkJ6y3HztRlr5uvR65cOuTreSsxtu/rH/L1VIP6wAAAIABJREFURqqWSOaU+rLfPqk/rthco9E0h5L3iLLWLpd0tDHmCEmHKRVkvGitXVHpwQEAAACVxHISjFTGGP3PW46u2PXCni6Vtz29Vd97++KKXXskaQ2HNBDP3PXo369/Wvv64nr3a+fUZlAjXNkbXVtrV0paWcGxAAAAAMMqSVk8UJQQ24UWJdcStc/dsooQY5iUtd+OMeYAY8wHjTFfc/580BiTb9cSAAAAoObKXU4CNKOHPnV6+nYskcxzZvOiX0j1lRxiGGM+K2m9pB9J+g/nz48krTfGfL6ywwMAAAAqxz/fuOvfTq3NQIAGMHPS6PTtr9z+XA1HUr/yhRhs6Tw8SgoxjDH/T6kdSp6S9A5JiyQtlvRO59jnnHMAAACAuuOfVBw4ZXSOMwF4PbpuV62HUHestUokrT52xsGBj1OlMTxK7YnxEUmPSzrZWhv3HH/aGHOjpIedc75fofEBAAAAFZNgUgGUZVRLuNZDqDuxROrfk2g4uH9IPGEV5WOruFKXk8yW9DtfgCFJstbGJP3WOQcAAACoO94M4ytvPEJtzDAwQl39piMrej2a4mZzQ9FwKHha/bZr/6GTrr43a/cSDE2pIcZGSePyPD7OOQcAAACoO+5ykkeuXKp3Hn9gjUcDDJ+Lj6vsd8ssjcgWT6bCiUiOnVye2bxXW/b06pW9fdUc1ohXaojxfUkfNMZM9z/g7E7yIUnfq8TAAAAAgEpLJK1Gt4R1wMRRtR4K0FBYipXNyTAKbkdr2K22okrtibFX0quS1hhjfiNpjSQraaFSzT1fkNRpjHm390nW2l9VYKwAAADAkCStFGZGgSbx2fMX6uiZEypyrSQ7bWRxt2z2VmJ88y1H686V2/S357anjw3UYHvaR9ft0sXXPqo/XnaiFs+eVPXXH06lhhi/9Nz+cMDjx/rOkVIhByEGAAAAai5pLd+Koml84OS5FbvWnp5Yxa41UrjLSUIhozs/doqe3bJX/3zsTM2ZOjozxKhBT4y/P79DkvTIS7uaPsQ4fVhGAQAAAFRB0tqCpd8ABn3tn4/Up256ttbDqEvucpKwMTps+ngdNn28JGnxrMzQoL+GjT1HYmhbUohhrb1/uAYCAAAADLektSwnAUrwttfM1sNrd+mZzXtqPZS6E7ScRMrukfHS9i5Fw0aHH1CZpT3FsBq5y39KrcSQJBljRkuaIGmvtbanskMCAAAAhkfSSoYQAyhJayRU02qCeuVuO1uouuvjf3hakrTh6vOGfUx+RiPv37uidycxxrQbY75ujHlJ0j5JmyXtM8a85ByfNmyjBAAAACogmbRiNQlQmtYoIUYQd9vZcMCsOsw/NMOmqBDDGHOCpGclfULSTEmrJD0iaaWkGc7xp40xxw/TOAEAAIAhS1rL5AIoUWskXJPmlPXO3XY2FFDddcvlJ+n0Q9urPaRBI3c1SeEQw6mwuE1Sq6TLJU201h5lrT3FWnu0pEmSLnMev42KDAAAANSrpA2ecADILbWcJFHrYdSVOVferi/ctkqSFAllT6uPmDFBn/unw6s9rKZQTCXGJySNl3SGtfZH1tpe74PW2l5r7f9KOtM57+OVHyYAAAAwdMkkW6wCpWqNhBVL2HTlQbOLJVJVKQ++uFNS8HISSYqGa/ePzUj+SRUTYpwn6dfW2ifzneQ8/htJ/1SJgQEAAACVxnISoHSt0dS0kSUlKX2xzKqUeq7uquOhla2YEGOOpEeLvN5jkg4sezQAAADAMGI5CVC61khq2siSkpReX4iRKxidMXGUXjtvSjWGlGUk/ytXTIiRkBQt8noR53wAAACg7iQsy0mAUrWkQwwqMSSpP5b5OeQKMYwxuuz0+dUYUpZmX07yoqTTi7ze6yStLXs0AAAAwDCy1ipMigGUpDUSlpQ9eW9W/uUk+ZaoeR+7+cnNwzamXEbiv3bFhBh/kvQmY8w5+U4yxpwt6U2Sbq7EwAAAAIBKSyZZTgKUyl1Osqd3oMYjqQ/+ipR8waj3sStueFrrdnTp3jWvytrhrZUY7uvXUjEhxnckbZL0J2PMV40x87wPGmPmGWO+qlTYsVnSdys/TAAAAGDoWE4ClG50S6oS44LvP1zjkdQHf4gRylOJEUtkhgk/vn+d3v/LZbpr1SvDMrZmECl0grV2n1NlcZukKyV9yhizT9JepbZUHa9UlcpaSRdYa/cN43gBAACAsll2JwFKNrql4LSxqfh3aYnk+Telqz+Wcf/6ZZskSVv39FV+YAFGYmhbTCWGrLUvSFok6WOSHpIUlzRdqSaeD0r6N0mLrLVrhmmcAAAAwJCt2LhHe3pihU8EkOZWYiBlIFF8JUb7uNbA44nkcC8nGdbL11TRkZq1tlfS95w/AAAAQMPZ1c2afqBUY1oJMbz6/Y0985Q7HHvgZF31Twt11W2rM47HhznEcJkR2NqzqEoMAAAAAEBz8i4nGckNI4vlr8QotETtiBkThm0sd69+Va92Zi9NGck/JUIMAAAANAUmX0B5vMtJHnxxZw1HUh/8PTEKhRhBy01aIkOfiltrdcmvlumNP2iuhqv/n737DpOrLP8//j5Tt5dkd9N7JZBGAgRCS+hdFBQVpIggiOWnooAICiqIoH5RUBGkKKA0Aek1EEpCeiV10zZtW7bv9PP7Y8rO7Oxk2+zObPbzui6unXlOmWc37Nk599zPfSuIISIiIiL9QusuASLSMdGZGGv31KZwJumhs0GMtrY6kxDECK9I2VMbn4kRrrlxKBYyVhBDRERERPqFni6kJ3Kois4aeOzj7TS4fd06n9cf6NMtRuMKe7bTAqStS08yMsMOdk3zhuZotyqIISIiIiLSJ3kDgfZ3EpE2rbr9dADK693c++bGbp3r929v4tp/LuOjPro0xe3tXCZGWwGLZMRUo4MYgajH8+9dwJOLd3b/BdKUghgiIiIi0i/4tJxEpMvyM+2Rx+u6uaRkR1UjADXNfbNbUOtMDFs7QYy2AhaBZGRiRJ2j3tWSHVNa2Rh5vG5PXbdfJ90oiCEiIiIi/YLPr0wMkWRYsv0Ae2ubu3x8pF5DO8sw0pW7VU2Mtgp3RosOWHx59nAAKurd3Z6HPyowO/2Ot9hS3hBXr+PfS3Z1+3XSjYIYIiIiItIveFUTQyRpfv7iui4fG44ntnfzfzCPfLSN37+9qcvHd0frQEF7mRjDCjIBuPmsyfz83CkAPLhga7fn4W+VzfHNx5fw3ob9cfsdap2ZFMQQERERkX4hnIlx38XTUzwTkb7polnDI4/f+bzlZnnZjmpeXrWnw+fxh+rTdCcT485X1nP/u5u7fHx3tA5itNdpZMSALJbdeirXnDi23SKgndG6sOeOqia+/a/lABTnOgG49ZzDOMRiGNja30VEREREpO8Lt1i1HYLV+kV6w+8umsZvvzSNcbe8BsD6PXVMGZrHl/7yKQDnTx/aofOEV0FY++jvotvnj3nutFnbPWZgTjCo0JNBjGh/vfRIZo0akLTXSifKxBARERGRfsEXCLcc1Ftgka4wDCOmE8cNTy+P2e7tYN2ZcCZGe8sw0lXrTAxHO5kY0ZJZBqT1cpJoVsuhe507dL8zEREREZEo4e4kffXGSSTdlNfFFqds8gQzFJZur+Z/B1le0tcLe7buTtJei9VoyczECBwkEyMZ3U/SlZaTiIiIiEi/EP6UWJkYIsnR4PbFLGl4eGEp9S4fj32yHYDzEiwvCSShsGeYaZoYvRwMcXn97e+UQDJjqL7Qz/7n507hzlfWx2w7lIO1uoKLiIiISMqYpsmvXlnPxn31Pf5a4Tf8nfnUVETifXf++MjjrRUNkcd/em9LJIAB8OTiHTz+yXa++/QKdlQ1RsYrG4IZHMn4TVy4uTIJZ+mcLeUNTB+ez6iBWVwwo2N1QMJ6oiZGUY4jZnz68HymDstP2uukG2ViiIiIiEjK1DZ7efijbTz92U7W3XFmj75WOBNDhT1Fuue78yfwp/e2AHDdv5Yl3O9n/10bebxpXz1fmDmM604ex95aFwDJ6HqczKBARzW6/cwYUch9X+58p6NkTje8ZMRmsXDL2ZP5zWsbALj/qzN7PTulNykTQ0RERERSJvxJYqOn6+nZnX0tLScR6Z7opQpbKxoPsmeLjfvr+e0bGzBNk+bQcgwzCXUbsp3tdwZJNl8ggNPetetIMoML4To/Vgtcc+K4yPiogdlJe410pCu4iIiIiKSMLxkfxXb0tVTYUyQpulPLIrqGRi/++ieV129iT4PrSDgTI9yJ5JxpQ3D0gyCtlpOIiIiISMp0tCVjMl9LmRgiyXfFcaNj6mEk4osJYnQ/ipGKLhz+gJn0Fqa1TV7ys+ydOqalzk/w+QNfOzKpc0pXuoKLiIiISMp4/b2YiRF6w6+aGCLJF72s4+RJxQn3iw5cJiMAkYrCnl5/AHsSryPPLNnF9DveYvP+zhU4Dme1pKIuSCqlLIhhGMY/DMMoNwxjbdTYLwzD2G0YxsrQf2dHbbvZMIwthmFsNAzjjKjxM0NjWwzDuKm3vw8RERER6TpfCjIxbEn+BFVEYNLgPABuPecwbj1nSsL9ogOXyUii+OM7m7t/kk7yBcykBkPfWr8PgG2VHasvEhZd2LM/SeVykseAPwNPtBr/g2ma90YPGIYxBbgEOBwYCrxjGMbE0OYHgNOAMmCJYRgvm6YZ2yRXRERERNJSr2Zi+MOFPfvXp5YiPWn2qEK+PHsE508fyjlTh2C1GBxo9CTcv7bZG3nc1UyMRAVBTdPkvQ3lzB49gPzMzi3N6MxrJ3s5idsXDLA6bJ07Z/ia1s9iGKnLxDBN80OguoO7XwD82zRNt2ma24AtwNGh/7aYpllqmqYH+HdoXxERERHpA3qzJoYvEG6x2s/e8Yv0oOeuO44vHzUCAGuo2GVhtoPJg3Nj9vvtl6YCsHR7yy1gVwt7+lsdGA5qrN9bxzcfX8rv3tzQtRN3QHhZWjILe3p8XcsSixT21HKSlLvBMIzVoeUmhaGxYcCuqH3KQmOJxuMYhnGNYRhLDcNYWlFR0RPzFhEREZFOWr27ttdeK5z1kQ5dBUT6uvxMO984dlTC7a9//wSuOG503PiNz62OPO5qJkbrrkY/e3EtpmlS0xTM8ti0v6FL5+3Qa4e7HCUpGFpR72bxtmBgp7NBXX8/rfOTbt1J/gLcCZihr/cBVwFt/auYtB2EafM3wTTNh4CHAGbPnt1Hm/mIiIiIHFr8UW/aKxvcFOU4e+y1wvU3lIkh0n2rbj/9oNsNw+Dn504hw27lolnDWLAx/oPkrtbEaJ2J8dTinYwrzuFfi3YAkGG3tnVYUoQzurqzLO0bx47i5VV7APjhMysj4y6vv0PHV9S78QdM9tW5gP5X2DOtghimae4PPzYM4+/AK6GnZcCIqF2HA3tCjxONi4iIiEiai66Jsa/W1bNBjH76qaVIqlgtBjedNTn0zOBXr34es73LmRht1NJZt7s2Uhgz2fUwXlq5m/I6N986cWzkta3dyOiyGEYkELO7pjkyHq6N0Z6jfv1OzPPuzKUvSqswtGEYQ6KeXgiEO5e8DFxiGIbTMIwxwATgM2AJMMEwjDGGYTgIFv98uTfnLCIiIiJd54nKxHD7OvYpZFeFAya2fvaGXyQdjC/JiRu7/snlXTqXp41lF25fgPCv9sBsR5fOm8j3/72SX78WDMB4k1Bbx2oxCISCGKUVLR1JulojqL8FMVKWiWEYxtPAyUCRYRhlwO3AyYZhzCC4JGQ7cC2AaZrrDMN4BlgP+IDvmKbpD53nBuBNwAr8wzTNdb38rYiIiIhIF0W/aXd5e7bIp08tVkUOCW3d7Nc2e8m0W2n0+CNLPpJte2UjTnvw+tGd2jpWi4G/jSyU1rU+OnO+/iRlQQzTNL/axvAjB9n/18Cv2xh/DXgtiVMTERERkV7iiUqfbvb0bCZGpKuAlpOIpI2Fmys4Ymg+he1kT5imyetr93HG4YPbDGJ8tKUy8rit5SbJ8Pb6/Zx5xGCg+8tJ2oqzdCQTw9fGPupOIiIiIiLSS2IyMXp4OYkvEMBqMTD62Rt+kXR22SOfcdXjS9rd7+VVe7j+yeU8+vE21rTT1cib5CDGsILM4HkDgcg1y96t5SS0mYnRkXk3uH1xYyMHZnV5Ln2RghgiIiIikjLRb9p76tPT6POrHoZI+tm4r77dfSrq3UCwAPANT60A4K+XHsn2u8+J27enlpP4/GZS2ppaQ4U9F5dWxYx3JBOjrjk+iOG09Vw3lnSkIIaIiIiIpEx0gb62ivUlk9dvduvTUxFJjtZLMZo6sZTs4Y+2RR6HLxmti4YmOyAaDlx4/YGkFAi2hI696rFgBkpuRrDKg7cD3UkaPfFBjP5GV3ERERERSRmvLxC5oenxTIxAQO1VRdLAuOLsuDGzC+1WT5hYBMD9l8yMGe9ql49Ewufz+s1Ilkd3CgSHa1g0hoI3px02KHj+DhT2bH2dfOeHJ3V5Hn2VghgiIiIikjIef4AsezAVOtk3Hq15/aY6k4ik0B0XHA7AkPzMuG0fbKpIeNyyHQf424elMWOnHjaIvAw7AFOG5jE0PyOyratdPhIJZ4n5ojMxuhEQtbTK4pg0OBe71ejQNdAbtVTm+PFFbbauPdTpKi4iIiIiKeP1B8hy9k4Qw+cPqDOJSAo5Qsu5inKccduueHQJu2ua48YDAZMv/eWTSE2MsLyM2Eabg6OCGD2XiRGIdFRydKuwZ+x1qCDLjt1qwesLUNvsZfav3uFHz6yKbP/pc6t5flkZ0LK05d6Lp/P3b8zu8hz6MgUxRERERCRlPD6TbEdoPXiPLycxtZxEJIXqXF4geNO+/e5zGFeczXfmjYts/ziqTWpYQxs1IP566SxuPXdKzNiJE4sjj5O9NC18bfIGzEgXpQxH14tphpeTTB9RAMBFs0Zgt1rwBUy+9cRSKhvcPL88GLR4dfVe/rN0Fz96NhjUCH9vwwoyyezGHPoyW/u7iIiIiIj0DK8/EHkj7uvx5SQB7FpOIpIy4V/xcFHMd390MgAPvL8VgJ88t5qLZw2PaYP8+Z66mHNs/vVZbRboDUQtIUlmd5JAoKUjiccXwO0NBTG60REk/O2t2lUDBDMz7FYDjz/Ayp01kf1eWrmb7/97Zcyx4e+tP2eV6SouIiIiIinj9QfIDNXEWFVW087e3ePzm3Fp3CLSewKh4p2tfw//d8Pxkcf3vrUx8viHz6zkKw8titk3UYehmaMKI4+TmdUVXYNiZ1UTt764DqBbWRBtXYfc3gC7DzTHdGlqHcCAlnof/flapiCGiIiIiKSMxxfAYbOQYbdEPu3sKcHuJHr7K5Iq4aUQrW/A8zJbFgg88P5WvP4An22r5oXluyPjPz59Ivd/NbYLSbR5k0pYeuupzJ9cktRMjOiAyGfbq6lsCNbm6E4mRFsBiHq3jw82VTAkqrZHa/tqXeytcQHd647S12k5iYiIiIikjNcfINtp44ih+bh9Pd+dpD+nYIukmj+UiWExDv57OOFnr8c8P33KIG6YP6Hd8xflONle2UhpZSPLdx7gyJGF7R7THm8b16WSXCfDC7O6fM4Me+IsjswE2/Iz7cy5693I8/5c36f/hm9EREREJOU8fhOHzYLTbunxIIY/YEbW4otI7wskWAoxcsDBAwKdaSNaWtkIwIsrdrezZ8eEO5OMGNDSFva86UO7dc5wa1hoKe4ZFp5/2PPXHcuphw2ittkbM96fr2XKxBARERGRlPH6AzisFvw2K3XN8V0Ikv1aWk4ikjotmRix40Y7mRlXnzC206/lS9LytHCNihynHYhvAdsV+ZktQYy/XTqrzX1W3XY6DR4fwwoygdK47Q5b/72W9d/vXERERERSzuMLYLcaOKwW3KHWhT3FF9ByEpFUasnE6Nht6M/PncKTVx/DgGxHp18rWd2OwjUxcjNaPv+/+oQx3TrnzJEt2RfhgE7rWhj5WfZQAAP+8JXpcedoL3vlUKZMDBERERFJGa8/gN1qwYQeX07i8wewOfX2VyRVvnXiWLaUN/C1o0fGbXv8qqP5zauf4/UHKK1s5KJZw/nm8Z0PFlgtBv6AGSki2l3hYMiArJZASkFm54Mq0aJrYlhCUQzzINPNjVp+EtZe9sqhTFdxEREREUkZrz/YncQwYEdVE6NvepXVvzg9Zs148l5LmRgiqVSU4+SRK45qc9tJE4s5aWIxAJv31zNqYHaXXiP8G+5NsJzENE2eXVbGedOGdqhNang5ydTh+byxbh8AGfbkLWgIFzk9/fBBPPHpDgCuO3lcwv1fuP64mOUo/ZGCGCIiIiKSMsHlJJaYbgVbyxuYmYSuAq35AoF+3ZZQpK+YMCi3y8eGLyXhDIpAwOSGp5dTUe/m3ouns6OqiZ88t5p1u2v55QVHtHu+8HKSopyW7ItkZkFYQ+e67dwpXHPiWHKddvKz4oMUH980n7pmL4cNyUvaa/dVCmKIiIiISMp4Q91J/Ekqwhftl/9bh8cX4NcXTgXA5zf7dVtCkf4kXNizzuXltTXBDIpVZbWR7VWNng6dJ9ydpDCre0tIEgnHVW1Wy0Hbtg4ryIzUyOjvFMQQERERkZTx+IOFPXuiXeCjH28HiAQxvIFg1oeIHLoMDMBkcWkV0JJJAeD2+iOBzI5ec7yhWj15mXamD8/npEklSZ2vpR/XtugqBTFEREREJCWaPD78ARO71RK68Qjqaqp2Rb2bvEwbTlvb69x9fhNrDwRLRCT91Ll87K2NbYnq8gV4+uOdQMc7pIRrYtitFl664fjkThJ0TeoChaJFREREpNc9s2QXU257EwCHzYIzqlBe4GBl+hPYtL+eo379Dt98bGnCfVTYU+TQd+Xc0ZHHlz68OKZLyaLSKtbvrQM6nokR7prktPXMrbMyMTpPQQwRERER6XWvrtkbeeywWnBELfMor3NT2+zt1PnO/r+FAHy0pTJu2z1vbGDJ9moV9hTpB246azLfnT8egK0VjZFMCoC6qOtKQZadTfvr47I1Wqt3+QB6pGMSgBIxOk/LSURERESk10VnW9itlpiCm9/+1zIAnrr6GI4bX9Sh8/kOUhj0wQVbeXDBVnKcNhX2FDnEGYYRE6yMzsRo8vgjjxduruRvH5YyeXAub/zgxITnCwc+8jJ75tZZy0k6T6FoEREREel10UEMh83SZh2Lrz28mCaPL9IdAMBsY6lJo9sXeZzjTHyj4VNhT5F+J/r60eBquVaEl5Vs2FdP4CBB0CZP8JgsR88EMZLZrrW/UCaGiIiIiPS6QMt9BaccVsLi0uo29wvXzbho1nB2VTexZncty39+Ghn2lqBH+GYEwOX1x50jzOc3e6QLioikr+gsrYaogGe0ygY3JXkZbW4Lx0CUMZE+FIoWERERkV7nD2VUDC/MpCQ3I5Ih4UhQPO+5ZWUs3lZNk8dPZYM7ZpvbG7zLOH58Eb6AiS/qk9dovoCJTZkYIoe84YWZkccvrtgdeby7Jrb+xbnThgDw6CfbE54rfK1KdgzjsSuP4uazJif3pP2EruIiIiIi0uvC6dvh4EX4BsHegTuFqx5bwg1PLafeFVyrHk4Xz80IJhl7EgQxOnp+EenbvnjkMA4fmofVYhw0O+usI4JBDK8v8TUjEDCxGMlf9nHypBKuPWlcUs/ZXyiIISIiIiK9LlwTI8thDX0NBiD8HWivuml/A6+s3svUX7zFiyt2R1oghoMYLm/iG5JB+W2njIvIocMwDE6ZXII/YOLxBWIyM6JlOiwUZNlj6ma05jdNLSVJMwpiiIiIiEivG1IQvKn4w1dmADC0IBhcmDa8oN1jrz+55dPL+9/bHJWJEWyB2Oz142+jUN+XZw/nvGlDuzdxEekTwkvH3L74gr75mcFrhdNmxW61HDR7K5iJoSBGOlEQQ0RERER6nQGMLcpm4qBcAMYW5/DSd+Zy38XTARJ+cgrwkzOj1pGb4GmViTH37vfa/GT1noumk+mI74IiIoeecPZEk8eHPaq18vKfn0ZxrhMI1uBxWC14fIkzwPwBZWKkGwUxRERERKTXuX0BnPbYgML0EQWMGJDFRz+dx/9uOL5D57FZjcinqOFMDDh4lxIROfSFOxG9v7GCBpePj2+azyvfPZ4B2Q5+9YUj+MaxozhiaD67a5p5fnkZb6zdS2WDm8seWUxVVPHggIkyMdKMWqyKiIiISKd9tLmSOWMHdLnbh8vrJ8Pe9rHDC7MwTZMvzBjK6KJs/vjO5si21p+IbtrfwIa9dVgtBkOi6l2c+vsPAbjjgsPx+k0unj28S/MUkb4p+tq0p9bFsIJMhoWWsc0ZO5A5YwfG7L9gYwXr9tSxcHMl/1q0k++fOgEI1u9RIkZ6URBDRERERDrl061VXPrIYr43fzw/PH1Sl87h9gZwJminCsHCfH+8ZCYNbh87q5uobfJy5KhCjhkzIG7ft9bvZ0C2g+MnFHHm4YNZVVbDiROKsdsMTj1sEEMLEi9NEZFDk62TkQdPgg4lWk6SfhTEEBEREZFOqQylWm+taOzyOVw+PwOyHe3ul+O08fsvz4gbP358ER9tqQSCn5Q6rBbyMuz89bJZXZ6TiBw6yutdHdpvyc9O5cw/fojbH6CtUIW6k6Qf1cQQERERkU4Jv6EPdKAdaiJub4AMW9eLbD5yxeyWc/kOntUhIv3P3pqWIMYZhw9KuF9xrpPiXGdMJsZ7G8sjj9WdJP3oai8iIiIinRL+ULKtNqYd5fL5cSaoidERTpuV/7skmKFR0+Sl3u3r8rlE5NATfX25eNaIg+9rs+COCmKs2lUTeRysiaEgRjpREENEREREOsUwwpkYXT+Hy+vvViYGwFlHDGH68HwAKurd7ewtIv3JzWcfFnnsC7Rd7yLMYbOwo6qRJxbtiNvmD8QXFJbUUhBDRERERDolvIrk062VXT6H2xdI2J2koxw2C7+9aFq3ziEih6a8qJbLXv/BI67jinPYUdXOwdwlAAAgAElEQVRETZM3blvANLHorjmt6J9DRERERDqlIlTYs9Hj7/I5gi1Wu5eJATC0IBOHzcKFM4d1+1wicmjy+g+eiXHXF6fyhRlDY8YaQkvU/AETq5aTpBV1JxERERGRTtlV3QRAXkbX3kqapomrnRarHZWXYWfdL8/odDtFETn0XTRrOM8tK8PXTiaGYRjkRmVuANz24louOXokjW5fUgKukjzKxBARERGRTnF7gxkYdS4fWysaOn98qICeM0k3BnarJVKnQ0QkbO74gQCMK8lpd98sR+z16IUVu/ny3z7l3Q3ljC3O7pH5SdcoiCEiIiIinRJdxf/ZpWVdPl6fbopIT7pw5nAW33IKs0YVtrtvpiPx9eiwwXnJnJZ0k5aTiIiIiEinRAcxinOdnT8+lMmRjOUkIiIHMygvo0P7hTMxpg7L54b547n79Q2cPXUwc8YO5KjRA3pyitJJCmKIiIiISKe4fX6G5mewp9aFr52CeW1xeZWJISLpJdMRvDWeOCiXMw4fzBmHD07xjCQRhb9FREREpFPc3gB5mcEieB5fF4IYvmAmRndbrIqIJEtWKKjqD3T+mia9S385RERERKRT3L4AWQ4rhgGeLmRiuEOZGE6bMjFEJD3YrMHiwL7AwTuZSOopiCEiIiIineL2+cmwW7EaBpUN7k4fr0wMEUk31lCbZr+CGGlPfzlEREREpFPcvgBOmwW/abJ8R02nj3d5w0EMZWKISHqwKYjRZyiIISIiIiKd4vYGcNqsTCzJPWhbwoMdD5Ch5SQikiasluCtsYIY6S9lQQzDMP5hGEa5YRhro8YGGIbxtmEYm0NfC0PjhmEY9xuGscUwjNWGYRwZdczlof03G4ZxeSq+FxEREZH+xO3z47RbGDUwiwa3r819XF4//1u1B9OMvyEILydxajmJiKSJwqxgseIhBR1rySqpk8q/HI8BZ7Yauwl41zTNCcC7oecAZwETQv9dA/wFgkEP4HbgGOBo4PZw4ENEREREksM0TW5/aS0vrtgNtCwneW9DOVvKG1i7uzbumN+9uZHvPr2Cl1buobFVoMOlTAwRSTOzRw/gwa8fya3nTEn1VKQdKQtimKb5IVDdavgC4PHQ48eBL0SNP2EGLQIKDMMYApwBvG2aZrVpmgeAt4kPjIiIiIhIN1Q3enj80x384D8rgXAQwxqp4r+ruinumPL6YMHPH/xnJRc88HHMtpaaGMrEEJH0cfbUIarV0wek21+OQaZp7gUIfS0JjQ8DdkXtVxYaSzQexzCMawzDWGoYxtKKioqkT1xERETkUOX2tbRRNU0Tl9eP02Zh/uTgWzV/G0tGnLaWt5lbyhvaPJ9arIqISGelWxAjEaONMfMg4/GDpvmQaZqzTdOcXVxcnNTJiYiIiBzKooMYXr8ZzMSwW7j9vGDadbhQZ7TWxfFOvOd91pQFl52EMzFUE0NERDor3f5y7A8tEyH0tTw0XgaMiNpvOLDnIOMiIiIikiQLN7dksTZ5fPgDJk6bNZJJER3kCGtd8HNndRPr9gSDGO5wEMOWbm9FRUQk3aXbX46XgXCHkcuBl6LGvxHqUjIHqA0tN3kTON0wjMJQQc/TQ2MiIiIikiS3vbQu8riuORiccNoskZoW7lC3kWiNbh8zRxbw4NeP5KOfzgOgORS8cIUKgxpGW0m1IiIiidlS9cKGYTwNnAwUGYZRRrDLyN3AM4ZhfBPYCVwc2v014GxgC9AEXAlgmma1YRh3AktC+91hmmbrYqEiIiIi0kWBVstC6lxeIBjECGdiuNpYTtLo9lGQ5eDsqUMiQY4mT/Cr2+tX8TwREemSlAUxTNP8aoJNp7Sxrwl8J8F5/gH8I4lTExEREZGQcPZE2IEmDwBOuxVHaDnIb9/YwKVzRpKbYY/s1+D2MbwwCwCH1YLFaKmF0ez1qzOJiIh0if56iIiIiEhC4SDGmYcPBuCyRz4Dgu1RrZaW5SDbK1varN7zxga2VjQyOD8DAMMwyLRbafaEgxgBshwp+yxNRET6MAUxRERERCShcOBh8pDcmPHW7VHrQ8tMAB5csBWAueMHRsYyHdZIQKTZo+UkIiLSNQpiiIiIiEhC4ToWE0pigxg2S2xRzqU7DsQdO3d8UeRxhr0liOHy+slyKIghIiKdpyCGiIiIiCTU5Al2I2kddGi9HOT3b2/C4wtQ1eCOjEVna2TarZGaGE0eH5nKxBARkS5QEENEREREEgovJ8lsFcQ4blxwqciTVx8TGbv+yeUs2FgBwAUzhsbsn+kI1sQwTZNmbyDufCIiIh2hikoiIiIiklB4OUnrTAxLaDlJ9JKRdz7fzzuf7wfg/Omtghh2K40eP3Pvfo89tS4mlOT05LRFROQQpUwMEREREUmoydsSxDh9yiAAzp46OGaf/Ex73HF5rcYyHVb21DSzp9YVfK7lJCIi0gXKxBARERGRhFyhTIwMu5WHvjG7zX3uu3g6Vz+xNGZs5oiCmOeZditlB5ojz63W2MKgIiIiHaFMDBERERFJqKWwZ+LPvlovNSnIsmOzxr7NbJ15cdLE4iTNUERE+hMFMUREREQkoejlJIlktNrW1vKSUQOzY55PH14Qt4+IiEh7FMQQERERkYSaPX4MA5y2xG8bWwc4dlQ1xe3z/VMnxDzPydCqZhER6Tz99RARERGRhJo8frLsVgwjcQ2LjhbpvO/i6WyrbKTB7SNbLVZFRKQLFMQQERERkYSaPH4y2wk4DMxxAnDl3NE8+vH2hPt9adbwZE5NRET6IQUxRERERCQhl7f9IEaO08bnd5xJht3Cs0vLuH7euF6anYiI9DcKYoiIiIhIQk0eH1n29t8yhgMda395Rk9PSURE+jEV9hQRERGRhDqynERERKS3KIghIiIiIgk1e/wHba8qIiLSmxTEEBEREZGEVu6q6XD3ERERkZ6mIIaIiIiItMnt8+MLmLh8/lRPRUREBFAQQ0REREQSaHQHgxenHTYoxTMREREJUhBDRERERNrU6PYBkO1UQzsREUkPCmKIiIiISJsaQkGMHAUxREQkTSiIISIiIiLUNHlYtqM68tw0TRZsrACUiSEiIulDQQwRERER4arHlvClv3yKxxcA4IXlu/ntGxsABTFERCR9KIghIiIiIqzdXQdAVaMbgE376yPbshxqsSoiIulBQQwRERERwWELvi2sbvQA4PEHItuGFWamZE4iIiKtKYghIiIiIgRME4BmT7CtqjcqiJGXYU/JnERERFpTEENEREREIkGMxlAQwx8wUzkdERGRNimIISIiIiIEQokXTy/eGTM+b1JxCmYjIiLSNgUxRERERITC7OCSkaU7DgCwr9bFuOJsHvrG7FROS0REJIaCGCIiIiJCePVIZYObu17/nL21LsYUZWO36u2iiIikD/1VEhEREREaXL7I4799UEppZSND8tWVRERE0ouCGCIiIiL9nNcfoNnrj7RZBfD4ApwzbUgKZyUiIhJPQQwRERGRfq7RHczCyHZYY8aPGTMgFdMRERFJSEEMERERkX6uPrSUJNMeG8QwDCMV0xEREUlIQQwRERGRfq4hlImR2SoTQ0REJN0oiCEiIiLSz4WDGEU5zsjY6VMGpWo6IiIiCdlSPQERERHpHV5/gI82V3LSxGIsFi0TkBbhziTfP3UCL6/cQ47TxndPmZDiWYmIiMRTJoaIiEg/8fb6/Vz52BJuf3ld3LY6l5cbn13FgUZPCmYmqVYfysQoyXVy95emceu5U8jPtKd4ViIiIvEUxBAREeknmjx+AD7fWxe37S8LtvLssjKeXbart6claSCciZHjVOBCRETSm4IYIiIi/cA1Tyzlx8+uAmDpjgMs23EgZntpRQMAm/c38Pyysl6fn6RWvcsLQG6GVhqLiEh6UxBDRESkH3hr/f6Y5y8sjw1UhFtsPrusjB+Fgh0AK3YeYOn26p6foKRUg9uHYUCWupOIiEiaUxBDRESkH/r3kthlIz6/2eZ+Fz74CRf99dPemJKkUL3LR47ThmGo4KuIiKQ3BTFERET6kWEFmQD4Ayam2RK4qGx0x+xnmiYeX6BX5yap0+D2kevUUhIREUl/CmKIiIgc4sLBiItnDeexK4+KjIcLfQJU1McGMRZurmTBxvLIc7fPjxy6Glw+clQPQ0RE+gAFMURERA5x5fUuAGaOLGTCoFx+c+FUAF5cuRvTNHF5/ZGaGGFbKxq48bnVkefPLlWxz0NZgzu4nERERCTdKYghIiJyiNtZ1QTA6IFZAORlBm9Wf/bftby/sZxPS6sAuPr4MZw9dTAQzMSobfZSlOMEoLxVpoZ03sur9vDVhxZFOsGkk3q3j5wMtVcVEZH0pyCGiIjIIay8zsXXHl4MwMhQECM36mb1qseWcuWjS8i0W/nxGZN48OuzAHhvQ3ApyfUnjwPgsY+3UV7n6s2pH3K+9/QKPi2tYv59HxAItF1INVUaXF7VxBARkT5BQQwREZFD2PKdByKPw0U989qofWAYkGEPttecM3ZAZHxgjgOnzUKdy8fRv3mXRrcv7ljpvPc2lLO3tjlu/JXVe7j+yWU0eXr356zlJCIi0lekZRDDMIzthmGsMQxjpWEYS0NjAwzDeNswjM2hr4WhccMwjPsNw9hiGMZqwzCOTO3sRURE0keju6UgZ7h9Zm4bywYuP2505PG/rzk28jg/0447qkvJ7pr4G29pX3QnGICrn1jKhQ98Erff955ewWtr9rFhX31vTQ1QYU8REek70jKIETLPNM0ZpmnODj2/CXjXNM0JwLuh5wBnARNC/10D/KXXZyoiItIDHlywhWm/eJNr/7m0y+eoc3njxsI1MaIdObKwzePzM2MDHqf/4UPm37eAZ5fu6vKc+qNFpdVxY/vqXLy4Yje1TcF/I9M0Ca8yaV1otSf5AyaNHj+5CmKIiEgf0Jf+Wl0AnBx6/DiwAPhpaPwJM/gRxyLDMAoMwxhimubelMxSREQkSRaXVlPn8vHJlqounyNckHPBj0+OjOW1ysSYPDiXY6KWkACcefhg3li3j7HFOfzmwqnc+9ZGqhs9AJRWNHLjc6tx2CxcMGNYl+fWnzQkWIbzg/+sBOCYMQPwR9XJeOD9LRw7diAOW89/3vS/VXsAtJxERET6hHTNxDCBtwzDWGYYxjWhsUHhwEToa0lofBgQ/XFQWWgshmEY1xiGsdQwjKUVFRU9OHUREemLlmyv5raX1vLR5spUTwWAPTXNfLAp+Peq3u2LucHtjP11LoYVZDK6KDsylmG3cvNZk3n0iqP4+Kb5vPGDE+MCG3/62kw23Hkm+Zl2vnbMSJb//DRmjYrN1nhp5Z4uzak/qg9lxDx65VFMHJQTt31ndRNWi8HRY4LBpM+2VbN0R3z2RrK5ff5IIGXKkLwefz0REZHuSteQ+1zTNPcYhlECvG0YxoaD7Gu0MRb3Ts80zYeAhwBmz56dXiXBRUQk5R5eWMqb6/bzxKc7OGfqEPIybdx+3uGRYpe97aEPS2Oe17u8FGQ5On2e8jo3JXnOuPFrTxp30OPsVgutv/X/u2QGD31YyiVHjeTs+xeS5UjNz6YvqgktGZkxvIA3vn8iAdNk/M9ej2y/cu5orjkx+G+yfk8dZ9+/MLLMBGBbZSOvrNrDzJGFHD+hKGnzuuGpFQDccvZkjhufvPOKiIj0lLTMxDBNc0/oaznwX+BoYL9hGEMAQl/LQ7uXASOiDh8O6KMhERHpFJc3WLxyfEkOK3fV8PRnu1i3py5l8/loSzAjJNzitNnrP9juCe2pbWZQbkZS5jS8MIs7LjiCKUPzmDmygKoGT1LO2x/UNHkwDMjLtGOxGNissW/BvnHs6Mjj/KxgVszLq1rezsy7dwH3vb2JSx9ZnNR5vb1+PwD+QDs7ioiIpIm0C2IYhpFtGEZu+DFwOrAWeBm4PLTb5cBLoccvA98IdSmZA9SqHoaIiHSW2+fn6NEDeOeHJ3H/V2cCLUsAUmFLeQMAE0JLD8JBls5w+/xsr2yMnCOZVuys4dPSKpbtOND+zsKBJi/5mXaslrYSSInJ+CnKCWbcVDW2HSTq6tKi19fs5eGFwQyfRrePm19YE9l29tTBXTqniIhIb0u7IAYwCPjIMIxVwGfAq6ZpvgHcDZxmGMZm4LTQc4DXgFJgC/B34Pren7KIiPR1y3YcwBcIBgryQ907vv2vZXhT8BF19E1qhi14c+v2dT4To6bJS8CEQXnJycRoy71vbqTB7ePzvanLWklXS7ZXc8EDH7O6rIZ/LtpBs6dj/4ZOm5Wpw/IxgLIDTbhaZeHUNccH11xeP88tK4tr5RrtuieX86tXP6emycNNL6zh6c92AnDOtCGMGpid8DgREZF0knY1MUzTLAWmtzFeBZzSxrgJfKcXpiYiIocot8+P12+yfGcNAKMHZjOsIJPdNc3srXExcmBWr84nnAFyxXGjcdqDnze8t6GcyYM7V3hxRej7KexCLY32HD1mAJ9tq8YfMDn/Tx9RWtnItrvOxjDazjToj3763GpKKxs5/88fA+D2xQbEfnHeFP65aAd//MrMuGOLc528t6Gc43/7PtOG5wPBLjIb9tVT0+ylMDv23/T2l9bxn6W7+HRrFeNKsrniuNFkOdp+m7dwc2WkIwnAn78a//oiIiLpKu2CGCIiIr3N64/99NpmtfCrLxzBlY8t4at/XxQJJPS0XKeNhy8/KpKJMWlwbuRG9J43NnL9yeM7fC6X18/dr38OwBHDkt914tErjuLiv37Kxv311IYyAz7fW8+Uof2rw8X7G8sZmO1g2vCCuG2llY0xz++5aFrM8yvmjuGKuWPaPO8vzz+cMw4fxE+fX8PqsloALp0ziltfXMuBJg9jiM2cWLM7uM/zy8sAGFuUw5lHtCwRic7m+O7TKyKPr5w7WoEnERHpUxTEEBGRfs8XWjIytrjlxnD26EK+MnsEjR5fr8yhssHNotJqSisaGJKfCYDDauGIYfmRfe55YwM/OXNyh8734aYKtlc1ATByQPIzSbKdNmaMLGB91DKSpz7bwW3nHo7D1jtBn3V7anl44TZ+d9G0uEKZvcEfMLny0SUAbL/7nJhtgVZ1K0p/czaWBPUw2jJiQBYXF4zg5hfWEDCDNSsODwWIoruWhK1vtZznnjc3cNqUQbyyeg/3v7uZvbWuuGN+c+FUvnbMyA7PSUREJB0oiCEiIv1eOBPjyuNGR8ZyM+z8ttUn5z1pUWkVlzy0CH/AxBMKqthtFnKcNp6/7li+9JdPeWnlng4HMe56vaU7eU990j6w1ZKGfy3ayX+X72bdHWf2yOu1dv2Ty9lR1cR35o1nfEmweGkgYPLZ9mpmjCjAHzDJdib/rY7PH+D3b2+i3tUS4Hr38/34AyaTBudyoMlLTVOwKOfEQTlcc+K4TgUwwiwWg3OnDWXN7lrmTx4UabF7oKml4KfPH+A/S3fFHVta0ci4W15LeO5xxdkKYIiISJ+kIIaIiPR74YKeVkvq6l3brcGbXG/AxBOqneAIZRfMGjWAy48dxYsrO9ZBvLLBzbbQUoajRw/ogdkGhWttWC0GVouBxxeg0eNn3Z5atlY0cv70oT322gA7Qpkmje6WYMJDC0u5OyqA0zpDIhk+3lrFgwu2xox98/Glbe77o9MnccbhXe/8cX9UvYrwsp0fPrOKHz+7ird/eBJlB5r52X/XAnDZnFH8c9GONs9z/vShfO+UCXzt74sor3fzwnVzuzwnERGRVFIQQ0RE+rVAwMQXysSwWVNXG8AWCqD4/AGavcGbcoetZT45GTZqm70s3V7N7HYCE2tD9RFuPecwrkxQcyEZBoQyMbLsVr42ZyR/+yDYvvOc+z8C4ITxRXEFKHtCdVQr0kWlVTHbtlU20uzxJ7VWx0srdwPwhRlDWV1WG6l98YUZQ5k1egBD8zMiQY3TDhuUtNfNz7RHHgdMOOW+D8iNyjSZN7mY7586gfxMO8fe9S6VDcGfy2+/NJWvHBXMunjr/52IaUJ+lh0REZG+KB1brIqIiPSKZ5buYuwtr/GvxcFPr+0pDGJYQ8sN3ly3jy/95VMAHFZrZPvc8UUAPLV4Z8xxH2yqoLSiIWbsP0t2keu0cdGs4ZHz9oSSXCcQDP7MGTswbvvMO9/usdeOVtngBoIBoAUbK8jPtDMoLzi3efcu4Oz7F1Lb5GXBxnJW7arh4YWl3Xq90opGjhs3kD9eMpPHrzqabIeV9398Mn+8ZCaXzRnFKVGBi64sIzmY0VGdcjLtVupDWSgf3jiP+ZMHUZTjxG61sPiWU3n8qqM5ZXIJJ0wojhxTkOXolcCSiIhIT1EmhoiI9Fv3v7sZgA82VgAt2RCpYA8tHXlmaVlkLCOqK8px44qYM3YAO6ubImP+gMnl//iMYQWZfHzTfCDYheL1tfv44pHDIjUUesrEwbkAfPmoEcybVMLaX57BXxdsZfXuWj7cFPyZrtxVw4wR8Z07uivYYT2oqtHDruomTr53AQCXHD2C782fwNRfvEm4vub0O96KOf7SOaPIsFvprA376li5q4aLZg0HggU426oBcv9XZ1KUk/yf//87bSJvr9/PPRdN485X1vP0Z7sYVpAZ1wbYajE4aWIxJ00sTnAmERGRvklBDBER6dNeWb0nUtwx2qdbq3j042388ZIZZDlsLNlezYqdBzhuXFGk40eTJ9h2Mlz40taDWQvtaWspy4hWXUWKcpys39PSheLRj7cBsLumOTK2YmcNAIcN7vlWp0U5Tj675RTyQssccpw2fnzGJABG3/QqAK+s2tMjQYyGqDoYD76/hQ176yKtaacMySPbaeP28w7n9pfXtXl8VaOHYQWZHX69TfvrOf/PH+HyBuuVTI3qGtOWnqoHcsGMYVwwYxgAxTnBbJOiUEaMiIhIf6AghoiI9Gk3PLUCgIIsO/tqXfzwtIkYhsEf39nE4m3VTLntTXIzbJFOEkePGcAz1x4LQFOofernofaUqWjTGWZvlQUyb1Ixg/IyYsaKcpxUhJZOADy/fHfceX4RummfN7mkB2YZr6TVHMO2330Op9y3gB1RmSPJFF0Ho87liyl6Oj/0vV9+3Gi+cewoKurdfOep5TS4/ZF/66oGd6eCGM8vL4sEML51whgumzMqGd9Gt1SFfgbeUCFYERGR/kBBDBER6bNcXn/kcbhDw1ePHonTZmHxturItuhWmLuqm3hz3T5ME1zeAONLchhakMnAbAdHjkx+xkBHWaMyMa6cO5rbzzs8bp+iHAf1Lh9rd9dyxLB8DhuSG7kpv+CBj8nLsLFxfz0FWXbGFmX32twTGT0wm7fX7+ed9fs5dUp8gUuX188dr6xn2rB8vnLUiE61gv3rB1vjxgqz7Cz48TxyM1qKVhqGQUleBs9++zgAlu88wBcf/ISqBk/c8QfjDxV/vfbEsdx4xqSk17roiuPHF/Hk4p3c04utgEVERFJNQQwREemz9tW6APjikcN4IZSV8N2nV7A3anlFtMmDc9mwr55r/7ksMnbl3NF8/ZjUf6qeYWvJxPj+KRPa3GdgaPnAuX/6iKW3nsqAqJoXq3bVRB7fMG98WtxkHzYkj3c3lPPAgi1xQQzTNJn88zcAeArIy7Rz9tQhHTpvncvL05/tAmDSoFw27q8Hgt1S2uu6UZQd/BlW1LsPul9rD38UXLpz89mHdeq4nnTW1CGsv+MMshx6OyciIv2H/uqJiEifVR66Eb1w5jDOnTaEqx5byrIdByLbvzJ7BP9Zuivy/PnrjmNHVRMmoZaqFgsTSnJ6d9IJDIjqGJGoIGdBVIvNKx79jJkjCiPP//XNY/jJc6sYnJ/Ra0tJ2vOdeeP56wdbY1qDhu2viw0ifLSlssNBjOgAxMOXzyYvw85b6/dFlpEczMBQsc2fPL+aZq+fy48b3e4xgVCtjdyM9HvbpACGiIj0N/rLJyIifVb4ZrY418mIwtgimEeNLuT286dQ2eDm3Q3lfPukcWQ7bUwZ2vMFL7vCMAweu/IoRg9MvAzkiKhikmt31zFxUC6D8zJYdMspAHxy8yk9Ps/OyHRYOXlSCdurGuO2tc6EeGrxTp5avJOvHj2Su7449aDnLY8KgAzMcZDlsHHx7BEdmlOWo6Ujye0vr2P+5JK4Aqqthef/szTKwhAREemvUlfBTEREpJvK64PLSUpyM8h22thw55lsu+tstt11Ns9ceyxZDhv3XjydUw8bxNUnjEnxbNt38qQSRh+klsWIAVks/Mm8yPM9Nc047en9p/yIYXlsKW+gpim2BsXe2uCSn5e+M5fHrzo6Mv70ZzvZVhkf9IgWLm562ZxRnc5EaF1344R73j/o/l5/gPn3fQC0tJQVERGR1Envdz4iItLvrdh5gNE3vcqf39sct6283o3NYkSWWWTYrRiGEfkPoDDbwcOXz6Yo59BoQzliQBZ/u2wWAItKq9lR1TPdP5Jl4qDgjf+eGlfMeNmBYBBjxIAsTppYzPSoNqy3vLDmoOcMZ3H88LSJSZnjT55bxfsbynlz3b64beHCqQAlamUqIiKScgpiiIhIWttc3gDAvW9tittWUe+mKMeZFkUse9OJE4r56ZmTATrVJjQVBucHW7A+8el2INgdxh8wKTvQTKbdSmGoEOe/vzWHO79wBACfllbxwvKyhOesqHdjtxoUtFPEsz2nhGpoPLO0jCsfW8K1/1zGkXe+zdy736PO5QVg/Z5gEONbJ4xheOHBl52IiIhIz1NNDBERSRs+f4D739vCpceMpCQvA3/A5MlFOyLb31i7j7IDTZw/Yyh5GXaeW5b4RvdQlumwct3J47jq+NEYpHcAZ3hhMMjy7yW7mDe5hGv/uYxvnTCGsgNNDC/MjGTMZDqsXDZnFFvLG3jsk+388JlV/PGdzTR7/ZHMi6NGF/L4VUdTUe+mOMfZqZas0aYOy2fN7loeueIorv3nUt5ctz+yrboxuOzlxRW7+foxo7gplBVy01mqh1q4CbgAACAASURBVCEiIpIOFMQQEZG0cf97W7j/3c3c/+5mbjxjEr97c2PM9m//K9ga9fW1+7hqbrDGxch2ijIeypw2a/s7pVhJbgZXzR3DPz7eFmlt+/eFwXal8yYVx+1//bxxPPbJdgB2VsculVmy/QB/em8L5fUuiruxtOOZa4/F5fUD8LfLZnOg0UNOqPPIrDvfps7l47aX1jGhJLgUJtdpw9rPsn1ERETSlZaTiIhI2tgZ1cWidQAj2rIdB/jOU8sBePbbx/b4vKR7bjtvSpvjbdUpKcnN4OrjxzA5qojmzWdNjjz+y4Kt7KlpZmg3ltFkOqwURrW0Lcx2YLdasFstrLjt9Mj4Syt3A/Dsdfp/TEREJF0oiCEiImmjvFXbTYCbzppMca6TI0cWcMO88fz6wiMi23565mQG5WX05hSli+65aFrc2IRBOW3ue+u5U3j1eyfwqy8cwarbT+fak8ax8VdnRrZvrWjssVogVovBU986BggugbFZDCYPTs+2vCIiIv2RlpOIiEha8PkDfLK1KmbsD1+ZzoUzh/Ptk8ZFxkzT5Gf/XQvA8eOLenWO0nVfnj2Cnzy3GoBVt53O3xeW8o1jRyfc32oxuHTOqMhzp83Kby6cyi3/DdaoGHWQVrTdNb6kJbhy2bGjDrKniIiI9DYFMUREJC2s2FUDwNzxA5k/eRB3vrKeGSMK4/YzDIPpw/NZVVaLw6aEwr7kjR+cgMNqIT/Lzo/PmNTp448eMwCAWaMK+fLs4cmeXkRxjpNcp43cDBu3ndv2UhgRERFJDQUxREQkLZTXBZeS3HjGZKYPz+e86UMoyW17qciDl87iiU+3M6Gk7eUIkp66uyxjfEkOy39+GoVZ9i53JukIwzD46Kb5OG2WHn0dERER6TwFMUREJKlM0+zUjZ/L62fyz9+IPJ84KAfDMBIGMACGFWRys1pe9ksDogpy9qT8THuvvI6IiIh0jvJwRUQkqb7wwMeccM97kee3vriGOb95l8+2VePzB3jg/S18uKmCQMDkrXX7OPaud2OOz3Iovi4iIiIibdM7RRERSapVZbUAjL7p1Zjxj7ZU8u7n+/nbh6UAjByQxc7qpph9buxCnQQRERER6T8UxBARkaTxB8yE295cu4+N++sjz1sHMJbeeipFOc4em5uIiIiI9H0KYoiISFL4/AHm3PVe3Pjt501hV3Uz/16yMzKWabfS7PVjsxh88/gx/ODUiWQ6rL05XRERERHpgxTEEBGRpNhe1UhlQ7DDyOTBuWzYV8//XTKDC2YMA+C286Yw+qZXybBbWHHbaeysbmJCSY66P4iIiIhIhymIISIi3fbhpgpu+e8aAJ761jEcN66ozf0W33IKdquFDLuViYNye3OKIiIiInIIUBBDRES67eYX1rC7phmASQcJTgzKS9w2VURERESkPWqxKiIi3WKaJgeaPECwu8hAFecUERERkR6iTAwREemWygYPTR4/t583hSvnjkn1dERERETkEKZMDBER6Zb/hLqOjCnKTvFMRERERORQpyCGiIh02TNLdnHvW5sABTFEREREpOdpOYmIiLRpS3k9i0qrcfsCnHnEYIYVZBIImOyuaeb9jeW4vH5+89oGAAblORk5ICvFMxYRERGRQ52CGCIi0qaL//opB5q8QLCF6uNXHc2Pnl3Ff1fsjtv3yauPwTCM3p6iiIiIiPQzCmKIiEicYMcRb+T5B5sqGH3TqzH7DMx2cM60Idxy9mFk2K29PUURERER6YcUxBARkTjbq5oSbrvxjEl88/gxClyIiIiISK9TEENEROIsLq0C4NErjuLIkYWsLKvhR8+s4uHLZzNjREGKZyciIiIi/ZWCGCIiKbC3tpnBeRlpW0fiheXBuhcnTyrGMAxOmljM0ltPTfGsRERERKS/U4tVEZFetmFfHcfe9R7/+Hh7qqfSJo8vwGfbq8l2WNM2yCIiIiIi/ZOCGCIivezxT3YA8NvXN7CjqhHTNHt9Dm+u28ef39tMvcsbt+0H/1kBwM/OmdLb0xIREREROSgtJxER6UVXP76Edz4vB8DjD3DS7xZwxXGj+cX5h3f4HIGASbPXT7bTxlvr9vHPRTsYX5JDjtPGiMIsTp0yiIBpkp9px25tiVXvqGrk1hfXMmfsQH735kYA7n1rE7+7aBqTBudSWtFIYbaD19bsA+DCmcOS+J2LiIiIiHSfghgiIr3knfX7eefzcoYVZPL8dcdx71sbeW5ZGY99sp2544s4bcqghMd6/QF8fpOd1U3c8NRytlc18tMzJ/PA+1s40ORl6fYDNHv9wZ2fD37JtFv5xflTGJjt5NU1e/nvimCdi4WbK4FgvYsFGyu48bnVca/3/HXHkulQ9xERERERSS9GKtKYU2327Nnm0qVLUz0NEemgzfvrWbCxgiNHFTKuOJuCLEeqpwQEMyKeW17GkSML2FndxMwRhRRmx86ttsnLa2v3cvMLawAozLKz6JZTcNqCAYIdVY2c9LsFAIwpyua788fT6PHzyMJStlc1ceLEYmqbPKzZXUsgweX6heuP48iRhTS6ffx3xW78AZPP99bx7yW74vb9+jEj2VbZSFGOk99/eTpLdxzg/nc30+z1k5dhxxcI8LWjR3HOtCHJ+0GJiIiIiHSCYRjLTNOc3eY2BTFE+rfyeheNbj9jirJTPZU2bdpfz+l/+DBu3GLAN48fw5yxAxmUl8HhQ/N6vQjl459s5/aX18WMTR6cS2WDm/xMO1srGmO2HTYkj7u+ODWuRemqXTX84n/rWLGzJu41CrPsjBiQRW6GjUy7jeJcB1ccNwanzcInW6uwGHDJ0SPbnF+9y8vfF26jINPOGUcMZkheBhaLCnWKiIiISHpTEKMVBTFEgtkNn5ZWcecr6/H6Tb4yewRDCzIZV5LN8MIsSisaaPT4qahz4fYHGF6Qicdv0uj2YbdayHZaGV+Sg9NmZXVZDf6AicUwcPn8FGQ6ItsbXD6Kcp1sKW8g22EjP9POwi0VbN7fwO6aZvbWNlPd4GFYYSbDC7MwTZNGj5/SikbsVoO9tS4Avjt/PE0eP498tK3N72fmyAJGDcjCMAzGl+QwOC+DIQUZjCjMYsSArIQ/h7W7axlfkkOGPZgZEQiYVDd5sBgGFfVu9tW5mDw4l5JcJwBLth9gb20zb63bz6tr9jIkP4MjRxYyINtBVaMbtzdAttPGhn11bNrfwPiSHGaPKuQ788YfdB4QbLu6+0AzFovBhJIcbBaLlnSIiIiISL+jIEYrCmJIbzJNMy5DIBAw2VLRgM9vUloZ/BreparBw4Z9dSzbcYDKBg8jB2RRlBNcomC1GAwvzCLTYcVuMbBaLNhtBvmZdgbnZZCfaWfXgSbc3gAef4DaJi9r99Ris1hw2CzYrQamCTuqmvhsezUATpuF0QOzKTvQRKPH32s/l6IcByW5GUwclMP6vcEb/sOG5GGzGPhDP58zDx+M1WJwwYyhnDypJHLslvIGhhdmUu/y8ca6fSwurWJLeQPNXj/761y4vIGY1xpblI3DZiHbacPnD7CqrJbBeRk0un3Uu30AnDixmIp6N1srGvD4Yo8HGJjtwOX1x/yM8jJsfHDjvLglJGFefyCmsKaIiIiIiLSvXwQxDMM4E/g/wAo8bJrm3Yn2VRAj/ZimSYPbx4FGL2UHmrBaDPbVudhV3YTNasFmMbBZDAbkOJk2LJ8hBRk4rJZuLx8If+qf47Rhmib+gIkvYBIwTdzeAHtqm6ls8OAPBCjJbVmy0OTxUVrRSIbdisWAA00e9te5Wb+njsoGN1aLgd1qoaLezcdbK8lx2oI3wG4/QwoyqKhzR26e25LrtDFrdCFD8jPZsK8Of6gYwoEmD5X1HnyBAF5/x353RwzIxGG14PEH8PgC7K9z47RZOGfaEL5+zEjGF+eSn2UHoLzOxWfbq3F5A9Q0eThiWD4jBmQxOC+DrRUN2K0WhhdmRgpM7q1tJmCaFGQ5mFCSQ73LhyX0b1LZ4GZRKLgwtjibwfmZkX/HQXkZTG+1pCIQMJOy1CFcD2JbZSNOm4WFmyupbvLg9QX4/+3deZRdVZXH8e+vxlQmDCSQoEmUKYiiTA3KGEQQURRUEBCBiEC3kW4MrYDKEkUhIKCIIqZFQFDEiRYUAVEUjSJhEhRlEoWQyAwxIVNV7f7jnEpe6Eqs4d3cui+/z1pZb7pVnNqcunXuvufss2DJcrq6gzv+/hz7bT2B9TpauW/+Alqbmli0rJOWJrHpuJFMGT+KrgjaW5qZOKaDx59fzH3zFjCivSX9LKOHscHINraZOIZmL88wMzMzM6urhk9iSGoGHgD2BuYCc4BDI+K+3o6vYhLjkafT2vpFSztZuLST7ggioDuC7kgXgCueRxA1z7u6g8UvucNee+0vVnnx/54Oa21m1LCWFRf4XTX/luUdEwCC/Ji71IqeFbHq65pjFi7t5NFnXuTae+bxYj9nAUisuGBe43Fr+KwzJweaxGqLJtZqa2lCpK0xV/ers+GodrojWNbZzYIlnWwwoo3tJo9h7Mh2lnZ28fTCZQxraWLbSWOYtP5wJm8wnBE5idId6ed61QYj/uUFfc/xyzq7eX7xMuY9v5gnFixl8w1HMqK9hfaWJoa1NjOifdVNiLq70/8pX3ybmZmZmdlQtKYkRqNssboj8FBE/BVA0neAdwK9JjGq6LyfPcC1f5hXdjMKsV5HKztvOpZtJ72Mke0tbDiqndEdrYxsb2HK+FF0R7C8K+js6ubhpxZx37wXWLSsiyXLu1abSOgRrPmACGhtbqKrO2jOswSaeh4lgmDrl7+MYa1NPPzUIv4yfwHNTaKtpYlXTxjN0s4ulncFG41OM0NeP3E9hretnV8rSTQLOtqa6WjrYMJ6HX36Ohd2NDMzMzOzqmqUJMbLgdq9BOcCO5XUlkJM33NT9thiHG0tTYwd2UaTlP+li9mmPCuhuUkrZiis/Bw62lrouXatvfBfdXZE1Dxf+f4/l3SypLOLliblC/2mFRf8PcsmeiZErLg8XvE6PVEvMzyklAwY2d73brjByHZ2fNX6fT6+nradNKaU/66ZmZmZmZkljZLE6O3W8iq34CUdCxwLMGlS79sRDmVbjh/NluNHl90MMzMzMzMzs9I0Stn8ucDEmtevAFZZexERsyJih4jYYdy4cWu1cWZmZmZmZmY2eI2SxJgDbC7pVZLagEOAa0puk5mZmZmZmZnVUUMsJ4mITkkfBm4gbbH6jYj4U8nNMjMzMzMzM7M6aogkBkBEXAdcV3Y7zMzMzMzMzKwYjbKcxMzMzMzMzMwanJMYZmZmZmZmZlYJTmKYmZmZmZmZWSU4iWFmZmZmZmZmleAkhpmZmZmZmZlVgpMYZmZmZmZmZlYJTmKYmZmZmZmZWSU4iWFmZmZmZmZmleAkhpmZmZmZmZlVgpMYZmZmZmZmZlYJTmKYmZmZmZmZWSU4iWFmZmZmZmZmleAkhpmZmZmZmZlVgpMYZmZmZmZmZlYJTmKYmZmZmZmZWSU4iWFmZmZmZmZmlaCIKLsNa52kp4C/l92OARgLPF12IxqcY1wsx7d4jnGxHN/iOcbFcnyL5xgXy/EtnmNcLMe3byZHxLjePlgnkxhVJen2iNih7HY0Mse4WI5v8RzjYjm+xXOMi+X4Fs8xLpbjWzzHuFiO7+B5OYmZmZmZmZmZVYKTGGZmZmZmZmZWCU5iVMusshuwDnCMi+X4Fs8xLpbjWzzHuFiOb/Ec42I5vsVzjIvl+A6Sa2KYmZmZmZmZWSV4JoaZmZmZmZmZVYKTGGZmZmZmZmZWCU5imJmZ2ZAmSWW3wczMzIaGlrIbYGsmaQqwPnA70B0RXSU3qaFJmgpsCLRExLdLbs46Q1JTRHSX3Q6z/pA0HOiKiKVlt6VRSdoAWBIRi8puSyOS1AzgsUVxfJ4olvtw8dyHi+X4DoxnYgxhkt4F/Aj4LHAxMF3S6HJb1bgk7QlcCUwCZki6UNLGJTerYUmaLOk1AD0JDN9trR9JW0raTVK7pJb8nuNbJ/n8fAXwU0lvl7RJ2W1qNDnGVwI/kXSMpJ3KblMjkXQg8A3gh5LeIGlU2W1qND5PFMt9uHjuw8VyfAfOu5MMUZJaSZ36SxExW9K7gTcAS4GzI2JBqQ1sMPni7ixgfkR8QdIwUuLoaeCMiHhCksK/MHWR+/MZwPz87yrgpohY6DgPXv6jOBN4hNSH7wAujYhnHd/Bk/Qq4AbgfcAU4I3Ak8CPIuLuMtvWKHIC+WbgUGAssAMpwfyDiPhZmW1rBJK2It0k+SCwDbAX8GPguoiYW2bbGoXPE8VyHy6e+3CxHN/B8UyMoW00sHl+fjXp5NwGHOY7qvWVL+ruBKZI2igilgDHABsBn6o5xgZJ0gjgCOB9ETEVuBXYg9SvRzjOg5NnXbwbODoi3gJ8j9SPPyppfce3LkYDcyNiTkRcAVxCWp65v6TJ5TatYbQCj0bEnRFxI/Ad4A/AgZK2L7dpDWEj4KGI+FVEnA98BdgJeIuk4R5j1IXPE8VyHy6e+3CxHN9BcBJjiIqI5cB5wLsk7Zan2/8GuBvYtdTGNRBJE/N0+w7gd8Ao4HWSOiLiRWAasJOkd5Ta0MbSDbwM2BQgDz5uA7YEpkKqkVFW4xqAgPWArQEi4n+Ba/Jnhzm2gxcRfwCel3R8fn07KcaTSP3YBiki/g48J+nc/PqvwI2ku1Rbg5dHDdJsYGHP37aIuAH4LvAeYCsnOwfP54nCuQ8XzH24WI7v4HgwO7T9mjRoe7+k3SOiKxeb3Bh4fblNqz5JbwN+ClxAWlO5jLT++gRgN0kTImIx8HPABaMGSUlzjukFwO6Stssff4e0rOQIWFkjw/pOUpukYTkBejbpbtSb88e/JSVAd8Pn/QGRNFXSwZLen9/6JjBZ0nsBImIOKRH6obwc0PpJ0t6Spkv6SH5rJtAi6aMAEfEwMAc4JPd1X6T0g6RdJL1Z0psjYhlpjLGzpDfCiovAnwPHO9k5MD5PFMt9uHjuw8VyfOvHv+BDWF7S8C3SFNpTJB0r6UjSFLr5pTauwvLF9ETSAPnDwKnAXcDv8+NXgcOBsySdCRwGPFhScxuCpHeSEkVfl7QLKdZPAO+QtH0knwfGuqhR/+UaI98GrpG0P/Aoaa3wwZL2zvG9CtiAtHbY+kGrFv09QdJ5wP2kmiM7SjoxH7oYWEiaDWP9IGlXUh9eArxH0hdIO3P9Apgo6Uv50JHAcqC5lIZWlKR9SH14H+Bzks4mDZ4BDpB0UH7+LPAi4ARRP/k8USz34eK5DxfL8a0vF/asAEltwC7AcaQB3vkRcVe5rao+SbOA00jFPCPf/TuBVEBVwL+RZrxcFRH3l9bQipP0elLhzhnAZOBDpMTRC8DuwCuAW4BO4JPAbhHxbDmtrR5JW5ASFh8AXkkqhPhT4AFSvN9DKhy1APgYsGdE/KOUxlZQXrLw0qK/l5AGHZcCmwHHkpaiTSDVevH5uZ8kzQBGRcSnc4xPJa0Nvh54ilSbaAQwETjCMe47pS0ovwHcEhEX57pEN5HuWJ9FKiq3D6kPTwQOcVG5vqtZ1uTzREHch4vlPlw8jyXqz0mMCskn8fBU+4HLd6k3A74MXA7cExFn1Hx+CqmY6n94v+b6kPQWUjwPyK/3A6YDXyPNyNiJdOL+J2nnHZ+0+0HSG4CZuUgqknYGjiLtSHINsAkpcfQicKHj23+SDiHVa/lU3qloOGnQ8XhEfCQfsznwbEQ8U1pDK0hKu+VI2gv4b+C/IuIBSe2kJHNHRJyQj90QWBoRL5TX4uroiW1+Ph0YDlwQEUskjSTNcrk5Ik7K44vtScVUneQcgDwdfCpwms8T9eE+vHa5DxfLY4n68nKSCsk1MZzAGKA8FfF04L5cN+Bk4N8lnVRz2JWkGQHLSmhio7obWCBpJ0lNEXEdqYr4mcAmEXENaTeNI32B3Xf5jx+koqiP5DWWLRHxW+Ay4ABgm4iYTao1cpzj23f610V/jyLVdTkQICIe9KCjfyTtRFrPLuC+/G9XpXpES4FPA7tI+iBARDzpBEa/bFjz/F7gzaTZWUTEwvx6V0lvzOOL23zx1z+Stq9ZAvkQ6SLb54n6cR8umPtwsTyWKI6TGLZOyHenLweOjYgbJI0F5pIu9E6QNCNPy58KbEfaPcMGKCcs9pC0Q0Q8AfwNeC8wSam453XARcBB+U7L0lyky/ogz26Znv8oQpp1sSNpMNeaExffAabl106A9oP6VvT3RdJUZvfbAch9+DJgSa7ZMp+0A9cuwH6Stsx1oa7BMe633IevlvQ/kj5DSiZfDXxT0pZ5AL2AlDjyWHAAch++inRRQkTcQZpdeDw+Twya+3Dx3IeL5bFEsVrKboDZWvIMqRjcBEkbAN8jzbj4E/B10hTEzYEdgGkR8VxZDa06SW8FvgTcDIyX9FBEzJB0EfCfpEHIr0lFt9p7popa3+T4ziRNu1+c37uUNBX/naT1wJeT4ruEtKWt9UGeEfAKVhb9/TNwJGlQ90ZWFv1F0uOkor9fL6WxFaZUxPMbwOERcYek0RGxICKulvQUcCBwhKS7gZ7pt9ZHkjYlDZqnkX7/9wSuA/YnnRe+ANwqqRvYi9TfrR+UCvR9GTgmIv4gaURELIqICyU9i88Tg+I+XDz34eJ4LLF2OIlh64SIuL8nqw+0kaYpXwx8kFS88+SIeEzSGCcwBi6vST0S+ExEXC5pNHCDpFkRcaykU4HjJH2CdLF9WJntrRpJWwEXAmdGxC9zQm4j4MWIOE3S4cC+ko4m7URyRER4e+A+yrUZ5pGmfD4IPBkRZ0taTtqm9g3Anaws+rtXRDxQWoOr63XAbOAZSZOBMyUtIt0NPDEiTpS0GzCFtP79oRLbWkXPADdGxK/yYPo3pKT9j4C3A/cAW5KS9m+LtHWt9c++wO3AbZImAadKWgY8B5xDStTvgM8TA/U0qdaF+3Bx9iHtfnir+3B95Ztzj0n6HanIuscSBXBhT1un5IvAPSPiKzXv3QCcEhF35qUN/qUYhFxjZF5EXF7z3m+B2RHxUUljgNcCj0TE3LLaWUWStgeOJv3xm0uaffEMMA6YExEn5eO2Bp6IiCfLamvVSNoMGAP8lZQouiMizq753EV/BynHuI20BeIhpCTF/sDZwK2kQfVuwEF5mrj1g6TXAGOBf5BmY303Is7JnzWRdnhZEhFn5vf8966fJL2W1IeXkW6CtJJmAnwNmAdsTTqPzPB5ov/yLK1XAt8i7Vr2k4iYmT9zH66DnCBen7Tz0+mk5dO74z5cF/IGAmuNZ2LYOiUieorHASDp3aRB3+P5c/8xHABJW9RkkR8HTpb0q4h4NL/3DmCWpK3y/4Nfl9LQiuqJb556P4y0beopwOdJA49XAJdK2j0ibomIe8tsb9VIejtwBukO1L2kAfSXcv2WM/NhVwIfx+tWB6Qmxs8D95MqsncBd0XErHzMPNJuOh7Y9VNeZnYWqf7QU6TC1RdIWhIRX46I7pxMPqDna/z3rn9yjM8mxfgF4FxSUb4Lem6MSJpPmgHnPtwPOUExnPT3rAVYCBwM3ChpcUSc7z48ODUxvoiUfOsgbWs/E/hqRJyfj3MfHiCt3EDgpIhYLulk4BZJXRFxVj7MY4k6cRLD1kl5euI00p3sg3LxSRuAfHHyXUnXRMQhEXGFpCnAbEm7RMSjEfF0nqY4quTmVk4v8Z2dpyTeEhFX58Mek/QY/qPYb0pFf88BDo2IuyTNIhVJ3Zk0zbaZVCR1V1YW/fWSs37oJcYXAYdExPFKW6n22IOUxOjAiYw+kzQVOJ9UY+Q2SdeStqw+gnTuaCLVF5gAbCFpVET8s7QGV1AvMb6GNIb+GKsWlZwMbCxpZKTdM6wPcuHphZIuIyU3DybNBtiLNJbozIki9+EB6iXG+wPDc82y2vOw+/AAaOUGAvvnc0TtBgI/yeO2H5PGFh5L1IGXk9g6KScx9gD+ERF/Kbs9VSVpBPAD4IekE3N7RByaPzudNAPjQtJsl8OB/SLikZKaWzm9xLctIg7Ln3XUFPZ8N+nO63si4u9ltbeK8sBji4i4NL8eB1waEW9T2nbuk6QCqTuSiv56lks/rSbGF5P667L83tGkwr+HRcSfymprFUl6NTA+Im6WNJ603Ox2UuFqSIVR7yUt1TnYfbj/eonxXcAc0vT7W4ErgGNJS0ze7z48MJJmAJOAa4FjSH14W9IY4hHSedh9eBB6ifHjwLKIOEXSCaTkp/twP+Wbdz8HppNquHyflRsI/JOUoF9AqjPyAffhwXMSw8wGRdLGpBPzMNI0xeU1iYwDgfGk3V++GBF/LK2hFdVLfJdGxPtqPj+SVP16muPbf3mmxYiIWJCfTyAN7vaLiPm58OTj+ZgXymxrVa0hxvtExFM5WfRhYJaTyoOjVDRZEfFZSceQ1rdfADwGjIyIp0ttYAN4SYynkYp8fhL4BHCOz8MDp7QryUERMVPSiaSlDqdHxGcktQGj3YcHp5cYnw5cEhHTJV0CnOs+PDCSXs/qNxCYGd5AoK6cxDCzulHaLWMWKat/aC40t9CzA+qjJr6LI+LwfHdwT+D6iPhrua2rPkktpGTRjyJiL6XdXnYDTuiZ9WKDs5oYbw18zsU860/S9cAnIuKOstvSqHKMP+Rz8ODlpP3nSDs4fIw0w2VH4NqIuKjMtjWKXmJ8ObATcBnw/bzsxAZI3kBgrXFNDDOrm4h4RtJxwOcl3Q80k6YyWx30El8Be0TE/JKb1hAiopO0ZvgxSWeSdss4ygmM+llNjKc5gTF4Lx0c52Vm48iFq23w1hBjnyPqICLm5fpOpwLTI+JaSXsC3ma5TlYT4zcBDzqBMXjeQGDt8UwMM6s7SR8BTgL29rq/+nN8i5Fr5bQCf86Pe0XEg+W2qrE4xsXLRfoOB2YA7/XU8PpzjIsjaSKwYc/sIUlNvriuL8e4eL1snHHkpgAABClJREFUIOAaI3XmJIaZ1ZWkMcB3gRMj4p6y29NoHN/iSToKmONBR3Ec4+JIagX2Bh6OiPvLbk8jcoyL52n3xXOMi+MNBIrnJIaZ1Z2kYRGxpOx2NCrHt1ge2BXPMTYzM7OBchLDzMzMzMzMzCqhqewGmJmZmZmZmZn1hZMYZmZmZmZmZlYJTmKYmZmZmZmZWSU4iWFmZmZmZmZmleAkhpmZma11kqZKipp/XZKek/RHSZdJ2jdvUzfQ77+NpNMkvbJ+rTYzM7OytZTdADMzM1unXQlcBwgYBUwBDgCOAG6SdFBEPD+A77sN8Cngl8Df6tJSMzMzK52TGGZmZlamOyPiito3JM0AzgZmkJIcby2jYWZmZjb0eDmJmZmZDSkR0RURJwK/AfaVtCuApI0lnSvp7rz0ZImk+ySdJKm55+slnQZckl/eXLNk5dKaY9olfVzSn/L3eV7StZK2XXs/qZmZmfWXZ2KYmZnZUHUxsCvwNlJC43XAu4CrgYeBVtIsjZnAJsBx+et+CEwAjgXOAP6c338YQFIrcD2wM3A58GVgPeAYYLak3SPi9oJ/NjMzMxsAJzHMzMxsqLonP26RH38FbBIRUXPMFyVdDnxQ0mkRMT8i7pH0O1IS42cR8cuXfN8PA1OBfSPihp43JV0I/BE4J39uZmZmQ4yXk5iZmdlQtSA/jgaIiMU9CQxJbZLWlzQWuIE0ptmhj9/3cOAvwB2Sxvb8A9qAnwG7Suqo5w9iZmZm9eGZGGZmZjZUjc6PCwAktQAnk3Yu2Yy0o0mtMX38vq8GOoCn1nDMWOCxPrfUzMzM1gonMczMzGyoel1+vD8/ngccD1wFfA54ElgObAecRd9nmAq4l7T7yeqsKcFhZmZmJXESw8zMzIaqo/PjT/Lj+4FbIuKQ2oMkbdbL10Yv7/V4EBgH/CIiugfdSjMzM1trXBPDzMzMhhRJzZLOIe1Mcl1EzM4fdfGSJSSSRgAf6eXbLMyP6/fy2TeB8axmJoakjQbSbjMzMyueZ2KYmZlZmbaTdHh+PgqYAhwATAZuBA6rOfb7wHGSrgJuAjYCPgA808v3nQN0A5+QNAZYBDwSEb8Hzgf2Bj4v6U3AL0h1NyYBewFLgD3r+UOamZlZfWjVXcrMzMzMiidpKnBzzVvdpNkTc4HbgSsj4vqXfM1w4NPAwaQExmPAxaSExU3AtIi4tOb4I4GTSEVAW4HLIuKo/FkL8CHSEpWt8pfMA27Lx91Ytx/WzMzM6sZJDDMzMzMzMzOrBNfEMDMzMzMzM7NKcBLDzMzMzMzMzCrBSQwzMzMzMzMzqwQnMczMzMzMzMysEpzEMDMzMzMzM7NKcBLDzMzMzMzMzCrBSQwzMzMzMzMzqwQnMczMzMzMzMysEpzEMDMzMzMzM7NKcBLDzMzMzMzMzCrh/wCouAZBkrHoxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1296x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(abl.shape[0]), abl['Open'])\n",
    "plt.xticks(range(0, abl.shape[0], 500), rotation=45)\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Open', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-Train-Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data is split into 3 parts. \n",
    "\n",
    "#### The first is the train part where we provide the model real prices as input and labels. It analyses the prices and attempts to predict the next day price. When predicted, the actual price for that day is given so it can learn accordingly.\n",
    "\n",
    "#### The second is the test part, where we provide the model data it has not seen before and ask it to predict using this new data entirely.\n",
    "\n",
    "#### The third part is the validation part. We keep some data never shown to the model in train and test parts and then ask it to predict this without ever seeing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(df, column):\n",
    "\n",
    "    prices = df[column] #We choose to predict the price of the given column\n",
    "\n",
    "    # 80-20 split into Train-Test\n",
    "\n",
    "    train_data = prices[:int(len(prices)*0.7)]\n",
    "    test_data = prices[int(len(prices)*0.7):int(len(prices)*0.8)]\n",
    "    val_data = prices[int(len(prices)*0.8):]\n",
    "    \n",
    "    #Convert to ndarray\n",
    "\n",
    "    train_data = np.asarray(train_data)\n",
    "    test_data = np.asarray(test_data)\n",
    "    val_data = np.asarray(val_data)\n",
    "    \n",
    "    #Transform data\n",
    "    \n",
    "    train_data = train_data.reshape(-1, 1)\n",
    "    test_data = test_data.reshape(-1, 1)\n",
    "    val_data = val_data.reshape(-1, 1)\n",
    "    \n",
    "\n",
    "    print(train_data.shape[0])\n",
    "    print(test_data.shape[0])\n",
    "    print(val_data.shape[0])\n",
    "\n",
    "    \n",
    "    return train_data, test_data, val_data\n",
    "    \n",
    "#train_data, test_data, val_data = test_train_split(abl, 'Open')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check distribution of data with scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A recurrent neural network works better when data is squished between a limit. We want to see if scaling our data this way has any change on its distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'fig1, ax1 = plt.subplots()\\nax1.set_title('Data Distribution')\\nax1.boxplot(train_data, patch_artist=True)\\n\\n#tempScaler = MinMaxScaler()\\n#scaled_data = tempScaler.fit_transform(train_data)\\n\\nfig1, ax1 = plt.subplots()\\nax1.set_title('Scaled Data Distribution')\\nax1.boxplot(scaled_data, patch_artist=True)\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Data Distribution')\n",
    "ax1.boxplot(train_data, patch_artist=True)\n",
    "\n",
    "#tempScaler = MinMaxScaler()\n",
    "#scaled_data = tempScaler.fit_transform(train_data)\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Scaled Data Distribution')\n",
    "ax1.boxplot(scaled_data, patch_artist=True)'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see scaling our data does not change the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data is prepared with the following methodology:\n",
    "\n",
    "#### We take a window (set of days) to analyze the recent stock movemement. Based on that analysis, and the weight of historical learning (previous analyses), a prediction for  the window+1 day is made. The model compares the predicted price with the original price and adjusts its parameters. This is done iteratively, with a step of window+1 in each iteration. \n",
    "\n",
    "#### For example, for a window of 5, the model would look at day 1 to day 5 and predict day 6. Once day 6 is predicted, it would adjust parameters and then move onto the next window of day 7 to day 11 and predict day 12 and so on.\n",
    "\n",
    "#### The days in the window are stored in x_train (the features). The days to be predicted are stored in y_train (labels). This results in two arrays, x_train and y_train which we would feed into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_prep(train_data, window):\n",
    "\n",
    "    scaler = MinMaxScaler((0, 1))\n",
    "    scaled_train_data = scaler.fit_transform(train_data)\n",
    "\n",
    "    x_train = [] #Features\n",
    "    y_train = [] #Labels\n",
    "\n",
    "    for i in range(window, scaled_train_data.shape[0], window+1): #Make windows of 60 days for training until last day\n",
    "\n",
    "        x_train.append(scaled_train_data[i-window:i]) #n day window to look back\n",
    "        y_train.append(scaled_train_data[i:i+1, 0]) #predict for specified days\n",
    "\n",
    "    x_train = np.array(x_train) \n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    print(x_train.shape, y_train.shape, scaler.scale_)\n",
    "    \n",
    "    return x_train, y_train, scaler.scale_\n",
    "    \n",
    "#x_train, y_train, scale_train = train_prep(train_data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare test and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exactly the same way as train set, we will make the test set. The only difference is that the test set takes the last window from the train set so it can predict in continuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prep(test_data, train_data, window):\n",
    "    \n",
    "    scaler = MinMaxScaler((0, 1))\n",
    "    test_data = np.concatenate((train_data[:window], test_data), axis = 0) #Add last previous observations of train data to test\n",
    "    scaled_test_data = scaler.fit_transform(test_data)\n",
    "\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for i in range(window, test_data.shape[0], window+1):\n",
    "        x_test.append(scaled_test_data[i-window:i])\n",
    "        y_test.append(scaled_test_data[i:i+1, 0])\n",
    "\n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "        \n",
    "    print(x_test.shape, y_test.shape, scaler.scale_)\n",
    "    return x_test, y_test, scaler.scale_\n",
    "    \n",
    "#x_test, y_test, test_scale = test_prep(test_data, train_data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_prep(val_data, test_data, window):\n",
    "    \n",
    "    scaler = MinMaxScaler((0, 1))\n",
    "    val_data = np.concatenate((test_data[:window], val_data), axis = 0) #Add last previous observations of train data to test\n",
    "    scaled_val_data = scaler.fit_transform(val_data)\n",
    "\n",
    "    x_val = []\n",
    "    y_val = []\n",
    "\n",
    "    for i in range(window, val_data.shape[0], window+1):\n",
    "        x_val.append(scaled_val_data[i-window:i])\n",
    "        y_val.append(scaled_val_data[i:i+1, 0])\n",
    "\n",
    "    x_val = np.array(x_val)\n",
    "    y_val = np.array(y_val)\n",
    "        \n",
    "    print(x_val.shape, y_val.shape, scaler.scale_)\n",
    "    return x_val, y_val, scaler.scale_\n",
    "    \n",
    "#x_val, y_val, val_scale = val_prep(val_data, test_data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We chose the GRU model as it performed the best on our problem. The architecture is such that there are two layers of GRU. The first layer takes the input as we defined in train and test sets. It then forwards its results to the second layer which works similarly. Both layers are activated by ReLU on the output to ensure non-linearity of our problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train, units_1, units_2):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(GRU(units = units_1, activation = 'relu', return_sequences = True, input_shape = (x_train.shape[1], x_train.shape[2])))\n",
    "\n",
    "    model.add(GRU(units = units_2, activation = 'relu'))\n",
    "\n",
    "    model.add(Dense(units = 1))\n",
    "    \n",
    "    return model\n",
    "    \n",
    "#model = create_model(x_train, 16, 32)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "#plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADAM is an optimizer used to optimize the Stochastic Gradient Descent used to minimize the error the model makes. Mean Squared Error is the error or loss function used to tell the model how to minimize the error in each step. The number of epochs is defined as how many times the model would need to go through the whole data in order to reach the minimum point of the loss. We have chosen the number at which we are sure the loss becomes constant and does not drop significiantly anymore. The batch size is defined as how many training examples should it put in one batch since the parameters are adjusted after each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compile_model(model, x_train, y_train, x_test, y_test):\n",
    "\n",
    "    model.compile(optimizer='adam', loss= 'mean_squared_error')\n",
    "    history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data = (x_test, y_test), verbose = 1)\n",
    "    return history\n",
    "\n",
    "#history = compile_model(model, x_train, y_train, x_test, y_test)\n",
    "#model.save(\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the part where we test our results. y_hat is an array which holds the predictions done on the test dataset we created earlier. After we have y_hat, we revert the scale of data to get original prices once again. These prices are then plotted to closely see how our model is behaving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_hat = model.predict(x_test) #Prediction over test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_hat_unscaled = y_hat/test_scale[0]\n",
    "#y_test_unscaled = y_test/test_scale[0]\n",
    "\n",
    "#Scaling back to original values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(y_test, y_hat):\n",
    "\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.plot(y_test[:], color = 'red', label = 'Real Stock Price')\n",
    "    plt.plot(y_hat[:], color = 'blue', label = 'Predicted Stock Price')\n",
    "    plt.title('Stock Price Prediction')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "#plot_comparison(y_test_unscaled, y_hat_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plot space\n",
    "#fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create scatter plot\n",
    "#ax.scatter(range(y_test.shape[0]), \n",
    "#           y_test)\n",
    "\n",
    "#ax.scatter(range(y_test.shape[0]), \n",
    "#           y_hat)\n",
    "\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_hat_val = model.predict(x_val) #Prediction over val set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_hat_val_unscaled = y_hat_val/val_scale[0]\n",
    "#y_val_unscaled = y_val/val_scale[0]\n",
    "\n",
    "#Scaling back to original values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_comparison(y_val_unscaled, y_hat_val_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "KSE100 = pd.read_csv('KSE100.csv')\n",
    "filtered = KSE100['Ticker'].tolist()\n",
    "name = []\n",
    "for i in range(len(filtered)):\n",
    "    name.append(str(filtered[i]) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2417\n",
      "346\n",
      "691\n",
      "(402, 5, 1) (402, 1) [0.00538974]\n",
      "(58, 5, 1) (58, 1) [0.00489284]\n",
      "(116, 5, 1) (116, 1) [0.00905371]\n",
      "WARNING:tensorflow:From C:\\Users\\CZ\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 402 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "402/402 [==============================] - 1s 3ms/step - loss: 0.1927 - val_loss: 0.3422\n",
      "Epoch 2/10\n",
      "402/402 [==============================] - 0s 268us/step - loss: 0.1435 - val_loss: 0.2570\n",
      "Epoch 3/10\n",
      "402/402 [==============================] - 0s 221us/step - loss: 0.0996 - val_loss: 0.1673\n",
      "Epoch 4/10\n",
      "402/402 [==============================] - 0s 241us/step - loss: 0.0566 - val_loss: 0.0795\n",
      "Epoch 5/10\n",
      "402/402 [==============================] - 0s 270us/step - loss: 0.0262 - val_loss: 0.0214\n",
      "Epoch 6/10\n",
      "402/402 [==============================] - 0s 238us/step - loss: 0.0150 - val_loss: 0.0095\n",
      "Epoch 7/10\n",
      "402/402 [==============================] - 0s 205us/step - loss: 0.0078 - val_loss: 0.0093\n",
      "Epoch 8/10\n",
      "402/402 [==============================] - 0s 247us/step - loss: 0.0019 - val_loss: 0.0064\n",
      "Epoch 9/10\n",
      "402/402 [==============================] - 0s 281us/step - loss: 3.6987e-04 - val_loss: 0.0068\n",
      "Epoch 10/10\n",
      "402/402 [==============================] - 0s 178us/step - loss: 2.3051e-04 - val_loss: 0.0063\n",
      "2778\n",
      "397\n",
      "794\n",
      "(463, 5, 1) (463, 1) [0.00836881]\n",
      "(67, 5, 1) (67, 1) [0.00473244]\n",
      "(133, 5, 1) (133, 1) [0.01112851]\n",
      "Train on 463 samples, validate on 67 samples\n",
      "Epoch 1/10\n",
      "463/463 [==============================] - 1s 3ms/step - loss: 0.1817 - val_loss: 0.3937\n",
      "Epoch 2/10\n",
      "463/463 [==============================] - 0s 249us/step - loss: 0.1186 - val_loss: 0.2288\n",
      "Epoch 3/10\n",
      "463/463 [==============================] - 0s 272us/step - loss: 0.0614 - val_loss: 0.0742\n",
      "Epoch 4/10\n",
      "463/463 [==============================] - 0s 270us/step - loss: 0.0282 - val_loss: 0.0174\n",
      "Epoch 5/10\n",
      "463/463 [==============================] - 0s 282us/step - loss: 0.0108 - val_loss: 0.0059\n",
      "Epoch 6/10\n",
      "463/463 [==============================] - 0s 383us/step - loss: 0.0010 - val_loss: 0.0041\n",
      "Epoch 7/10\n",
      "463/463 [==============================] - 0s 263us/step - loss: 5.3991e-04 - val_loss: 0.0040\n",
      "Epoch 8/10\n",
      "463/463 [==============================] - 0s 300us/step - loss: 2.9414e-04 - val_loss: 0.0035\n",
      "Epoch 9/10\n",
      "463/463 [==============================] - 0s 387us/step - loss: 2.7251e-04 - val_loss: 0.0037\n",
      "Epoch 10/10\n",
      "463/463 [==============================] - 0s 368us/step - loss: 2.5969e-04 - val_loss: 0.0037\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.02252658]\n",
      "(86, 5, 1) (86, 1) [0.00944901]\n",
      "(172, 5, 1) (172, 1) [0.01279671]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 1s 2ms/step - loss: 0.0822 - val_loss: 0.2485\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 218us/step - loss: 0.0358 - val_loss: 0.0828\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 234us/step - loss: 0.0146 - val_loss: 0.0214\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 242us/step - loss: 0.0021 - val_loss: 0.0029\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 226us/step - loss: 3.3780e-04 - val_loss: 0.0020\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 232us/step - loss: 9.4670e-05 - val_loss: 0.0020\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 212us/step - loss: 6.6885e-05 - val_loss: 0.0019\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 236us/step - loss: 6.6167e-05 - val_loss: 0.0020\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 254us/step - loss: 6.3871e-05 - val_loss: 0.0020\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 239us/step - loss: 6.2066e-05 - val_loss: 0.0019\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.0051931]\n",
      "(86, 5, 1) (86, 1) [0.00462417]\n",
      "(172, 5, 1) (172, 1) [0.01178935]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 1s 2ms/step - loss: 0.1100 - val_loss: 0.3323\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 234us/step - loss: 0.0488 - val_loss: 0.1179\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 231us/step - loss: 0.0117 - val_loss: 0.0143\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 243us/step - loss: 6.7448e-04 - val_loss: 0.0098\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 256us/step - loss: 2.7117e-04 - val_loss: 0.0079\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 241us/step - loss: 1.6862e-04 - val_loss: 0.0083\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 241us/step - loss: 1.6353e-04 - val_loss: 0.0082\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 262us/step - loss: 1.6173e-04 - val_loss: 0.0082\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 212us/step - loss: 1.6257e-04 - val_loss: 0.0082\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 261us/step - loss: 1.6546e-04 - val_loss: 0.0082\n",
      "3054\n",
      "437\n",
      "873\n",
      "(509, 5, 1) (509, 1) [0.00531301]\n",
      "(73, 5, 1) (73, 1) [0.00746703]\n",
      "(146, 5, 1) (146, 1) [0.0127982]\n",
      "Train on 509 samples, validate on 73 samples\n",
      "Epoch 1/10\n",
      "509/509 [==============================] - 1s 3ms/step - loss: 0.1302 - val_loss: 0.1822\n",
      "Epoch 2/10\n",
      "509/509 [==============================] - 0s 227us/step - loss: 0.0445 - val_loss: 0.0365\n",
      "Epoch 3/10\n",
      "509/509 [==============================] - 0s 201us/step - loss: 0.0152 - val_loss: 0.0081\n",
      "Epoch 4/10\n",
      "509/509 [==============================] - 0s 198us/step - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 5/10\n",
      "509/509 [==============================] - 0s 242us/step - loss: 7.6634e-04 - val_loss: 0.0051\n",
      "Epoch 6/10\n",
      "509/509 [==============================] - 0s 239us/step - loss: 3.0946e-04 - val_loss: 0.0048\n",
      "Epoch 7/10\n",
      "509/509 [==============================] - 0s 229us/step - loss: 1.9171e-04 - val_loss: 0.0047\n",
      "Epoch 8/10\n",
      "509/509 [==============================] - 0s 266us/step - loss: 1.8337e-04 - val_loss: 0.0048\n",
      "Epoch 9/10\n",
      "509/509 [==============================] - ETA: 0s - loss: 1.6854e-0 - 0s 225us/step - loss: 1.8147e-04 - val_loss: 0.0048\n",
      "Epoch 10/10\n",
      "509/509 [==============================] - 0s 229us/step - loss: 1.6757e-04 - val_loss: 0.0048\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00263141]\n",
      "(86, 5, 1) (86, 1) [0.00145407]\n",
      "(172, 5, 1) (172, 1) [0.00158477]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 1s 2ms/step - loss: 0.0701 - val_loss: 0.3974\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 232us/step - loss: 0.0399 - val_loss: 0.2177\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 223us/step - loss: 0.0208 - val_loss: 0.0836\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 221us/step - loss: 0.0077 - val_loss: 0.0119\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 218us/step - loss: 6.7535e-04 - val_loss: 0.0069\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 217us/step - loss: 2.6985e-04 - val_loss: 0.0036\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 219us/step - loss: 1.3238e-04 - val_loss: 0.0037\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 214us/step - loss: 1.0386e-04 - val_loss: 0.0037\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 224us/step - loss: 9.3317e-05 - val_loss: 0.0037\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 215us/step - loss: 8.1340e-05 - val_loss: 0.0036\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00816324]\n",
      "(86, 5, 1) (86, 1) [0.00448285]\n",
      "(172, 5, 1) (172, 1) [0.00397365]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 2s 3ms/step - loss: 0.1294 - val_loss: 0.3553\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 225us/step - loss: 0.0450 - val_loss: 0.1073\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 222us/step - loss: 0.0144 - val_loss: 0.0196\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 233us/step - loss: 0.0058 - val_loss: 0.0095\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 237us/step - loss: 8.7407e-04 - val_loss: 0.0022\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 236us/step - loss: 3.6593e-04 - val_loss: 0.0023\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 248us/step - loss: 2.9294e-04 - val_loss: 0.0026\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 224us/step - loss: 2.7068e-04 - val_loss: 0.0025\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 221us/step - loss: 2.5002e-04 - val_loss: 0.0024\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 228us/step - loss: 2.4800e-04 - val_loss: 0.0023\n",
      "2970\n",
      "425\n",
      "849\n",
      "(495, 5, 1) (495, 1) [0.00775192]\n",
      "(71, 5, 1) (71, 1) [0.00899474]\n",
      "(142, 5, 1) (142, 1) [0.0112093]\n",
      "Train on 495 samples, validate on 71 samples\n",
      "Epoch 1/10\n",
      "495/495 [==============================] - 2s 3ms/step - loss: 0.2019 - val_loss: 0.4539\n",
      "Epoch 2/10\n",
      "495/495 [==============================] - 0s 224us/step - loss: 0.0880 - val_loss: 0.1904\n",
      "Epoch 3/10\n",
      "495/495 [==============================] - 0s 214us/step - loss: 0.0321 - val_loss: 0.0508\n",
      "Epoch 4/10\n",
      "495/495 [==============================] - 0s 224us/step - loss: 0.0096 - val_loss: 0.0110\n",
      "Epoch 5/10\n",
      "495/495 [==============================] - 0s 221us/step - loss: 0.0011 - val_loss: 0.0049\n",
      "Epoch 6/10\n",
      "495/495 [==============================] - 0s 219us/step - loss: 4.3221e-04 - val_loss: 0.0046\n",
      "Epoch 7/10\n",
      "495/495 [==============================] - 0s 232us/step - loss: 2.3080e-04 - val_loss: 0.0043\n",
      "Epoch 8/10\n",
      "495/495 [==============================] - 0s 223us/step - loss: 1.8970e-04 - val_loss: 0.0046\n",
      "Epoch 9/10\n",
      "495/495 [==============================] - 0s 252us/step - loss: 1.5318e-04 - val_loss: 0.0046\n",
      "Epoch 10/10\n",
      "495/495 [==============================] - 0s 236us/step - loss: 1.4257e-04 - val_loss: 0.0047\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.0148373]\n",
      "(86, 5, 1) (86, 1) [0.01008657]\n",
      "(172, 5, 1) (172, 1) [0.016618]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 2s 3ms/step - loss: 0.1327 - val_loss: 0.3592\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 230us/step - loss: 0.0669 - val_loss: 0.1379\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 220us/step - loss: 0.0185 - val_loss: 0.0123\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 230us/step - loss: 0.0010 - val_loss: 0.0074\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 225us/step - loss: 3.7289e-04 - val_loss: 0.0051\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 231us/step - loss: 1.3510e-04 - val_loss: 0.0054\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 236us/step - loss: 9.5685e-05 - val_loss: 0.0054\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 228us/step - loss: 7.5854e-05 - val_loss: 0.0054\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 243us/step - loss: 6.7561e-05 - val_loss: 0.0054\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 239us/step - loss: 6.7409e-05 - val_loss: 0.0055\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00512841]\n",
      "(86, 5, 1) (86, 1) [0.00518394]\n",
      "(172, 5, 1) (172, 1) [0.00722163]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 2s 3ms/step - loss: 0.0645 - val_loss: 0.2047\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 249us/step - loss: 0.0141 - val_loss: 0.0376\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 232us/step - loss: 0.0069 - val_loss: 0.0347\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 241us/step - loss: 0.0021 - val_loss: 0.0083\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 402us/step - loss: 2.5597e-04 - val_loss: 0.0074\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 346us/step - loss: 2.1566e-04 - val_loss: 0.0070\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 324us/step - loss: 1.6904e-04 - val_loss: 0.0069\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 339us/step - loss: 1.6642e-04 - val_loss: 0.0069\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 300us/step - loss: 1.6101e-04 - val_loss: 0.0070\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 260us/step - loss: 1.5986e-04 - val_loss: 0.0070\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.01860392]\n",
      "(86, 5, 1) (86, 1) [0.00903012]\n",
      "(172, 5, 1) (172, 1) [0.0080969]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 2s 3ms/step - loss: 0.1290 - val_loss: 0.2439\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 229us/step - loss: 0.0403 - val_loss: 0.0423\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 228us/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 241us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 203us/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 255us/step - loss: 4.3991e-04 - val_loss: 0.0023\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 231us/step - loss: 3.5193e-04 - val_loss: 0.0022\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 235us/step - loss: 3.1362e-04 - val_loss: 0.0023\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 233us/step - loss: 2.9907e-04 - val_loss: 0.0023\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 232us/step - loss: 2.9406e-04 - val_loss: 0.0023\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.01267859]\n",
      "(86, 5, 1) (86, 1) [0.00558107]\n",
      "(172, 5, 1) (172, 1) [0.0055875]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 2s 3ms/step - loss: 0.1702 - val_loss: 0.1900\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 204us/step - loss: 0.0438 - val_loss: 0.0084\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 227us/step - loss: 0.0110 - val_loss: 0.0050\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 231us/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 234us/step - loss: 4.6542e-04 - val_loss: 0.0016\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 230us/step - loss: 3.2732e-04 - val_loss: 0.0017\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 244us/step - loss: 3.1546e-04 - val_loss: 0.0017\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 235us/step - loss: 3.1324e-04 - val_loss: 0.0017\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 232us/step - loss: 3.0610e-04 - val_loss: 0.0017\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 239us/step - loss: 3.0563e-04 - val_loss: 0.0017\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00408506]\n",
      "(86, 5, 1) (86, 1) [0.00423303]\n",
      "(172, 5, 1) (172, 1) [0.00435504]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 2s 4ms/step - loss: 0.1206 - val_loss: 0.3443\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 245us/step - loss: 0.0690 - val_loss: 0.1854\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 240us/step - loss: 0.0306 - val_loss: 0.0529\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 251us/step - loss: 0.0058 - val_loss: 0.0111\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 233us/step - loss: 3.3389e-04 - val_loss: 0.0104\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 251us/step - loss: 1.0286e-04 - val_loss: 0.0101\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 249us/step - loss: 6.5754e-05 - val_loss: 0.0101\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 249us/step - loss: 6.1237e-05 - val_loss: 0.0101\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 254us/step - loss: 6.0662e-05 - val_loss: 0.0102\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 234us/step - loss: 6.1602e-05 - val_loss: 0.0102\n",
      "1227\n",
      "176\n",
      "351\n",
      "(204, 5, 1) (204, 1) [0.02467059]\n",
      "(30, 5, 1) (30, 1) [0.02250281]\n",
      "(59, 5, 1) (59, 1) [0.04327824]\n",
      "Train on 204 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "204/204 [==============================] - 2s 11ms/step - loss: 0.3868 - val_loss: 0.8194\n",
      "Epoch 2/10\n",
      "204/204 [==============================] - 0s 300us/step - loss: 0.3446 - val_loss: 0.7463\n",
      "Epoch 3/10\n",
      "204/204 [==============================] - 0s 259us/step - loss: 0.3051 - val_loss: 0.6726\n",
      "Epoch 4/10\n",
      "204/204 [==============================] - 0s 284us/step - loss: 0.2695 - val_loss: 0.6084\n",
      "Epoch 5/10\n",
      "204/204 [==============================] - 0s 259us/step - loss: 0.2347 - val_loss: 0.5383\n",
      "Epoch 6/10\n",
      "204/204 [==============================] - 0s 241us/step - loss: 0.1949 - val_loss: 0.4584\n",
      "Epoch 7/10\n",
      "204/204 [==============================] - 0s 260us/step - loss: 0.1521 - val_loss: 0.3653\n",
      "Epoch 8/10\n",
      "204/204 [==============================] - 0s 243us/step - loss: 0.1054 - val_loss: 0.2627\n",
      "Epoch 9/10\n",
      "204/204 [==============================] - 0s 264us/step - loss: 0.0614 - val_loss: 0.1585\n",
      "Epoch 10/10\n",
      "204/204 [==============================] - 0s 255us/step - loss: 0.0280 - val_loss: 0.0709\n",
      "2827\n",
      "404\n",
      "808\n",
      "(471, 5, 1) (471, 1) [0.02125137]\n",
      "(68, 5, 1) (68, 1) [0.02074409]\n",
      "(135, 5, 1) (135, 1) [0.0275051]\n",
      "Train on 471 samples, validate on 68 samples\n",
      "Epoch 1/10\n",
      "471/471 [==============================] - 2s 5ms/step - loss: 0.1259 - val_loss: 0.6620\n",
      "Epoch 2/10\n",
      "471/471 [==============================] - 0s 263us/step - loss: 0.0928 - val_loss: 0.4990\n",
      "Epoch 3/10\n",
      "471/471 [==============================] - 0s 256us/step - loss: 0.0592 - val_loss: 0.3058\n",
      "Epoch 4/10\n",
      "471/471 [==============================] - 0s 244us/step - loss: 0.0326 - val_loss: 0.1311\n",
      "Epoch 5/10\n",
      "471/471 [==============================] - 0s 229us/step - loss: 0.0119 - val_loss: 0.0352\n",
      "Epoch 6/10\n",
      "471/471 [==============================] - 0s 233us/step - loss: 0.0012 - val_loss: 0.0097\n",
      "Epoch 7/10\n",
      "471/471 [==============================] - 0s 239us/step - loss: 3.9023e-04 - val_loss: 0.0093\n",
      "Epoch 8/10\n",
      "471/471 [==============================] - 0s 229us/step - loss: 1.0736e-04 - val_loss: 0.0090\n",
      "Epoch 9/10\n",
      "471/471 [==============================] - 0s 241us/step - loss: 9.5862e-05 - val_loss: 0.0091\n",
      "Epoch 10/10\n",
      "471/471 [==============================] - 0s 254us/step - loss: 8.2773e-05 - val_loss: 0.0091\n",
      "3404\n",
      "486\n",
      "973\n",
      "(567, 5, 1) (567, 1) [0.01996438]\n",
      "(81, 5, 1) (81, 1) [0.01428912]\n",
      "(163, 5, 1) (163, 1) [0.02003229]\n",
      "Train on 567 samples, validate on 81 samples\n",
      "Epoch 1/10\n",
      "567/567 [==============================] - 2s 4ms/step - loss: 0.1515 - val_loss: 0.2499\n",
      "Epoch 2/10\n",
      "567/567 [==============================] - 0s 252us/step - loss: 0.0649 - val_loss: 0.0664\n",
      "Epoch 3/10\n",
      "567/567 [==============================] - 0s 235us/step - loss: 0.0163 - val_loss: 0.0101\n",
      "Epoch 4/10\n",
      "567/567 [==============================] - 0s 239us/step - loss: 0.0034 - val_loss: 0.0057\n",
      "Epoch 5/10\n",
      "567/567 [==============================] - 0s 242us/step - loss: 4.3769e-04 - val_loss: 0.0061\n",
      "Epoch 6/10\n",
      "567/567 [==============================] - 0s 231us/step - loss: 2.8588e-04 - val_loss: 0.0057\n",
      "Epoch 7/10\n",
      "567/567 [==============================] - 0s 229us/step - loss: 2.4230e-04 - val_loss: 0.0058\n",
      "Epoch 8/10\n",
      "567/567 [==============================] - 0s 252us/step - loss: 2.2247e-04 - val_loss: 0.0059\n",
      "Epoch 9/10\n",
      "567/567 [==============================] - 0s 236us/step - loss: 2.1542e-04 - val_loss: 0.0059\n",
      "Epoch 10/10\n",
      "567/567 [==============================] - 0s 248us/step - loss: 2.1632e-04 - val_loss: 0.0058\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [9.31003585e-05]\n",
      "(86, 5, 1) (86, 1) [0.00011117]\n",
      "(172, 5, 1) (172, 1) [0.00014454]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 3s 4ms/step - loss: 0.0452 - val_loss: 0.4699\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 251us/step - loss: 0.0259 - val_loss: 0.2873\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 249us/step - loss: 0.0135 - val_loss: 0.1353\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 205us/step - loss: 0.0048 - val_loss: 0.0349\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 264us/step - loss: 3.1216e-04 - val_loss: 0.0085\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 241us/step - loss: 1.7471e-04 - val_loss: 0.0099\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 249us/step - loss: 1.1203e-04 - val_loss: 0.0089\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 245us/step - loss: 1.0311e-04 - val_loss: 0.0079\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 239us/step - loss: 1.0155e-04 - val_loss: 0.0078\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 254us/step - loss: 1.0446e-04 - val_loss: 0.0075\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.01084632]\n",
      "(86, 5, 1) (86, 1) [0.00301174]\n",
      "(172, 5, 1) (172, 1) [0.00256473]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 3s 5ms/step - loss: 0.0165 - val_loss: 0.1610\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 199us/step - loss: 0.0080 - val_loss: 0.0799\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 194us/step - loss: 0.0033 - val_loss: 0.0185\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 202us/step - loss: 3.8004e-04 - val_loss: 0.0012\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 193us/step - loss: 1.1488e-04 - val_loss: 0.0017\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 216us/step - loss: 9.1645e-05 - val_loss: 0.0012\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 203us/step - loss: 6.9700e-05 - val_loss: 0.0012\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 192us/step - loss: 6.5635e-05 - val_loss: 0.0012\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 197us/step - loss: 6.4254e-05 - val_loss: 0.0012\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 212us/step - loss: 6.2089e-05 - val_loss: 0.0011\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.02799999]\n",
      "(86, 5, 1) (86, 1) [0.02487841]\n",
      "(172, 5, 1) (172, 1) [0.0216506]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 3s 5ms/step - loss: 0.1217 - val_loss: 0.6994\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 252us/step - loss: 0.0883 - val_loss: 0.5641\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 228us/step - loss: 0.0642 - val_loss: 0.4152\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 237us/step - loss: 0.0372 - val_loss: 0.2160\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 231us/step - loss: 0.0117 - val_loss: 0.0349\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 241us/step - loss: 7.5028e-04 - val_loss: 0.0095\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 231us/step - loss: 1.3800e-04 - val_loss: 0.0087\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 235us/step - loss: 9.1517e-05 - val_loss: 0.0089\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 235us/step - loss: 7.5101e-05 - val_loss: 0.0089\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 266us/step - loss: 7.0494e-05 - val_loss: 0.0089\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.0669053]\n",
      "(86, 5, 1) (86, 1) [0.02913184]\n",
      "(172, 5, 1) (172, 1) [0.03554507]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 3s 5ms/step - loss: 0.0766 - val_loss: 0.0902\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 250us/step - loss: 0.0158 - val_loss: 0.0052\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 240us/step - loss: 0.0063 - val_loss: 0.0090\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 254us/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 259us/step - loss: 4.2663e-04 - val_loss: 0.0019\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 259us/step - loss: 3.5558e-04 - val_loss: 0.0019\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 252us/step - loss: 3.3294e-04 - val_loss: 0.0019\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 241us/step - loss: 3.3938e-04 - val_loss: 0.0019\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 242us/step - loss: 3.4863e-04 - val_loss: 0.0020\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 256us/step - loss: 3.3249e-04 - val_loss: 0.0020\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.03109298]\n",
      "(86, 5, 1) (86, 1) [0.01268366]\n",
      "(172, 5, 1) (172, 1) [0.01119163]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 3s 5ms/step - loss: 0.0887 - val_loss: 0.1880\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 246us/step - loss: 0.0274 - val_loss: 0.0361\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 247us/step - loss: 0.0098 - val_loss: 0.0094\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 237us/step - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 257us/step - loss: 3.9163e-04 - val_loss: 0.0011\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 244us/step - loss: 3.4138e-04 - val_loss: 0.0011\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 242us/step - loss: 3.0399e-04 - val_loss: 0.0011\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 247us/step - loss: 2.8744e-04 - val_loss: 9.5226e-04\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 242us/step - loss: 2.8469e-04 - val_loss: 9.4188e-04\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 256us/step - loss: 2.8170e-04 - val_loss: 9.5200e-04\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.08585681]\n",
      "(86, 5, 1) (86, 1) [0.25316456]\n",
      "(172, 5, 1) (172, 1) [0.12562814]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 3s 5ms/step - loss: 0.0971 - val_loss: 0.0942\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 244us/step - loss: 0.0377 - val_loss: 0.0273\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 244us/step - loss: 0.0144 - val_loss: 0.0129\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 251us/step - loss: 0.0080 - val_loss: 0.0088\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 242us/step - loss: 0.0023 - val_loss: 0.0038\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 257us/step - loss: 7.2364e-04 - val_loss: 0.0035\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 243us/step - loss: 6.1024e-04 - val_loss: 0.0033\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 253us/step - loss: 5.7050e-04 - val_loss: 0.0033\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 240us/step - loss: 5.6042e-04 - val_loss: 0.0032\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 241us/step - loss: 5.3640e-04 - val_loss: 0.0033\n",
      "3577\n",
      "511\n",
      "1022\n",
      "(596, 5, 1) (596, 1) [0.00184111]\n",
      "(86, 5, 1) (86, 1) [0.0011659]\n",
      "(171, 5, 1) (171, 1) [0.00171649]\n",
      "Train on 596 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "596/596 [==============================] - 3s 6ms/step - loss: 0.1591 - val_loss: 0.2257\n",
      "Epoch 2/10\n",
      "596/596 [==============================] - 0s 257us/step - loss: 0.0722 - val_loss: 0.0560\n",
      "Epoch 3/10\n",
      "596/596 [==============================] - 0s 244us/step - loss: 0.0269 - val_loss: 0.0074\n",
      "Epoch 4/10\n",
      "596/596 [==============================] - 0s 241us/step - loss: 0.0115 - val_loss: 0.0057\n",
      "Epoch 5/10\n",
      "596/596 [==============================] - 0s 241us/step - loss: 0.0017 - val_loss: 0.0038\n",
      "Epoch 6/10\n",
      "596/596 [==============================] - 0s 226us/step - loss: 6.1910e-04 - val_loss: 0.0038\n",
      "Epoch 7/10\n",
      "596/596 [==============================] - 0s 233us/step - loss: 5.0735e-04 - val_loss: 0.0038\n",
      "Epoch 8/10\n",
      "596/596 [==============================] - 0s 243us/step - loss: 4.6117e-04 - val_loss: 0.0040\n",
      "Epoch 9/10\n",
      "596/596 [==============================] - 0s 238us/step - loss: 4.5146e-04 - val_loss: 0.0040\n",
      "Epoch 10/10\n",
      "596/596 [==============================] - 0s 243us/step - loss: 4.0180e-04 - val_loss: 0.0040\n",
      "1241\n",
      "177\n",
      "355\n",
      "(206, 5, 1) (206, 1) [0.03006262]\n",
      "(30, 5, 1) (30, 1) [0.08681008]\n",
      "(60, 5, 1) (60, 1) [0.13812155]\n",
      "Train on 206 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "206/206 [==============================] - 3s 17ms/step - loss: 0.3718 - val_loss: 0.5627\n",
      "Epoch 2/10\n",
      "206/206 [==============================] - 0s 295us/step - loss: 0.3019 - val_loss: 0.4660\n",
      "Epoch 3/10\n",
      "206/206 [==============================] - 0s 290us/step - loss: 0.2466 - val_loss: 0.3879\n",
      "Epoch 4/10\n",
      "206/206 [==============================] - 0s 260us/step - loss: 0.1987 - val_loss: 0.3142\n",
      "Epoch 5/10\n",
      "206/206 [==============================] - 0s 285us/step - loss: 0.1512 - val_loss: 0.2391\n",
      "Epoch 6/10\n",
      "206/206 [==============================] - 0s 294us/step - loss: 0.1048 - val_loss: 0.1650\n",
      "Epoch 7/10\n",
      "206/206 [==============================] - 0s 275us/step - loss: 0.0620 - val_loss: 0.0985\n",
      "Epoch 8/10\n",
      "206/206 [==============================] - 0s 281us/step - loss: 0.0297 - val_loss: 0.0466\n",
      "Epoch 9/10\n",
      "206/206 [==============================] - 0s 300us/step - loss: 0.0108 - val_loss: 0.0208\n",
      "Epoch 10/10\n",
      "206/206 [==============================] - 0s 262us/step - loss: 0.0070 - val_loss: 0.0166\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00193821]\n",
      "(86, 5, 1) (86, 1) [0.00165323]\n",
      "(172, 5, 1) (172, 1) [0.00167628]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 4s 6ms/step - loss: 0.0520 - val_loss: 0.4146\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 226us/step - loss: 0.0242 - val_loss: 0.2287\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 213us/step - loss: 0.0149 - val_loss: 0.1446\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 216us/step - loss: 0.0101 - val_loss: 0.0895\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 217us/step - loss: 0.0049 - val_loss: 0.0322\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 204us/step - loss: 8.0092e-04 - val_loss: 0.0079\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 203us/step - loss: 1.7938e-04 - val_loss: 0.0076\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 233us/step - loss: 1.2835e-04 - val_loss: 0.0076\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 217us/step - loss: 1.1465e-04 - val_loss: 0.0077\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 225us/step - loss: 1.1374e-04 - val_loss: 0.0078\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00919026]\n",
      "(86, 5, 1) (86, 1) [0.00837959]\n",
      "(172, 5, 1) (172, 1) [0.00892791]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 4s 7ms/step - loss: 0.0932 - val_loss: 0.2285\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 252us/step - loss: 0.0259 - val_loss: 0.0411\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 250us/step - loss: 0.0097 - val_loss: 0.0205\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 248us/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 247us/step - loss: 4.8639e-04 - val_loss: 0.0056\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 236us/step - loss: 3.9016e-04 - val_loss: 0.0056\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 266us/step - loss: 2.9124e-04 - val_loss: 0.0057\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 286us/step - loss: 2.1964e-04 - val_loss: 0.0059\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 256us/step - loss: 1.7589e-04 - val_loss: 0.0060\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 271us/step - loss: 1.6581e-04 - val_loss: 0.0061\n",
      "2970\n",
      "425\n",
      "849\n",
      "(495, 5, 1) (495, 1) [0.05072614]\n",
      "(71, 5, 1) (71, 1) [0.03452311]\n",
      "(142, 5, 1) (142, 1) [0.03397593]\n",
      "Train on 495 samples, validate on 71 samples\n",
      "Epoch 1/10\n",
      "495/495 [==============================] - 5s 10ms/step - loss: 0.1943 - val_loss: 0.3105\n",
      "Epoch 2/10\n",
      "495/495 [==============================] - 0s 371us/step - loss: 0.1360 - val_loss: 0.2117\n",
      "Epoch 3/10\n",
      "495/495 [==============================] - 0s 381us/step - loss: 0.0882 - val_loss: 0.1272\n",
      "Epoch 4/10\n",
      "495/495 [==============================] - 0s 361us/step - loss: 0.0492 - val_loss: 0.0563\n",
      "Epoch 5/10\n",
      "495/495 [==============================] - 0s 416us/step - loss: 0.0203 - val_loss: 0.0166\n",
      "Epoch 6/10\n",
      "495/495 [==============================] - 0s 314us/step - loss: 0.0047 - val_loss: 0.0044\n",
      "Epoch 7/10\n",
      "495/495 [==============================] - 0s 290us/step - loss: 6.8055e-04 - val_loss: 0.0040\n",
      "Epoch 8/10\n",
      "495/495 [==============================] - 0s 289us/step - loss: 4.6837e-04 - val_loss: 0.0036\n",
      "Epoch 9/10\n",
      "495/495 [==============================] - 0s 302us/step - loss: 3.9920e-04 - val_loss: 0.0036\n",
      "Epoch 10/10\n",
      "495/495 [==============================] - 0s 307us/step - loss: 3.8463e-04 - val_loss: 0.0036\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00324251]\n",
      "(86, 5, 1) (86, 1) [0.0012333]\n",
      "(172, 5, 1) (172, 1) [0.00084619]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 5s 8ms/step - loss: 0.0222 - val_loss: 0.1702\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 449us/step - loss: 0.0095 - val_loss: 0.0823\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 329us/step - loss: 0.0059 - val_loss: 0.0469\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 369us/step - loss: 0.0017 - val_loss: 0.0031\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 337us/step - loss: 1.8369e-04 - val_loss: 0.0038\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 300us/step - loss: 1.0052e-04 - val_loss: 0.0023\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 290us/step - loss: 8.9647e-05 - val_loss: 0.0026\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 295us/step - loss: 8.9408e-05 - val_loss: 0.0023\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 285us/step - loss: 8.8216e-05 - val_loss: 0.0024\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 291us/step - loss: 9.1278e-05 - val_loss: 0.0024\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.03111833]\n",
      "(86, 5, 1) (86, 1) [0.01249994]\n",
      "(172, 5, 1) (172, 1) [0.00970074]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 4s 7ms/step - loss: 0.1055 - val_loss: 0.1963\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 242us/step - loss: 0.0344 - val_loss: 0.0332\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 241us/step - loss: 0.0149 - val_loss: 0.0099\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - ETA: 0s - loss: 0.004 - 0s 233us/step - loss: 0.0040 - val_loss: 0.0017\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 252us/step - loss: 4.8893e-04 - val_loss: 0.0015\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 294us/step - loss: 3.4593e-04 - val_loss: 0.0014\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 276us/step - loss: 3.2440e-04 - val_loss: 0.0014\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 281us/step - loss: 2.9695e-04 - val_loss: 0.0014\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 440us/step - loss: 2.9677e-04 - val_loss: 0.0013\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 378us/step - loss: 2.9602e-04 - val_loss: 0.0014\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.02910479]\n",
      "(86, 5, 1) (86, 1) [0.01812908]\n",
      "(172, 5, 1) (172, 1) [0.02295184]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 5s 8ms/step - loss: 0.1301 - val_loss: 0.2901\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 265us/step - loss: 0.0474 - val_loss: 0.0597\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 253us/step - loss: 0.0159 - val_loss: 0.0123\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 252us/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 251us/step - loss: 3.1363e-04 - val_loss: 0.0045\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 288us/step - loss: 1.4587e-04 - val_loss: 0.0042\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 268us/step - loss: 1.4178e-04 - val_loss: 0.0042\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 266us/step - loss: 1.3016e-04 - val_loss: 0.0042\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 238us/step - loss: 1.1742e-04 - val_loss: 0.0042\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 250us/step - loss: 1.1480e-04 - val_loss: 0.0042\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.02678423]\n",
      "(86, 5, 1) (86, 1) [0.01292025]\n",
      "(172, 5, 1) (172, 1) [0.01080104]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 4s 7ms/step - loss: 0.0684 - val_loss: 0.1941\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 272us/step - loss: 0.0256 - val_loss: 0.0473\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 274us/step - loss: 0.0102 - val_loss: 0.0166\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 255us/step - loss: 0.0021 - val_loss: 9.3138e-04\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 254us/step - loss: 4.1295e-04 - val_loss: 9.9589e-04\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 260us/step - loss: 3.1529e-04 - val_loss: 0.0011\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 266us/step - loss: 3.0285e-04 - val_loss: 0.0011\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 260us/step - loss: 2.8696e-04 - val_loss: 9.6303e-04\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 255us/step - loss: 2.8015e-04 - val_loss: 9.4484e-04\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 264us/step - loss: 2.7730e-04 - val_loss: 9.5141e-04\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00194006]\n",
      "(86, 5, 1) (86, 1) [0.00135099]\n",
      "(172, 5, 1) (172, 1) [0.00149424]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 5s 8ms/step - loss: 0.0482 - val_loss: 0.2869\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 275us/step - loss: 0.0233 - val_loss: 0.1272\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 255us/step - loss: 0.0149 - val_loss: 0.0815\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 266us/step - loss: 0.0052 - val_loss: 0.0128\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 257us/step - loss: 2.3650e-04 - val_loss: 0.0073\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 262us/step - loss: 1.6223e-04 - val_loss: 0.0055\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 272us/step - loss: 7.8624e-05 - val_loss: 0.0055\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 277us/step - loss: 7.1534e-05 - val_loss: 0.0055\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 284us/step - loss: 6.4880e-05 - val_loss: 0.0054\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 272us/step - loss: 6.1033e-05 - val_loss: 0.0055\n",
      "2766\n",
      "395\n",
      "791\n",
      "(461, 5, 1) (461, 1) [0.01173252]\n",
      "(66, 5, 1) (66, 1) [0.01108735]\n",
      "(132, 5, 1) (132, 1) [0.0250075]\n",
      "Train on 461 samples, validate on 66 samples\n",
      "Epoch 1/10\n",
      "461/461 [==============================] - 5s 11ms/step - loss: 0.1592 - val_loss: 0.4620\n",
      "Epoch 2/10\n",
      "461/461 [==============================] - 0s 330us/step - loss: 0.1014 - val_loss: 0.2957\n",
      "Epoch 3/10\n",
      "461/461 [==============================] - 0s 299us/step - loss: 0.0557 - val_loss: 0.1390\n",
      "Epoch 4/10\n",
      "461/461 [==============================] - 0s 283us/step - loss: 0.0334 - val_loss: 0.0567\n",
      "Epoch 5/10\n",
      "461/461 [==============================] - 0s 317us/step - loss: 0.0274 - val_loss: 0.0492\n",
      "Epoch 6/10\n",
      "461/461 [==============================] - 0s 597us/step - loss: 0.0192 - val_loss: 0.0390\n",
      "Epoch 7/10\n",
      "461/461 [==============================] - 0s 389us/step - loss: 0.0102 - val_loss: 0.0157\n",
      "Epoch 8/10\n",
      "461/461 [==============================] - 0s 368us/step - loss: 0.0023 - val_loss: 0.0058\n",
      "Epoch 9/10\n",
      "461/461 [==============================] - 0s 409us/step - loss: 3.4175e-04 - val_loss: 0.0065\n",
      "Epoch 10/10\n",
      "461/461 [==============================] - 0s 376us/step - loss: 2.5075e-04 - val_loss: 0.0057\n",
      "3185\n",
      "455\n",
      "910\n",
      "(530, 5, 1) (530, 1) [0.02627769]\n",
      "(76, 5, 1) (76, 1) [0.01870606]\n",
      "(152, 5, 1) (152, 1) [0.01942879]\n",
      "Train on 530 samples, validate on 76 samples\n",
      "Epoch 1/10\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.0341 - val_loss: 0.1661\n",
      "Epoch 2/10\n",
      "530/530 [==============================] - 0s 272us/step - loss: 0.0136 - val_loss: 0.0766\n",
      "Epoch 3/10\n",
      "530/530 [==============================] - 0s 262us/step - loss: 0.0081 - val_loss: 0.0507\n",
      "Epoch 4/10\n",
      "530/530 [==============================] - 0s 322us/step - loss: 0.0045 - val_loss: 0.0311\n",
      "Epoch 5/10\n",
      "530/530 [==============================] - 0s 268us/step - loss: 0.0016 - val_loss: 0.0119\n",
      "Epoch 6/10\n",
      "530/530 [==============================] - 0s 269us/step - loss: 5.2706e-04 - val_loss: 0.0064\n",
      "Epoch 7/10\n",
      "530/530 [==============================] - 0s 275us/step - loss: 4.3438e-04 - val_loss: 0.0056\n",
      "Epoch 8/10\n",
      "530/530 [==============================] - 0s 266us/step - loss: 3.9375e-04 - val_loss: 0.0053\n",
      "Epoch 9/10\n",
      "530/530 [==============================] - 0s 278us/step - loss: 3.9538e-04 - val_loss: 0.0053\n",
      "Epoch 10/10\n",
      "530/530 [==============================] - 0s 268us/step - loss: 3.8443e-04 - val_loss: 0.0049\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00486701]\n",
      "(86, 5, 1) (86, 1) [0.01025663]\n",
      "(172, 5, 1) (172, 1) [0.01357386]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 6s 10ms/step - loss: 0.0321 - val_loss: 0.5728\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 259us/step - loss: 0.0204 - val_loss: 0.4320\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 244us/step - loss: 0.0152 - val_loss: 0.3290\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 256us/step - loss: 0.0116 - val_loss: 0.2480\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 249us/step - loss: 0.0067 - val_loss: 0.1255\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 244us/step - loss: 0.0017 - val_loss: 0.0196\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 256us/step - loss: 2.6395e-04 - val_loss: 0.0051\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 240us/step - loss: 1.8891e-04 - val_loss: 0.0059\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 243us/step - loss: 1.4180e-04 - val_loss: 0.0050\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 215us/step - loss: 1.2129e-04 - val_loss: 0.0047\n",
      "1674\n",
      "239\n",
      "479\n",
      "(279, 5, 1) (279, 1) [0.0056233]\n",
      "(40, 5, 1) (40, 1) [0.0121515]\n",
      "(80, 5, 1) (80, 1) [0.0191022]\n",
      "Train on 279 samples, validate on 40 samples\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - 5s 18ms/step - loss: 0.2854 - val_loss: 0.5898\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - 0s 277us/step - loss: 0.2272 - val_loss: 0.4845\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - 0s 267us/step - loss: 0.1764 - val_loss: 0.3811\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - 0s 273us/step - loss: 0.1260 - val_loss: 0.2751\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - 0s 295us/step - loss: 0.0773 - val_loss: 0.1713\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - 0s 305us/step - loss: 0.0396 - val_loss: 0.0812\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - 0s 355us/step - loss: 0.0168 - val_loss: 0.0285\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - 0s 358us/step - loss: 0.0084 - val_loss: 0.0159\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - 0s 344us/step - loss: 0.0042 - val_loss: 0.0140\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - 0s 327us/step - loss: 0.0014 - val_loss: 0.0135\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00191707]\n",
      "(86, 5, 1) (86, 1) [0.00080546]\n",
      "(172, 5, 1) (172, 1) [0.0009024]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 6s 9ms/step - loss: 0.0313 - val_loss: 0.1662\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 276us/step - loss: 0.0121 - val_loss: 0.0631\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 269us/step - loss: 0.0050 - val_loss: 0.0189\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 270us/step - loss: 7.3872e-04 - val_loss: 0.0033\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 276us/step - loss: 1.8446e-04 - val_loss: 0.0024\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 272us/step - loss: 7.9940e-05 - val_loss: 0.0024\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 270us/step - loss: 6.1253e-05 - val_loss: 0.0023\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 261us/step - loss: 6.0076e-05 - val_loss: 0.0023\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 267us/step - loss: 5.9616e-05 - val_loss: 0.0023\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 268us/step - loss: 5.3739e-05 - val_loss: 0.0023\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00368793]\n",
      "(86, 5, 1) (86, 1) [0.00182732]\n",
      "(172, 5, 1) (172, 1) [0.00137588]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 6s 9ms/step - loss: 0.1284 - val_loss: 0.0475\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 289us/step - loss: 0.0213 - val_loss: 0.0086\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 280us/step - loss: 0.0098 - val_loss: 0.0046\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 275us/step - loss: 0.0044 - val_loss: 0.0016\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 275us/step - loss: 7.1097e-04 - val_loss: 0.0011\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 277us/step - loss: 3.4568e-04 - val_loss: 0.0010\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 282us/step - loss: 2.9428e-04 - val_loss: 9.9255e-04\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 309us/step - loss: 2.9191e-04 - val_loss: 0.0010\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 285us/step - loss: 2.8306e-04 - val_loss: 0.0010\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 273us/step - loss: 2.9224e-04 - val_loss: 0.0011\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00125014]\n",
      "(86, 5, 1) (86, 1) [0.00102237]\n",
      "(172, 5, 1) (172, 1) [0.00308468]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 6s 10ms/step - loss: 0.0224 - val_loss: 0.1533\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 288us/step - loss: 0.0090 - val_loss: 0.0446\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 270us/step - loss: 0.0014 - val_loss: 0.0072\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 272us/step - loss: 2.1259e-04 - val_loss: 0.0098\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 254us/step - loss: 1.5034e-04 - val_loss: 0.0072\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 272us/step - loss: 1.3732e-04 - val_loss: 0.0073\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 275us/step - loss: 1.2658e-04 - val_loss: 0.0073\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 277us/step - loss: 1.2926e-04 - val_loss: 0.0073\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 274us/step - loss: 1.2977e-04 - val_loss: 0.0073\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 288us/step - loss: 1.2438e-04 - val_loss: 0.0074\n",
      "1956\n",
      "280\n",
      "559\n",
      "(326, 5, 1) (326, 1) [0.03100343]\n",
      "(47, 5, 1) (47, 1) [0.04195106]\n",
      "(94, 5, 1) (94, 1) [0.06543304]\n",
      "Train on 326 samples, validate on 47 samples\n",
      "Epoch 1/10\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2214 - val_loss: 0.4989\n",
      "Epoch 2/10\n",
      "326/326 [==============================] - 0s 319us/step - loss: 0.1711 - val_loss: 0.3869\n",
      "Epoch 3/10\n",
      "326/326 [==============================] - 0s 322us/step - loss: 0.1211 - val_loss: 0.2722\n",
      "Epoch 4/10\n",
      "326/326 [==============================] - 0s 312us/step - loss: 0.0768 - val_loss: 0.1636\n",
      "Epoch 5/10\n",
      "326/326 [==============================] - 0s 315us/step - loss: 0.0392 - val_loss: 0.0671\n",
      "Epoch 6/10\n",
      "326/326 [==============================] - 0s 315us/step - loss: 0.0167 - val_loss: 0.0175\n",
      "Epoch 7/10\n",
      "326/326 [==============================] - 0s 312us/step - loss: 0.0093 - val_loss: 0.0136\n",
      "Epoch 8/10\n",
      "326/326 [==============================] - 0s 315us/step - loss: 0.0043 - val_loss: 0.0116\n",
      "Epoch 9/10\n",
      "326/326 [==============================] - 0s 319us/step - loss: 0.0011 - val_loss: 0.0117\n",
      "Epoch 10/10\n",
      "326/326 [==============================] - 0s 303us/step - loss: 3.5989e-04 - val_loss: 0.0130\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00637945]\n",
      "(86, 5, 1) (86, 1) [0.00191124]\n",
      "(172, 5, 1) (172, 1) [0.0014577]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 6s 11ms/step - loss: 0.0272 - val_loss: 0.1745\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 253us/step - loss: 0.0140 - val_loss: 0.1072\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 247us/step - loss: 0.0091 - val_loss: 0.0715\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 248us/step - loss: 0.0061 - val_loss: 0.0403\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 256us/step - loss: 0.0028 - val_loss: 0.0161\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 251us/step - loss: 9.3261e-04 - val_loss: 0.0067\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 256us/step - loss: 4.5778e-04 - val_loss: 0.0035\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 241us/step - loss: 2.2933e-04 - val_loss: 0.0025\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 276us/step - loss: 1.6919e-04 - val_loss: 0.0019\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 285us/step - loss: 1.2571e-04 - val_loss: 0.0015\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00496855]\n",
      "(86, 5, 1) (86, 1) [0.0045507]\n",
      "(172, 5, 1) (172, 1) [0.00673788]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 6s 11ms/step - loss: 0.0333 - val_loss: 0.2604\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 292us/step - loss: 0.0159 - val_loss: 0.1013\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 282us/step - loss: 0.0038 - val_loss: 0.0131\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 276us/step - loss: 4.2126e-04 - val_loss: 0.0094\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 276us/step - loss: 1.7931e-04 - val_loss: 0.0082\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 283us/step - loss: 1.2396e-04 - val_loss: 0.0084\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 288us/step - loss: 1.0740e-04 - val_loss: 0.0084\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 288us/step - loss: 1.1783e-04 - val_loss: 0.0082\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 275us/step - loss: 1.1909e-04 - val_loss: 0.0086\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 275us/step - loss: 1.1545e-04 - val_loss: 0.0082\n",
      "1170\n",
      "167\n",
      "335\n",
      "(195, 5, 1) (195, 1) [0.01243082]\n",
      "(28, 5, 1) (28, 1) [0.01534623]\n",
      "(56, 5, 1) (56, 1) [0.01486253]\n",
      "Train on 195 samples, validate on 28 samples\n",
      "Epoch 1/10\n",
      "195/195 [==============================] - 6s 33ms/step - loss: 0.2916 - val_loss: 0.2663\n",
      "Epoch 2/10\n",
      "195/195 [==============================] - 0s 351us/step - loss: 0.2359 - val_loss: 0.2131\n",
      "Epoch 3/10\n",
      "195/195 [==============================] - 0s 344us/step - loss: 0.1863 - val_loss: 0.1652\n",
      "Epoch 4/10\n",
      "195/195 [==============================] - 0s 339us/step - loss: 0.1422 - val_loss: 0.1216\n",
      "Epoch 5/10\n",
      "195/195 [==============================] - 0s 322us/step - loss: 0.0992 - val_loss: 0.0830\n",
      "Epoch 6/10\n",
      "195/195 [==============================] - 0s 412us/step - loss: 0.0641 - val_loss: 0.0537\n",
      "Epoch 7/10\n",
      "195/195 [==============================] - 0s 327us/step - loss: 0.0372 - val_loss: 0.0354\n",
      "Epoch 8/10\n",
      "195/195 [==============================] - 0s 339us/step - loss: 0.0205 - val_loss: 0.0275\n",
      "Epoch 9/10\n",
      "195/195 [==============================] - 0s 338us/step - loss: 0.0125 - val_loss: 0.0269\n",
      "Epoch 10/10\n",
      "195/195 [==============================] - 0s 367us/step - loss: 0.0092 - val_loss: 0.0276\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.021381]\n",
      "(86, 5, 1) (86, 1) [0.00986506]\n",
      "(172, 5, 1) (172, 1) [0.00800434]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 7s 11ms/step - loss: 0.0754 - val_loss: 0.3238\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 282us/step - loss: 0.0330 - val_loss: 0.1329\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 304us/step - loss: 0.0111 - val_loss: 0.0291\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 324us/step - loss: 0.0011 - val_loss: 0.0039\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 333us/step - loss: 3.9629e-04 - val_loss: 0.0029\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 501us/step - loss: 2.5778e-04 - val_loss: 0.0028\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 447us/step - loss: 2.1256e-04 - val_loss: 0.0027\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 375us/step - loss: 2.1049e-04 - val_loss: 0.0026\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 350us/step - loss: 2.2002e-04 - val_loss: 0.0026\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 314us/step - loss: 2.0452e-04 - val_loss: 0.0026\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.04470153]\n",
      "(86, 5, 1) (86, 1) [0.04195476]\n",
      "(172, 5, 1) (172, 1) [0.04710049]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 7s 11ms/step - loss: 0.0688 - val_loss: 0.1644\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 296us/step - loss: 0.0116 - val_loss: 0.0153\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 307us/step - loss: 0.0067 - val_loss: 0.0295\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 313us/step - loss: 0.0025 - val_loss: 0.0096\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 304us/step - loss: 4.0475e-04 - val_loss: 0.0106\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 305us/step - loss: 2.1522e-04 - val_loss: 0.0104\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 316us/step - loss: 1.8063e-04 - val_loss: 0.0098\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 314us/step - loss: 1.6519e-04 - val_loss: 0.0099\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 326us/step - loss: 1.5797e-04 - val_loss: 0.0098\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 297us/step - loss: 1.6385e-04 - val_loss: 0.0099-0\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00075317]\n",
      "(86, 5, 1) (86, 1) [0.00079349]\n",
      "(172, 5, 1) (172, 1) [0.00048236]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 7s 11ms/step - loss: 0.0271 - val_loss: 0.3764\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 282us/step - loss: 0.0211 - val_loss: 0.2841\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 302us/step - loss: 0.0162 - val_loss: 0.2043\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 305us/step - loss: 0.0085 - val_loss: 0.0578\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 275us/step - loss: 8.9125e-04 - val_loss: 0.0081\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 351us/step - loss: 2.4008e-04 - val_loss: 0.0065\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 312us/step - loss: 1.1570e-04 - val_loss: 0.0066\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 451us/step - loss: 9.6657e-05 - val_loss: 0.0065\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 482us/step - loss: 9.6472e-05 - val_loss: 0.0065\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 448us/step - loss: 9.3310e-05 - val_loss: 0.0066\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00326878]\n",
      "(86, 5, 1) (86, 1) [0.00229178]\n",
      "(172, 5, 1) (172, 1) [0.00259787]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 8s 13ms/step - loss: 0.0261 - val_loss: 0.2726\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 319us/step - loss: 0.0122 - val_loss: 0.1479\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 304us/step - loss: 0.0056 - val_loss: 0.0517\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 305us/step - loss: 7.3711e-04 - val_loss: 0.0073\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 329us/step - loss: 2.4605e-04 - val_loss: 0.0072\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 304us/step - loss: 1.4096e-04 - val_loss: 0.0066\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 325us/step - loss: 1.1706e-04 - val_loss: 0.0062\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 310us/step - loss: 1.0621e-04 - val_loss: 0.0058\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 308us/step - loss: 9.1428e-05 - val_loss: 0.0058\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 308us/step - loss: 9.1846e-05 - val_loss: 0.0057\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.02231067]\n",
      "(86, 5, 1) (86, 1) [0.00893451]\n",
      "(172, 5, 1) (172, 1) [0.00670406]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 8s 13ms/step - loss: 0.0841 - val_loss: 0.1996\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 333us/step - loss: 0.0371 - val_loss: 0.0565\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 296us/step - loss: 0.0159 - val_loss: 0.0135\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 349us/step - loss: 0.0067 - val_loss: 0.0043\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 342us/step - loss: 7.9956e-04 - val_loss: 0.0020\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 330us/step - loss: 3.5016e-04 - val_loss: 0.0019\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 322us/step - loss: 2.6793e-04 - val_loss: 0.0018\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 304us/step - loss: 2.6693e-04 - val_loss: 0.0019\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 324us/step - loss: 2.6851e-04 - val_loss: 0.0019\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 305us/step - loss: 2.5513e-04 - val_loss: 0.0019\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.01177881]\n",
      "(86, 5, 1) (86, 1) [0.00537157]\n",
      "(172, 5, 1) (172, 1) [0.0059057]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 8s 14ms/step - loss: 0.0559 - val_loss: 0.3129\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 308us/step - loss: 0.0278 - val_loss: 0.1195\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 345us/step - loss: 0.0149 - val_loss: 0.0585\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 289us/step - loss: 0.0032 - val_loss: 0.0026\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 333us/step - loss: 3.8124e-04 - val_loss: 0.0041\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 327us/step - loss: 1.9159e-04 - val_loss: 0.0029\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 329us/step - loss: 1.5135e-04 - val_loss: 0.0027\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 297us/step - loss: 1.5015e-04 - val_loss: 0.0027\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 331us/step - loss: 1.4146e-04 - val_loss: 0.0027\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 334us/step - loss: 1.4288e-04 - val_loss: 0.0027\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.01502822]\n",
      "(86, 5, 1) (86, 1) [0.02603014]\n",
      "(172, 5, 1) (172, 1) [0.02305938]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 9s 14ms/step - loss: 0.0687 - val_loss: 0.4002\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 269us/step - loss: 0.0324 - val_loss: 0.1737\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 252us/step - loss: 0.0144 - val_loss: 0.0655\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 282us/step - loss: 0.0045 - val_loss: 0.0136\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 288us/step - loss: 4.9911e-04 - val_loss: 0.0064\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 270us/step - loss: 3.6144e-04 - val_loss: 0.0059\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 267us/step - loss: 3.0173e-04 - val_loss: 0.0060\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 273us/step - loss: 2.9833e-04 - val_loss: 0.0058\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 273us/step - loss: 3.3110e-04 - val_loss: 0.0058\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 285us/step - loss: 3.1408e-04 - val_loss: 0.0059\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00296592]\n",
      "(86, 5, 1) (86, 1) [0.00209217]\n",
      "(172, 5, 1) (172, 1) [0.00140289]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 8s 14ms/step - loss: 0.0736 - val_loss: 0.2361\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 273us/step - loss: 0.0317 - val_loss: 0.0653\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 288us/step - loss: 0.0081 - val_loss: 0.0099\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 282us/step - loss: 4.2286e-04 - val_loss: 0.0063\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 268us/step - loss: 1.5538e-04 - val_loss: 0.0054\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 352us/step - loss: 1.0147e-04 - val_loss: 0.0055\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 271us/step - loss: 8.4375e-05 - val_loss: 0.0054\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 270us/step - loss: 7.7038e-05 - val_loss: 0.0055\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 286us/step - loss: 7.1778e-05 - val_loss: 0.0054\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 259us/step - loss: 7.1818e-05 - val_loss: 0.0054\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00592033]\n",
      "(86, 5, 1) (86, 1) [0.00510951]\n",
      "(172, 5, 1) (172, 1) [0.00647609]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 8s 14ms/step - loss: 0.0624 - val_loss: 0.5758\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 288us/step - loss: 0.0378 - val_loss: 0.4209\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 310us/step - loss: 0.0228 - val_loss: 0.3048\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 304us/step - loss: 0.0130 - val_loss: 0.1953\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 307us/step - loss: 0.0058 - val_loss: 0.1060\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - ETA: 0s - loss: 0.002 - 0s 294us/step - loss: 0.0022 - val_loss: 0.0435\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 277us/step - loss: 5.5261e-04 - val_loss: 0.0166\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 293us/step - loss: 1.9007e-04 - val_loss: 0.0103\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 290us/step - loss: 1.2075e-04 - val_loss: 0.0093\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 281us/step - loss: 1.0111e-04 - val_loss: 0.0091\n",
      "3597\n",
      "515\n",
      "1028\n",
      "(599, 5, 1) (599, 1) [0.00421284]\n",
      "(86, 5, 1) (86, 1) [0.00249577]\n",
      "(172, 5, 1) (172, 1) [0.00242112]\n",
      "Train on 599 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "599/599 [==============================] - 8s 14ms/step - loss: 0.0531 - val_loss: 0.2771\n",
      "Epoch 2/10\n",
      "599/599 [==============================] - 0s 270us/step - loss: 0.0264 - val_loss: 0.1497\n",
      "Epoch 3/10\n",
      "599/599 [==============================] - 0s 308us/step - loss: 0.0121 - val_loss: 0.0602\n",
      "Epoch 4/10\n",
      "599/599 [==============================] - 0s 361us/step - loss: 0.0029 - val_loss: 0.0099\n",
      "Epoch 5/10\n",
      "599/599 [==============================] - 0s 309us/step - loss: 6.6066e-04 - val_loss: 0.0051\n",
      "Epoch 6/10\n",
      "599/599 [==============================] - 0s 538us/step - loss: 1.6286e-04 - val_loss: 0.0042\n",
      "Epoch 7/10\n",
      "599/599 [==============================] - 0s 383us/step - loss: 1.1870e-04 - val_loss: 0.0043\n",
      "Epoch 8/10\n",
      "599/599 [==============================] - 0s 370us/step - loss: 1.0608e-04 - val_loss: 0.0042\n",
      "Epoch 9/10\n",
      "599/599 [==============================] - 0s 286us/step - loss: 1.1951e-04 - val_loss: 0.0043\n",
      "Epoch 10/10\n",
      "599/599 [==============================] - 0s 278us/step - loss: 9.6794e-05 - val_loss: 0.0042\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00507222]\n",
      "(86, 5, 1) (86, 1) [0.00378571]\n",
      "(172, 5, 1) (172, 1) [0.00290586]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 9s 15ms/step - loss: 0.0967 - val_loss: 0.1645\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 299us/step - loss: 0.0354 - val_loss: 0.0424\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 285us/step - loss: 0.0122 - val_loss: 0.0095\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 301us/step - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 301us/step - loss: 4.7637e-04 - val_loss: 0.0041\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 303us/step - loss: 3.6251e-04 - val_loss: 0.0039\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 302us/step - loss: 3.2037e-04 - val_loss: 0.0039\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 303us/step - loss: 3.1247e-04 - val_loss: 0.0038\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 299us/step - loss: 3.1531e-04 - val_loss: 0.0039\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 286us/step - loss: 3.1941e-04 - val_loss: 0.0039\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00294983]\n",
      "(86, 5, 1) (86, 1) [0.00182938]\n",
      "(172, 5, 1) (172, 1) [0.00134246]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 9s 15ms/step - loss: 0.1155 - val_loss: 0.3853\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 302us/step - loss: 0.0681 - val_loss: 0.2244\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 318us/step - loss: 0.0327 - val_loss: 0.0766\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 306us/step - loss: 0.0081 - val_loss: 0.0092\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 308us/step - loss: 5.9094e-04 - val_loss: 0.0042\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 318us/step - loss: 1.5934e-04 - val_loss: 0.0039\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 302us/step - loss: 1.4426e-04 - val_loss: 0.0039\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 319us/step - loss: 1.3635e-04 - val_loss: 0.0040\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 319us/step - loss: 1.4550e-04 - val_loss: 0.0039\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 336us/step - loss: 1.3316e-04 - val_loss: 0.0039\n",
      "2848\n",
      "407\n",
      "814\n",
      "(474, 5, 1) (474, 1) [0.00287646]\n",
      "(68, 5, 1) (68, 1) [0.00198451]\n",
      "(136, 5, 1) (136, 1) [0.00378395]\n",
      "Train on 474 samples, validate on 68 samples\n",
      "Epoch 1/10\n",
      "474/474 [==============================] - 9s 19ms/step - loss: 0.2283 - val_loss: 0.4418\n",
      "Epoch 2/10\n",
      "474/474 [==============================] - 0s 379us/step - loss: 0.1509 - val_loss: 0.2796\n",
      "Epoch 3/10\n",
      "474/474 [==============================] - 0s 340us/step - loss: 0.0816 - val_loss: 0.1192\n",
      "Epoch 4/10\n",
      "474/474 [==============================] - 0s 319us/step - loss: 0.0321 - val_loss: 0.0179\n",
      "Epoch 5/10\n",
      "474/474 [==============================] - 0s 303us/step - loss: 0.0180 - val_loss: 0.0136\n",
      "Epoch 6/10\n",
      "474/474 [==============================] - 0s 302us/step - loss: 0.0093 - val_loss: 0.0059\n",
      "Epoch 7/10\n",
      "474/474 [==============================] - ETA: 0s - loss: 0.002 - 0s 314us/step - loss: 0.0025 - val_loss: 0.0034\n",
      "Epoch 8/10\n",
      "474/474 [==============================] - 0s 319us/step - loss: 3.2513e-04 - val_loss: 0.0039\n",
      "Epoch 9/10\n",
      "474/474 [==============================] - 0s 319us/step - loss: 2.6146e-04 - val_loss: 0.0036\n",
      "Epoch 10/10\n",
      "474/474 [==============================] - 0s 320us/step - loss: 1.6194e-04 - val_loss: 0.0035\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00266036]\n",
      "(86, 5, 1) (86, 1) [0.00084846]\n",
      "(172, 5, 1) (172, 1) [0.0014908]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 10s 16ms/step - loss: 0.0221 - val_loss: 0.1646\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 318us/step - loss: 0.0068 - val_loss: 0.0631\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 311us/step - loss: 0.0022 - val_loss: 0.0120\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 386us/step - loss: 2.8000e-04 - val_loss: 0.0029\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 333us/step - loss: 9.6060e-05 - val_loss: 0.0016\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 333us/step - loss: 6.2130e-05 - val_loss: 0.0016\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 325us/step - loss: 5.7248e-05 - val_loss: 0.0015\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 279us/step - loss: 5.4733e-05 - val_loss: 0.0016\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 334us/step - loss: 5.8936e-05 - val_loss: 0.0016\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 312us/step - loss: 5.0598e-05 - val_loss: 0.0015\n",
      "2027\n",
      "289\n",
      "580\n",
      "(337, 5, 1) (337, 1) [0.02218008]\n",
      "(49, 5, 1) (49, 1) [0.02972289]\n",
      "(97, 5, 1) (97, 1) [0.03842223]\n",
      "Train on 337 samples, validate on 49 samples\n",
      "Epoch 1/10\n",
      "337/337 [==============================] - 9s 28ms/step - loss: 0.2777 - val_loss: 0.4858\n",
      "Epoch 2/10\n",
      "337/337 [==============================] - 0s 626us/step - loss: 0.1745 - val_loss: 0.3060\n",
      "Epoch 3/10\n",
      "337/337 [==============================] - 0s 478us/step - loss: 0.1053 - val_loss: 0.1558\n",
      "Epoch 4/10\n",
      "337/337 [==============================] - 0s 418us/step - loss: 0.0499 - val_loss: 0.0568\n",
      "Epoch 5/10\n",
      "337/337 [==============================] - 0s 418us/step - loss: 0.0234 - val_loss: 0.0187\n",
      "Epoch 6/10\n",
      "337/337 [==============================] - 0s 449us/step - loss: 0.0155 - val_loss: 0.0144\n",
      "Epoch 7/10\n",
      "337/337 [==============================] - 0s 323us/step - loss: 0.0075 - val_loss: 0.0174\n",
      "Epoch 8/10\n",
      "337/337 [==============================] - 0s 292us/step - loss: 0.0026 - val_loss: 0.0171\n",
      "Epoch 9/10\n",
      "337/337 [==============================] - 0s 298us/step - loss: 5.0403e-04 - val_loss: 0.0189\n",
      "Epoch 10/10\n",
      "337/337 [==============================] - 0s 360us/step - loss: 3.4595e-04 - val_loss: 0.0193\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.03579124]\n",
      "(86, 5, 1) (86, 1) [0.08818342]\n",
      "(172, 5, 1) (172, 1) [0.10926335]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 10s 17ms/step - loss: 0.1235 - val_loss: 0.3041\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 288us/step - loss: 0.0622 - val_loss: 0.1497\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 299us/step - loss: 0.0228 - val_loss: 0.0438\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 285us/step - loss: 0.0078 - val_loss: 0.0147\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 285us/step - loss: 0.0014 - val_loss: 0.0072\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 285us/step - loss: 3.6191e-04 - val_loss: 0.0067\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 304us/step - loss: 2.8101e-04 - val_loss: 0.0064\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 285us/step - loss: 2.5825e-04 - val_loss: 0.0063\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 319us/step - loss: 2.5589e-04 - val_loss: 0.0062\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 336us/step - loss: 2.4724e-04 - val_loss: 0.0062\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.01929139]\n",
      "(86, 5, 1) (86, 1) [0.02001041]\n",
      "(172, 5, 1) (172, 1) [0.00725133]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 10s 16ms/step - loss: 0.0957 - val_loss: 0.0807\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 283us/step - loss: 0.0274 - val_loss: 0.0109\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 309us/step - loss: 0.0183 - val_loss: 0.0123\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 321us/step - loss: 0.0130 - val_loss: 0.0079\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 318us/step - loss: 0.0074 - val_loss: 0.0037\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 337us/step - loss: 0.0031 - val_loss: 0.0022\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 320us/step - loss: 8.4161e-04 - val_loss: 0.0013\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 319us/step - loss: 4.1315e-04 - val_loss: 0.0014\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 333us/step - loss: 3.5044e-04 - val_loss: 0.0014\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 301us/step - loss: 2.9199e-04 - val_loss: 0.0014\n",
      "1709\n",
      "244\n",
      "489\n",
      "(284, 5, 1) (284, 1) [0.00713971]\n",
      "(41, 5, 1) (41, 1) [0.00964835]\n",
      "(82, 5, 1) (82, 1) [0.01734861]\n",
      "Train on 284 samples, validate on 41 samples\n",
      "Epoch 1/10\n",
      "284/284 [==============================] - 10s 35ms/step - loss: 0.0991 - val_loss: 0.6544\n",
      "Epoch 2/10\n",
      "284/284 [==============================] - 0s 291us/step - loss: 0.0803 - val_loss: 0.5584\n",
      "Epoch 3/10\n",
      "284/284 [==============================] - 0s 282us/step - loss: 0.0671 - val_loss: 0.4815\n",
      "Epoch 4/10\n",
      "284/284 [==============================] - 0s 314us/step - loss: 0.0573 - val_loss: 0.4154\n",
      "Epoch 5/10\n",
      "284/284 [==============================] - 0s 285us/step - loss: 0.0496 - val_loss: 0.3509\n",
      "Epoch 6/10\n",
      "284/284 [==============================] - 0s 254us/step - loss: 0.0413 - val_loss: 0.2928\n",
      "Epoch 7/10\n",
      "284/284 [==============================] - 0s 298us/step - loss: 0.0343 - val_loss: 0.2329\n",
      "Epoch 8/10\n",
      "284/284 [==============================] - 0s 297us/step - loss: 0.0263 - val_loss: 0.1737\n",
      "Epoch 9/10\n",
      "284/284 [==============================] - 0s 288us/step - loss: 0.0173 - val_loss: 0.1197\n",
      "Epoch 10/10\n",
      "284/284 [==============================] - 0s 305us/step - loss: 0.0092 - val_loss: 0.0663\n",
      "2033\n",
      "291\n",
      "581\n",
      "(338, 5, 1) (338, 1) [0.02078268]\n",
      "(49, 5, 1) (49, 1) [0.0298511]\n",
      "(97, 5, 1) (97, 1) [0.04796439]\n",
      "Train on 338 samples, validate on 49 samples\n",
      "Epoch 1/10\n",
      "338/338 [==============================] - 12s 36ms/step - loss: 0.1563 - val_loss: 0.4218\n",
      "Epoch 2/10\n",
      "338/338 [==============================] - 0s 505us/step - loss: 0.1007 - val_loss: 0.2737\n",
      "Epoch 3/10\n",
      "338/338 [==============================] - 0s 519us/step - loss: 0.0607 - val_loss: 0.1540\n",
      "Epoch 4/10\n",
      "338/338 [==============================] - 0s 549us/step - loss: 0.0338 - val_loss: 0.0681\n",
      "Epoch 5/10\n",
      "338/338 [==============================] - 0s 552us/step - loss: 0.0195 - val_loss: 0.0294\n",
      "Epoch 6/10\n",
      "338/338 [==============================] - 0s 522us/step - loss: 0.0116 - val_loss: 0.0216\n",
      "Epoch 7/10\n",
      "338/338 [==============================] - 0s 466us/step - loss: 0.0052 - val_loss: 0.0177\n",
      "Epoch 8/10\n",
      "338/338 [==============================] - 0s 375us/step - loss: 0.0014 - val_loss: 0.0161\n",
      "Epoch 9/10\n",
      "338/338 [==============================] - 0s 428us/step - loss: 2.6455e-04 - val_loss: 0.0170\n",
      "Epoch 10/10\n",
      "338/338 [==============================] - 0s 366us/step - loss: 2.6193e-04 - val_loss: 0.0169\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.02892615]\n",
      "(86, 5, 1) (86, 1) [0.01945045]\n",
      "(172, 5, 1) (172, 1) [0.02598077]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 11s 18ms/step - loss: 0.0676 - val_loss: 0.1287\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 350us/step - loss: 0.0172 - val_loss: 0.0172\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 352us/step - loss: 0.0040 - val_loss: 0.0050\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 339us/step - loss: 5.8532e-04 - val_loss: 0.0033\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 378us/step - loss: 2.4410e-04 - val_loss: 0.0030\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 381us/step - loss: 2.0209e-04 - val_loss: 0.0030\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 419us/step - loss: 2.0778e-04 - val_loss: 0.0030\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 631us/step - loss: 2.0274e-04 - val_loss: 0.0030\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 568us/step - loss: 1.9584e-04 - val_loss: 0.0030\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 604us/step - loss: 1.9683e-04 - val_loss: 0.0030\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00515757]\n",
      "(86, 5, 1) (86, 1) [0.0047521]\n",
      "(172, 5, 1) (172, 1) [0.00481339]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 11s 19ms/step - loss: 0.0493 - val_loss: 0.4095\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 374us/step - loss: 0.0240 - val_loss: 0.2093\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 387us/step - loss: 0.0090 - val_loss: 0.0680\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 415us/step - loss: 0.0016 - val_loss: 0.0117\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 569us/step - loss: 1.9706e-04 - val_loss: 0.0080\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 587us/step - loss: 1.3189e-04 - val_loss: 0.0078\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 500us/step - loss: 1.2169e-04 - val_loss: 0.0078\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 456us/step - loss: 1.2321e-04 - val_loss: 0.0079\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 380us/step - loss: 1.2754e-04 - val_loss: 0.0080\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 358us/step - loss: 1.2285e-04 - val_loss: 0.0077\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00477959]\n",
      "(86, 5, 1) (86, 1) [0.00270872]\n",
      "(172, 5, 1) (172, 1) [0.00196165]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 11s 19ms/step - loss: 0.0432 - val_loss: 0.5391\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 463us/step - loss: 0.0270 - val_loss: 0.3520\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 404us/step - loss: 0.0196 - val_loss: 0.2216\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 493us/step - loss: 0.0122 - val_loss: 0.1108\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 416us/step - loss: 0.0039 - val_loss: 0.0079\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 417us/step - loss: 2.2168e-04 - val_loss: 0.0069\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 399us/step - loss: 1.2079e-04 - val_loss: 0.0035\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 442us/step - loss: 7.2729e-05 - val_loss: 0.0038\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 436us/step - loss: 7.4367e-05 - val_loss: 0.0036\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 438us/step - loss: 7.1752e-05 - val_loss: 0.0036\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00201822]\n",
      "(86, 5, 1) (86, 1) [0.00161899]\n",
      "(172, 5, 1) (172, 1) [0.00132406]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 12s 20ms/step - loss: 0.0348 - val_loss: 0.2072\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 438us/step - loss: 0.0163 - val_loss: 0.0972\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 451us/step - loss: 0.0078 - val_loss: 0.0398\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 401us/step - loss: 0.0021 - val_loss: 0.0073\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 418us/step - loss: 2.2242e-04 - val_loss: 0.0074\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 450us/step - loss: 9.2929e-05 - val_loss: 0.0063\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 453us/step - loss: 7.7862e-05 - val_loss: 0.0065\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 464us/step - loss: 8.0359e-05 - val_loss: 0.0065\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 426us/step - loss: 7.7016e-05 - val_loss: 0.0064\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 450us/step - loss: 7.5458e-05 - val_loss: 0.0063\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00372551]\n",
      "(86, 5, 1) (86, 1) [0.00238379]\n",
      "(172, 5, 1) (172, 1) [0.00203477]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 12s 20ms/step - loss: 0.0674 - val_loss: 0.0797\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 445us/step - loss: 0.0176 - val_loss: 0.0125\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 751us/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 584us/step - loss: 6.9987e-04 - val_loss: 0.0027\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 404us/step - loss: 2.4820e-04 - val_loss: 0.0025\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 381us/step - loss: 1.9920e-04 - val_loss: 0.0023\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 427us/step - loss: 1.9196e-04 - val_loss: 0.0023\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 436us/step - loss: 1.7547e-04 - val_loss: 0.0023\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 381us/step - loss: 1.8747e-04 - val_loss: 0.0023\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 381us/step - loss: 1.8858e-04 - val_loss: 0.0023\n",
      "3576\n",
      "511\n",
      "1022\n",
      "(596, 5, 1) (596, 1) [0.0007879]\n",
      "(86, 5, 1) (86, 1) [0.00064099]\n",
      "(171, 5, 1) (171, 1) [0.00046001]\n",
      "Train on 596 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "596/596 [==============================] - 12s 20ms/step - loss: 0.0888 - val_loss: 0.2508\n",
      "Epoch 2/10\n",
      "596/596 [==============================] - 0s 444us/step - loss: 0.0498 - val_loss: 0.0798\n",
      "Epoch 3/10\n",
      "596/596 [==============================] - 0s 448us/step - loss: 0.0258 - val_loss: 0.0247\n",
      "Epoch 4/10\n",
      "596/596 [==============================] - 0s 450us/step - loss: 0.0068 - val_loss: 0.0083\n",
      "Epoch 5/10\n",
      "596/596 [==============================] - 0s 457us/step - loss: 4.0308e-04 - val_loss: 0.0082\n",
      "Epoch 6/10\n",
      "596/596 [==============================] - 0s 445us/step - loss: 2.5862e-04 - val_loss: 0.0074\n",
      "Epoch 7/10\n",
      "596/596 [==============================] - 0s 428us/step - loss: 1.7392e-04 - val_loss: 0.0075\n",
      "Epoch 8/10\n",
      "596/596 [==============================] - 0s 450us/step - loss: 1.5682e-04 - val_loss: 0.0076\n",
      "Epoch 9/10\n",
      "596/596 [==============================] - 0s 466us/step - loss: 1.5002e-04 - val_loss: 0.0076\n",
      "Epoch 10/10\n",
      "596/596 [==============================] - 0s 429us/step - loss: 1.4488e-04 - val_loss: 0.0076\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.0153478]\n",
      "(86, 5, 1) (86, 1) [0.00370882]\n",
      "(172, 5, 1) (172, 1) [0.00467239]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 12s 20ms/step - loss: 0.0482 - val_loss: 0.1869\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 388us/step - loss: 0.0192 - val_loss: 0.0676\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 430us/step - loss: 0.0134 - val_loss: 0.0540\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 438us/step - loss: 0.0069 - val_loss: 0.0196\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 401us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 411us/step - loss: 2.2063e-04 - val_loss: 9.3803e-04\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 407us/step - loss: 1.7543e-04 - val_loss: 0.0011\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 394us/step - loss: 1.6083e-04 - val_loss: 0.0011\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 409us/step - loss: 1.5050e-04 - val_loss: 0.0010\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 399us/step - loss: 1.4898e-04 - val_loss: 0.0010\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.04587556]\n",
      "(86, 5, 1) (86, 1) [0.0586139]\n",
      "(172, 5, 1) (172, 1) [0.10635696]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 12s 21ms/step - loss: 0.1571 - val_loss: 0.4521\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 507us/step - loss: 0.0602 - val_loss: 0.1608\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 574us/step - loss: 0.0164 - val_loss: 0.0226\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 593us/step - loss: 0.0066 - val_loss: 0.0149\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 506us/step - loss: 0.0020 - val_loss: 0.0074\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 391us/step - loss: 3.7118e-04 - val_loss: 0.0084\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 375us/step - loss: 2.6359e-04 - val_loss: 0.0077-0\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 377us/step - loss: 2.3036e-04 - val_loss: 0.0078\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 396us/step - loss: 2.2786e-04 - val_loss: 0.0079\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 399us/step - loss: 2.1291e-04 - val_loss: 0.0078\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00615509]\n",
      "(86, 5, 1) (86, 1) [0.00297467]\n",
      "(172, 5, 1) (172, 1) [0.0048247]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 13s 22ms/step - loss: 0.0266 - val_loss: 0.2299\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 407us/step - loss: 0.0117 - val_loss: 0.0884\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 440us/step - loss: 0.0036 - val_loss: 0.0140\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 411us/step - loss: 1.8742e-04 - val_loss: 0.0032\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 430us/step - loss: 1.5589e-04 - val_loss: 0.0032\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 438us/step - loss: 9.8760e-05 - val_loss: 0.0029\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 447us/step - loss: 9.2072e-05 - val_loss: 0.0030\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 446us/step - loss: 9.0160e-05 - val_loss: 0.0029\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 397us/step - loss: 9.0657e-05 - val_loss: 0.0029\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 411us/step - loss: 8.6040e-05 - val_loss: 0.0029\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00035091]\n",
      "(86, 5, 1) (86, 1) [0.00030122]\n",
      "(172, 5, 1) (172, 1) [0.0003556]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 13s 22ms/step - loss: 0.0513 - val_loss: 0.4175\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 396us/step - loss: 0.0311 - val_loss: 0.2507\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 536us/step - loss: 0.0159 - val_loss: 0.1033\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 611us/step - loss: 0.0037 - val_loss: 0.0080\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 642us/step - loss: 2.7870e-04 - val_loss: 0.0068\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 583us/step - loss: 1.2765e-04 - val_loss: 0.0066\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 606us/step - loss: 1.0374e-04 - val_loss: 0.0067\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 410us/step - loss: 1.1074e-04 - val_loss: 0.0066\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 474us/step - loss: 1.0626e-04 - val_loss: 0.0066\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 438us/step - loss: 1.0021e-04 - val_loss: 0.0066\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.07626078]\n",
      "(86, 5, 1) (86, 1) [0.02349094]\n",
      "(172, 5, 1) (172, 1) [0.01984277]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 14s 23ms/step - loss: 0.1956 - val_loss: 0.2259\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 457us/step - loss: 0.1302 - val_loss: 0.1450\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 449us/step - loss: 0.0724 - val_loss: 0.0640\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 458us/step - loss: 0.0261 - val_loss: 0.0121\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 448us/step - loss: 0.0074 - val_loss: 0.0029\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 412us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 453us/step - loss: 3.9915e-04 - val_loss: 0.0013\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 440us/step - loss: 3.2833e-04 - val_loss: 0.0011\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 464us/step - loss: 3.1562e-04 - val_loss: 0.0011\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 441us/step - loss: 3.0640e-04 - val_loss: 0.0011\n",
      "3048\n",
      "436\n",
      "871\n",
      "(508, 5, 1) (508, 1) [0.03975732]\n",
      "(73, 5, 1) (73, 1) [0.03601307]\n",
      "(146, 5, 1) (146, 1) [0.0385505]\n",
      "Train on 508 samples, validate on 73 samples\n",
      "Epoch 1/10\n",
      "508/508 [==============================] - 15s 30ms/step - loss: 0.1017 - val_loss: 0.1375\n",
      "Epoch 2/10\n",
      "508/508 [==============================] - 0s 604us/step - loss: 0.0315 - val_loss: 0.0244\n",
      "Epoch 3/10\n",
      "508/508 [==============================] - 0s 591us/step - loss: 0.0101 - val_loss: 0.0054\n",
      "Epoch 4/10\n",
      "508/508 [==============================] - 0s 388us/step - loss: 0.0055 - val_loss: 0.0072\n",
      "Epoch 5/10\n",
      "508/508 [==============================] - ETA: 0s - loss: 0.002 - 0s 492us/step - loss: 0.0017 - val_loss: 0.0034\n",
      "Epoch 6/10\n",
      "508/508 [==============================] - 0s 377us/step - loss: 3.4716e-04 - val_loss: 0.0039\n",
      "Epoch 7/10\n",
      "508/508 [==============================] - 0s 558us/step - loss: 2.7786e-04 - val_loss: 0.0039\n",
      "Epoch 8/10\n",
      "508/508 [==============================] - 0s 431us/step - loss: 2.2016e-04 - val_loss: 0.0038\n",
      "Epoch 9/10\n",
      "508/508 [==============================] - 0s 496us/step - loss: 1.9272e-04 - val_loss: 0.0038\n",
      "Epoch 10/10\n",
      "508/508 [==============================] - 0s 380us/step - loss: 1.8995e-04 - val_loss: 0.0039\n",
      "1063\n",
      "152\n",
      "304\n",
      "(177, 5, 1) (177, 1) [0.16682793]\n",
      "(26, 5, 1) (26, 1) [0.14340824]\n",
      "(51, 5, 1) (51, 1) [0.12010858]\n",
      "Train on 177 samples, validate on 26 samples\n",
      "Epoch 1/10\n",
      "177/177 [==============================] - 20s 111ms/step - loss: 0.3772 - val_loss: 0.4408\n",
      "Epoch 2/10\n",
      "177/177 [==============================] - 0s 518us/step - loss: 0.3189 - val_loss: 0.3733\n",
      "Epoch 3/10\n",
      "177/177 [==============================] - 0s 592us/step - loss: 0.2652 - val_loss: 0.3129\n",
      "Epoch 4/10\n",
      "177/177 [==============================] - 0s 456us/step - loss: 0.2143 - val_loss: 0.2507\n",
      "Epoch 5/10\n",
      "177/177 [==============================] - 0s 406us/step - loss: 0.1631 - val_loss: 0.1895\n",
      "Epoch 6/10\n",
      "177/177 [==============================] - 0s 359us/step - loss: 0.1127 - val_loss: 0.1301\n",
      "Epoch 7/10\n",
      "177/177 [==============================] - 0s 563us/step - loss: 0.0698 - val_loss: 0.0780\n",
      "Epoch 8/10\n",
      "177/177 [==============================] - 0s 451us/step - loss: 0.0355 - val_loss: 0.0424\n",
      "Epoch 9/10\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.018 - 0s 1ms/step - loss: 0.0178 - val_loss: 0.0277\n",
      "Epoch 10/10\n",
      "177/177 [==============================] - 0s 558us/step - loss: 0.0158 - val_loss: 0.0253\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.04251683]\n",
      "(86, 5, 1) (86, 1) [0.0783994]\n",
      "(172, 5, 1) (172, 1) [0.05621135]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 15s 24ms/step - loss: 0.0487 - val_loss: 0.1010\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 369us/step - loss: 0.0090 - val_loss: 0.0109\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 367us/step - loss: 0.0037 - val_loss: 0.0142\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 352us/step - loss: 9.2167e-04 - val_loss: 0.0080\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 348us/step - loss: 3.0355e-04 - val_loss: 0.0089\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 349us/step - loss: 2.7225e-04 - val_loss: 0.0086\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 338us/step - loss: 2.3749e-04 - val_loss: 0.0085\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 348us/step - loss: 2.2557e-04 - val_loss: 0.0086\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 369us/step - loss: 2.2239e-04 - val_loss: 0.0086\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 343us/step - loss: 2.1588e-04 - val_loss: 0.0086\n",
      "3570\n",
      "510\n",
      "1021\n",
      "(595, 5, 1) (595, 1) [0.00224291]\n",
      "(85, 5, 1) (85, 1) [0.00149601]\n",
      "(171, 5, 1) (171, 1) [0.00176892]\n",
      "Train on 595 samples, validate on 85 samples\n",
      "Epoch 1/10\n",
      "595/595 [==============================] - 15s 25ms/step - loss: 0.0267 - val_loss: 0.2006\n",
      "Epoch 2/10\n",
      "595/595 [==============================] - 0s 350us/step - loss: 0.0164 - val_loss: 0.1344\n",
      "Epoch 3/10\n",
      "595/595 [==============================] - 0s 354us/step - loss: 0.0114 - val_loss: 0.0822\n",
      "Epoch 4/10\n",
      "595/595 [==============================] - 0s 397us/step - loss: 0.0056 - val_loss: 0.0326\n",
      "Epoch 5/10\n",
      "595/595 [==============================] - 0s 357us/step - loss: 0.0016 - val_loss: 0.0065\n",
      "Epoch 6/10\n",
      "595/595 [==============================] - 0s 351us/step - loss: 3.7168e-04 - val_loss: 0.0054\n",
      "Epoch 7/10\n",
      "595/595 [==============================] - 0s 341us/step - loss: 1.8501e-04 - val_loss: 0.0043\n",
      "Epoch 8/10\n",
      "595/595 [==============================] - 0s 364us/step - loss: 1.1310e-04 - val_loss: 0.0042\n",
      "Epoch 9/10\n",
      "595/595 [==============================] - 0s 357us/step - loss: 9.6864e-05 - val_loss: 0.0042\n",
      "Epoch 10/10\n",
      "595/595 [==============================] - 0s 356us/step - loss: 9.5004e-05 - val_loss: 0.0041\n",
      "3523\n",
      "504\n",
      "1007\n",
      "(587, 5, 1) (587, 1) [0.01668318]\n",
      "(84, 5, 1) (84, 1) [0.0207975]\n",
      "(168, 5, 1) (168, 1) [0.03626894]\n",
      "Train on 587 samples, validate on 84 samples\n",
      "Epoch 1/10\n",
      "587/587 [==============================] - 14s 25ms/step - loss: 0.0645 - val_loss: 0.2472\n",
      "Epoch 2/10\n",
      "587/587 [==============================] - 0s 409us/step - loss: 0.0273 - val_loss: 0.0906\n",
      "Epoch 3/10\n",
      "587/587 [==============================] - 0s 381us/step - loss: 0.0136 - val_loss: 0.0314\n",
      "Epoch 4/10\n",
      "587/587 [==============================] - 0s 395us/step - loss: 0.0073 - val_loss: 0.0140\n",
      "Epoch 5/10\n",
      "587/587 [==============================] - 0s 389us/step - loss: 0.0019 - val_loss: 0.0029\n",
      "Epoch 6/10\n",
      "587/587 [==============================] - 0s 379us/step - loss: 3.4829e-04 - val_loss: 0.0026\n",
      "Epoch 7/10\n",
      "587/587 [==============================] - 0s 379us/step - loss: 2.3693e-04 - val_loss: 0.0023\n",
      "Epoch 8/10\n",
      "587/587 [==============================] - 0s 386us/step - loss: 2.0303e-04 - val_loss: 0.0024\n",
      "Epoch 9/10\n",
      "587/587 [==============================] - 0s 394us/step - loss: 1.9158e-04 - val_loss: 0.0023\n",
      "Epoch 10/10\n",
      "587/587 [==============================] - 0s 378us/step - loss: 1.9222e-04 - val_loss: 0.0024\n",
      "965\n",
      "139\n",
      "276\n",
      "(160, 5, 1) (160, 1) [0.21614611]\n",
      "(24, 5, 1) (24, 1) [0.28526601]\n",
      "(46, 5, 1) (46, 1) [0.27303009]\n",
      "Train on 160 samples, validate on 24 samples\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 15s 92ms/step - loss: 0.2849 - val_loss: 0.5383\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 0s 377us/step - loss: 0.2377 - val_loss: 0.4550\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 0s 367us/step - loss: 0.1961 - val_loss: 0.3823\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 0s 376us/step - loss: 0.1630 - val_loss: 0.3144\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 0s 376us/step - loss: 0.1285 - val_loss: 0.2493\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 0s 346us/step - loss: 0.0995 - val_loss: 0.1844\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 0s 347us/step - loss: 0.0715 - val_loss: 0.1247\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 0s 358us/step - loss: 0.0496 - val_loss: 0.0759\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 0s 356us/step - loss: 0.0324 - val_loss: 0.0445\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 0s 431us/step - loss: 0.0242 - val_loss: 0.0287\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.0214595]\n",
      "(86, 5, 1) (86, 1) [0.07305829]\n",
      "(172, 5, 1) (172, 1) [0.09754099]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 16s 26ms/step - loss: 0.0623 - val_loss: 0.0678\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 402us/step - loss: 0.0229 - val_loss: 0.0096\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 401us/step - loss: 0.0070 - val_loss: 0.0039\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 408us/step - loss: 4.9181e-04 - val_loss: 0.0034\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 429us/step - loss: 2.0489e-04 - val_loss: 0.0026\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 405us/step - loss: 1.6175e-04 - val_loss: 0.0026\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 421us/step - loss: 1.5223e-04 - val_loss: 0.0027\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 414us/step - loss: 1.5769e-04 - val_loss: 0.0027\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 369us/step - loss: 1.5756e-04 - val_loss: 0.0027\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 411us/step - loss: 1.5782e-04 - val_loss: 0.0027\n",
      "3103\n",
      "444\n",
      "887\n",
      "(517, 5, 1) (517, 1) [0.00461608]\n",
      "(74, 5, 1) (74, 1) [0.00241408]\n",
      "(148, 5, 1) (148, 1) [0.00554719]\n",
      "Train on 517 samples, validate on 74 samples\n",
      "Epoch 1/10\n",
      "517/517 [==============================] - 15s 29ms/step - loss: 0.0872 - val_loss: 0.1681\n",
      "Epoch 2/10\n",
      "517/517 [==============================] - 0s 467us/step - loss: 0.0477 - val_loss: 0.0747\n",
      "Epoch 3/10\n",
      "517/517 [==============================] - 0s 451us/step - loss: 0.0238 - val_loss: 0.0206\n",
      "Epoch 4/10\n",
      "517/517 [==============================] - 0s 654us/step - loss: 0.0052 - val_loss: 0.0028\n",
      "Epoch 5/10\n",
      "517/517 [==============================] - 0s 603us/step - loss: 5.7277e-04 - val_loss: 0.0026\n",
      "Epoch 6/10\n",
      "517/517 [==============================] - 0s 448us/step - loss: 2.0795e-04 - val_loss: 0.0024\n",
      "Epoch 7/10\n",
      "517/517 [==============================] - 0s 378us/step - loss: 1.5011e-04 - val_loss: 0.0024\n",
      "Epoch 8/10\n",
      "517/517 [==============================] - 0s 384us/step - loss: 1.3566e-04 - val_loss: 0.0024\n",
      "Epoch 9/10\n",
      "517/517 [==============================] - 0s 388us/step - loss: 1.3076e-04 - val_loss: 0.0024\n",
      "Epoch 10/10\n",
      "517/517 [==============================] - 0s 417us/step - loss: 1.1888e-04 - val_loss: 0.0024\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00539065]\n",
      "(86, 5, 1) (86, 1) [0.03520309]\n",
      "(172, 5, 1) (172, 1) [0.03572705]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 16s 26ms/step - loss: 0.0194 - val_loss: 0.1510\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 403us/step - loss: 0.0077 - val_loss: 0.0514\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 400us/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 395us/step - loss: 3.0582e-04 - val_loss: 0.0022\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 412us/step - loss: 1.8138e-04 - val_loss: 0.0022\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 412us/step - loss: 1.6496e-04 - val_loss: 0.0022\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 412us/step - loss: 1.6315e-04 - val_loss: 0.0021\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 376us/step - loss: 1.5812e-04 - val_loss: 0.0022\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 392us/step - loss: 1.5159e-04 - val_loss: 0.0021\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 403us/step - loss: 1.5544e-04 - val_loss: 0.0021\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.01944735]\n",
      "(86, 5, 1) (86, 1) [0.02960787]\n",
      "(172, 5, 1) (172, 1) [0.02964641]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 16s 26ms/step - loss: 0.0720 - val_loss: 0.3942\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - ETA: 0s - loss: 0.042 - 0s 389us/step - loss: 0.0375 - val_loss: 0.1972\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 474us/step - loss: 0.0166 - val_loss: 0.0688\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 511us/step - loss: 0.0075 - val_loss: 0.0284\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 676us/step - loss: 7.9776e-04 - val_loss: 0.0068\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 438us/step - loss: 2.3345e-04 - val_loss: 0.0073\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 351us/step - loss: 1.7191e-04 - val_loss: 0.0071\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 384us/step - loss: 1.6560e-04 - val_loss: 0.0068\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 372us/step - loss: 1.5562e-04 - val_loss: 0.0068\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 379us/step - loss: 1.5462e-04 - val_loss: 0.0068\n",
      "3382\n",
      "483\n",
      "967\n",
      "(563, 5, 1) (563, 1) [0.03423614]\n",
      "(81, 5, 1) (81, 1) [0.02269025]\n",
      "(162, 5, 1) (162, 1) [0.01423206]\n",
      "Train on 563 samples, validate on 81 samples\n",
      "Epoch 1/10\n",
      "563/563 [==============================] - 16s 29ms/step - loss: 0.0727 - val_loss: 0.2196\n",
      "Epoch 2/10\n",
      "563/563 [==============================] - 0s 412us/step - loss: 0.0336 - val_loss: 0.0783\n",
      "Epoch 3/10\n",
      "563/563 [==============================] - 0s 412us/step - loss: 0.0093 - val_loss: 0.0106\n",
      "Epoch 4/10\n",
      "563/563 [==============================] - 0s 420us/step - loss: 6.9243e-04 - val_loss: 0.0055\n",
      "Epoch 5/10\n",
      "563/563 [==============================] - 0s 411us/step - loss: 1.5534e-04 - val_loss: 0.0043\n",
      "Epoch 6/10\n",
      "563/563 [==============================] - 0s 399us/step - loss: 8.3122e-05 - val_loss: 0.0041-0\n",
      "Epoch 7/10\n",
      "563/563 [==============================] - 0s 393us/step - loss: 6.5983e-05 - val_loss: 0.0040\n",
      "Epoch 8/10\n",
      "563/563 [==============================] - 0s 387us/step - loss: 6.2660e-05 - val_loss: 0.0040\n",
      "Epoch 9/10\n",
      "563/563 [==============================] - 0s 386us/step - loss: 7.2090e-05 - val_loss: 0.0040\n",
      "Epoch 10/10\n",
      "563/563 [==============================] - 0s 386us/step - loss: 5.9501e-05 - val_loss: 0.0040\n",
      "2769\n",
      "396\n",
      "792\n",
      "(461, 5, 1) (461, 1) [0.05672118]\n",
      "(66, 5, 1) (66, 1) [0.06840975]\n",
      "(132, 5, 1) (132, 1) [0.06489293]\n",
      "Train on 461 samples, validate on 66 samples\n",
      "Epoch 1/10\n",
      "461/461 [==============================] - 17s 36ms/step - loss: 0.0722 - val_loss: 0.2826\n",
      "Epoch 2/10\n",
      "461/461 [==============================] - 0s 477us/step - loss: 0.0406 - val_loss: 0.1216\n",
      "Epoch 3/10\n",
      "461/461 [==============================] - 0s 480us/step - loss: 0.0203 - val_loss: 0.0504\n",
      "Epoch 4/10\n",
      "461/461 [==============================] - 0s 422us/step - loss: 0.0049 - val_loss: 0.0084\n",
      "Epoch 5/10\n",
      "461/461 [==============================] - 0s 453us/step - loss: 5.9451e-04 - val_loss: 0.0083\n",
      "Epoch 6/10\n",
      "461/461 [==============================] - 0s 462us/step - loss: 4.0079e-04 - val_loss: 0.0069\n",
      "Epoch 7/10\n",
      "461/461 [==============================] - 0s 446us/step - loss: 2.5985e-04 - val_loss: 0.0067\n",
      "Epoch 8/10\n",
      "461/461 [==============================] - 0s 446us/step - loss: 2.1431e-04 - val_loss: 0.0067\n",
      "Epoch 9/10\n",
      "461/461 [==============================] - 0s 423us/step - loss: 1.9019e-04 - val_loss: 0.0067\n",
      "Epoch 10/10\n",
      "461/461 [==============================] - 0s 445us/step - loss: 1.9505e-04 - val_loss: 0.0066\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.059241]\n",
      "(86, 5, 1) (86, 1) [0.09642923]\n",
      "(172, 5, 1) (172, 1) [0.17157662]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 17s 28ms/step - loss: 0.1049 - val_loss: 0.5242\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 398us/step - loss: 0.0665 - val_loss: 0.3513\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 419us/step - loss: 0.0366 - val_loss: 0.1832\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 426us/step - loss: 0.0159 - val_loss: 0.0656\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 425us/step - loss: 0.0028 - val_loss: 0.0097\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 415us/step - loss: 4.8920e-04 - val_loss: 0.0079\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 421us/step - loss: 2.6598e-04 - val_loss: 0.0078\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 415us/step - loss: 2.4363e-04 - val_loss: 0.0076\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 396us/step - loss: 2.4682e-04 - val_loss: 0.0077\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 440us/step - loss: 2.3900e-04 - val_loss: 0.0077\n",
      "3600\n",
      "515\n",
      "1029\n",
      "(600, 5, 1) (600, 1) [0.00862325]\n",
      "(86, 5, 1) (86, 1) [0.00275447]\n",
      "(172, 5, 1) (172, 1) [0.00681792]\n",
      "Train on 600 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "600/600 [==============================] - 21s 35ms/step - loss: 0.0325 - val_loss: 0.0515\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 0s 526us/step - loss: 0.0206 - val_loss: 0.0267\n",
      "Epoch 3/10\n",
      "600/600 [==============================] - 0s 507us/step - loss: 0.0116 - val_loss: 0.0105\n",
      "Epoch 4/10\n",
      "600/600 [==============================] - 0s 486us/step - loss: 0.0041 - val_loss: 0.0011\n",
      "Epoch 5/10\n",
      "600/600 [==============================] - 0s 531us/step - loss: 0.0019 - val_loss: 9.1055e-04\n",
      "Epoch 6/10\n",
      "600/600 [==============================] - 0s 609us/step - loss: 0.0017 - val_loss: 8.1264e-04\n",
      "Epoch 7/10\n",
      "600/600 [==============================] - 0s 822us/step - loss: 0.0017 - val_loss: 9.5882e-04\n",
      "Epoch 8/10\n",
      "600/600 [==============================] - 0s 688us/step - loss: 0.0017 - val_loss: 9.0289e-04\n",
      "Epoch 9/10\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.0019    - 0s 504us/step - loss: 0.0016 - val_loss: 8.3312e-04\n",
      "Epoch 10/10\n",
      "600/600 [==============================] - 0s 509us/step - loss: 0.0016 - val_loss: 8.2660e-04\n",
      "2464\n",
      "352\n",
      "704\n",
      "(410, 5, 1) (410, 1) [0.04745972]\n",
      "(59, 5, 1) (59, 1) [0.13884261]\n",
      "(118, 5, 1) (118, 1) [0.05667392]\n",
      "Train on 410 samples, validate on 59 samples\n",
      "Epoch 1/10\n",
      "410/410 [==============================] - 19s 45ms/step - loss: 0.1215 - val_loss: 0.1051\n",
      "Epoch 2/10\n",
      "410/410 [==============================] - 0s 566us/step - loss: 0.0553 - val_loss: 0.0345\n",
      "Epoch 3/10\n",
      "410/410 [==============================] - 0s 505us/step - loss: 0.0252 - val_loss: 0.0151\n",
      "Epoch 4/10\n",
      "410/410 [==============================] - 0s 506us/step - loss: 0.0159 - val_loss: 0.0130\n",
      "Epoch 5/10\n",
      "410/410 [==============================] - 0s 537us/step - loss: 0.0058 - val_loss: 0.0112\n",
      "Epoch 6/10\n",
      "410/410 [==============================] - 0s 567us/step - loss: 5.6395e-04 - val_loss: 0.0123\n",
      "Epoch 7/10\n",
      "410/410 [==============================] - 0s 567us/step - loss: 4.2354e-04 - val_loss: 0.0120\n",
      "Epoch 8/10\n",
      "410/410 [==============================] - 0s 539us/step - loss: 2.2720e-04 - val_loss: 0.0117\n",
      "Epoch 9/10\n",
      "410/410 [==============================] - 0s 530us/step - loss: 2.2598e-04 - val_loss: 0.0117\n",
      "Epoch 10/10\n",
      "410/410 [==============================] - 0s 552us/step - loss: 2.0815e-04 - val_loss: 0.0118\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.01542146]\n",
      "(86, 5, 1) (86, 1) [0.01289489]\n",
      "(172, 5, 1) (172, 1) [0.02005234]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 18s 30ms/step - loss: 0.0803 - val_loss: 0.4165\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - ETA: 0s - loss: 0.033 - 0s 536us/step - loss: 0.0307 - val_loss: 0.1578\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 500us/step - loss: 0.0111 - val_loss: 0.0494\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 497us/step - loss: 0.0022 - val_loss: 0.0085\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 505us/step - loss: 2.1047e-04 - val_loss: 0.0055\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 488us/step - loss: 1.4080e-04 - val_loss: 0.0055\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 490us/step - loss: 1.1698e-04 - val_loss: 0.0054\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 489us/step - loss: 1.1157e-04 - val_loss: 0.0054\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 501us/step - loss: 1.0873e-04 - val_loss: 0.0055\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 502us/step - loss: 1.0652e-04 - val_loss: 0.0056\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00010367]\n",
      "(86, 5, 1) (86, 1) [0.0001086]\n",
      "(172, 5, 1) (172, 1) [0.0003008]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 20s 34ms/step - loss: 0.0444 - val_loss: 0.3629\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 674us/step - loss: 0.0255 - val_loss: 0.1779\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 553us/step - loss: 0.0143 - val_loss: 0.0891\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 646us/step - loss: 0.0036 - val_loss: 0.0123\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 634us/step - loss: 2.9574e-04 - val_loss: 0.0117\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 605us/step - loss: 1.7922e-04 - val_loss: 0.0110\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 559us/step - loss: 1.3408e-04 - val_loss: 0.0108\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 604us/step - loss: 1.2227e-04 - val_loss: 0.0108\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 450us/step - loss: 1.2430e-04 - val_loss: 0.0108\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 555us/step - loss: 1.2080e-04 - val_loss: 0.0109\n",
      "1304\n",
      "186\n",
      "373\n",
      "(217, 5, 1) (217, 1) [0.0677204]\n",
      "(31, 5, 1) (31, 1) [0.24853983]\n",
      "(63, 5, 1) (63, 1) [0.12703736]\n",
      "Train on 217 samples, validate on 31 samples\n",
      "Epoch 1/10\n",
      "217/217 [==============================] - 19s 88ms/step - loss: 0.1702 - val_loss: 0.1975\n",
      "Epoch 2/10\n",
      "217/217 [==============================] - 0s 649us/step - loss: 0.1290 - val_loss: 0.1514\n",
      "Epoch 3/10\n",
      "217/217 [==============================] - 0s 568us/step - loss: 0.0942 - val_loss: 0.1106\n",
      "Epoch 4/10\n",
      "217/217 [==============================] - 0s 558us/step - loss: 0.0633 - val_loss: 0.0754\n",
      "Epoch 5/10\n",
      "217/217 [==============================] - 0s 539us/step - loss: 0.0376 - val_loss: 0.0478\n",
      "Epoch 6/10\n",
      "217/217 [==============================] - 0s 549us/step - loss: 0.0197 - val_loss: 0.0313\n",
      "Epoch 7/10\n",
      "217/217 [==============================] - 0s 551us/step - loss: 0.0122 - val_loss: 0.0263\n",
      "Epoch 8/10\n",
      "217/217 [==============================] - 0s 571us/step - loss: 0.0107 - val_loss: 0.0260\n",
      "Epoch 9/10\n",
      "217/217 [==============================] - 0s 575us/step - loss: 0.0085 - val_loss: 0.0263\n",
      "Epoch 10/10\n",
      "217/217 [==============================] - 0s 586us/step - loss: 0.0067 - val_loss: 0.0268\n",
      "1684\n",
      "240\n",
      "482\n",
      "(280, 5, 1) (280, 1) [0.05826827]\n",
      "(40, 5, 1) (40, 1) [0.08190411]\n",
      "(81, 5, 1) (81, 1) [0.08620169]\n",
      "Train on 280 samples, validate on 40 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - 19s 69ms/step - loss: 0.3108 - val_loss: 0.5201\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - 0s 649us/step - loss: 0.2591 - val_loss: 0.4359\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - 0s 529us/step - loss: 0.2044 - val_loss: 0.3376\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - 0s 565us/step - loss: 0.1458 - val_loss: 0.2278\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - 0s 591us/step - loss: 0.0860 - val_loss: 0.1208\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - 0s 579us/step - loss: 0.0405 - val_loss: 0.0433\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - 0s 627us/step - loss: 0.0167 - val_loss: 0.0189\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - 0s 618us/step - loss: 0.0094 - val_loss: 0.0223\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - 0s 564us/step - loss: 0.0037 - val_loss: 0.0233\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - 0s 623us/step - loss: 0.0010 - val_loss: 0.0255\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00471111]\n",
      "(86, 5, 1) (86, 1) [0.00532617]\n",
      "(172, 5, 1) (172, 1) [0.00587844]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 20s 32ms/step - loss: 0.0546 - val_loss: 0.4110\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 604us/step - loss: 0.0319 - val_loss: 0.2669\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 602us/step - loss: 0.0209 - val_loss: 0.1614\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 769us/step - loss: 0.0125 - val_loss: 0.0868\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 686us/step - loss: 0.0037 - val_loss: 0.0138\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 494us/step - loss: 3.9913e-04 - val_loss: 0.0047\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 473us/step - loss: 2.5920e-04 - val_loss: 0.0041\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 509us/step - loss: 2.2243e-04 - val_loss: 0.0039\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 515us/step - loss: 1.9394e-04 - val_loss: 0.0038\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 530us/step - loss: 1.8456e-04 - val_loss: 0.0038\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.05923293]\n",
      "(86, 5, 1) (86, 1) [0.03289733]\n",
      "(172, 5, 1) (172, 1) [0.0743865]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 23s 39ms/step - loss: 0.0428 - val_loss: 0.3069\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 423us/step - loss: 0.0168 - val_loss: 0.1225\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 527us/step - loss: 0.0090 - val_loss: 0.0723\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - ETA: 0s - loss: 0.003 - 0s 409us/step - loss: 0.0033 - val_loss: 0.0139\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 390us/step - loss: 3.4407e-04 - val_loss: 0.0043\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 366us/step - loss: 1.6608e-04 - val_loss: 0.0037\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 415us/step - loss: 1.2041e-04 - val_loss: 0.0037\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 483us/step - loss: 1.0564e-04 - val_loss: 0.0036\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 603us/step - loss: 1.0240e-04 - val_loss: 0.0036\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 682us/step - loss: 9.7748e-05 - val_loss: 0.0036\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.00094813]\n",
      "(86, 5, 1) (86, 1) [0.00084957]\n",
      "(172, 5, 1) (172, 1) [0.00203887]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 24s 39ms/step - loss: 0.0320 - val_loss: 0.0939\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 469us/step - loss: 0.0111 - val_loss: 0.0234\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 470us/step - loss: 0.0012 - val_loss: 0.0059\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 498us/step - loss: 2.8304e-04 - val_loss: 0.0046\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 499us/step - loss: 1.2753e-04 - val_loss: 0.0045\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 436us/step - loss: 1.0582e-04 - val_loss: 0.0046\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 459us/step - loss: 9.1740e-05 - val_loss: 0.0046\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 455us/step - loss: 9.0176e-05 - val_loss: 0.0046\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 420us/step - loss: 9.3954e-05 - val_loss: 0.0046\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 410us/step - loss: 8.8046e-05 - val_loss: 0.0046\n",
      "2511\n",
      "359\n",
      "718\n",
      "(418, 5, 1) (418, 1) [0.02770237]\n",
      "(60, 5, 1) (60, 1) [0.04884172]\n",
      "(120, 5, 1) (120, 1) [0.09403003]\n",
      "Train on 418 samples, validate on 60 samples\n",
      "Epoch 1/10\n",
      "418/418 [==============================] - 23s 56ms/step - loss: 0.1368 - val_loss: 0.4701\n",
      "Epoch 2/10\n",
      "418/418 [==============================] - 0s 453us/step - loss: 0.0882 - val_loss: 0.2744\n",
      "Epoch 3/10\n",
      "418/418 [==============================] - 0s 570us/step - loss: 0.0483 - val_loss: 0.1232\n",
      "Epoch 4/10\n",
      "418/418 [==============================] - 0s 487us/step - loss: 0.0298 - val_loss: 0.0572\n",
      "Epoch 5/10\n",
      "418/418 [==============================] - 0s 548us/step - loss: 0.0135 - val_loss: 0.0245\n",
      "Epoch 6/10\n",
      "418/418 [==============================] - 0s 478us/step - loss: 0.0018 - val_loss: 0.0112\n",
      "Epoch 7/10\n",
      "418/418 [==============================] - 0s 468us/step - loss: 5.1333e-04 - val_loss: 0.0118\n",
      "Epoch 8/10\n",
      "418/418 [==============================] - 0s 566us/step - loss: 2.5394e-04 - val_loss: 0.0109\n",
      "Epoch 9/10\n",
      "418/418 [==============================] - 0s 564us/step - loss: 2.2693e-04 - val_loss: 0.0107\n",
      "Epoch 10/10\n",
      "418/418 [==============================] - 0s 510us/step - loss: 2.1101e-04 - val_loss: 0.0107\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.01068849]\n",
      "(86, 5, 1) (86, 1) [0.01184141]\n",
      "(172, 5, 1) (172, 1) [0.03519763]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 22s 37ms/step - loss: 0.0784 - val_loss: 0.3116\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 488us/step - loss: 0.0453 - val_loss: 0.1716\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 488us/step - loss: 0.0196 - val_loss: 0.0713\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 521us/step - loss: 0.0125 - val_loss: 0.0452\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 512us/step - loss: 0.0071 - val_loss: 0.0199\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 485us/step - loss: 0.0018 - val_loss: 0.0048\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 448us/step - loss: 3.5339e-04 - val_loss: 0.0042\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 436us/step - loss: 2.5484e-04 - val_loss: 0.0040\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 453us/step - loss: 2.4537e-04 - val_loss: 0.0040\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 429us/step - loss: 2.2994e-04 - val_loss: 0.0040\n",
      "1789\n",
      "256\n",
      "512\n",
      "(298, 5, 1) (298, 1) [0.00297202]\n",
      "(43, 5, 1) (43, 1) [0.00398502]\n",
      "(86, 5, 1) (86, 1) [0.00871671]\n",
      "Train on 298 samples, validate on 43 samples\n",
      "Epoch 1/10\n",
      "298/298 [==============================] - 21s 70ms/step - loss: 0.1205 - val_loss: 0.3526\n",
      "Epoch 2/10\n",
      "298/298 [==============================] - 0s 475us/step - loss: 0.0714 - val_loss: 0.2071\n",
      "Epoch 3/10\n",
      "298/298 [==============================] - 0s 512us/step - loss: 0.0384 - val_loss: 0.0894\n",
      "Epoch 4/10\n",
      "298/298 [==============================] - 0s 507us/step - loss: 0.0187 - val_loss: 0.0286\n",
      "Epoch 5/10\n",
      "298/298 [==============================] - 0s 535us/step - loss: 0.0109 - val_loss: 0.0134\n",
      "Epoch 6/10\n",
      "298/298 [==============================] - 0s 582us/step - loss: 0.0043 - val_loss: 0.0135\n",
      "Epoch 7/10\n",
      "298/298 [==============================] - 0s 508us/step - loss: 8.9046e-04 - val_loss: 0.0111\n",
      "Epoch 8/10\n",
      "298/298 [==============================] - 0s 542us/step - loss: 4.7511e-04 - val_loss: 0.0119\n",
      "Epoch 9/10\n",
      "298/298 [==============================] - 0s 568us/step - loss: 5.0933e-04 - val_loss: 0.0114\n",
      "Epoch 10/10\n",
      "298/298 [==============================] - 0s 507us/step - loss: 3.8412e-04 - val_loss: 0.0111\n",
      "3594\n",
      "514\n",
      "1027\n",
      "(599, 5, 1) (599, 1) [0.01545082]\n",
      "(86, 5, 1) (86, 1) [0.017134]\n",
      "(172, 5, 1) (172, 1) [0.02640578]\n",
      "Train on 599 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "599/599 [==============================] - 22s 36ms/step - loss: 0.0668 - val_loss: 0.2726\n",
      "Epoch 2/10\n",
      "599/599 [==============================] - 0s 475us/step - loss: 0.0287 - val_loss: 0.0991\n",
      "Epoch 3/10\n",
      "599/599 [==============================] - 0s 473us/step - loss: 0.0146 - val_loss: 0.0456\n",
      "Epoch 4/10\n",
      "599/599 [==============================] - 0s 466us/step - loss: 0.0037 - val_loss: 0.0082\n",
      "Epoch 5/10\n",
      "599/599 [==============================] - 0s 505us/step - loss: 4.6455e-04 - val_loss: 0.0081\n",
      "Epoch 6/10\n",
      "599/599 [==============================] - 0s 610us/step - loss: 2.7027e-04 - val_loss: 0.0083\n",
      "Epoch 7/10\n",
      "599/599 [==============================] - 0s 513us/step - loss: 2.2905e-04 - val_loss: 0.0081\n",
      "Epoch 8/10\n",
      "599/599 [==============================] - 0s 449us/step - loss: 2.1601e-04 - val_loss: 0.0081\n",
      "Epoch 9/10\n",
      "599/599 [==============================] - 0s 465us/step - loss: 2.0454e-04 - val_loss: 0.0081\n",
      "Epoch 10/10\n",
      "599/599 [==============================] - 0s 542us/step - loss: 2.0915e-04 - val_loss: 0.0082-0\n",
      "3610\n",
      "516\n",
      "1032\n",
      "(601, 5, 1) (601, 1) [0.08888889]\n",
      "(86, 5, 1) (86, 1) [0.00708466]\n",
      "(172, 5, 1) (172, 1) [0.00974659]\n",
      "Train on 601 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 23s 38ms/step - loss: 0.1364 - val_loss: 0.0721\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 0s 487us/step - loss: 0.0503 - val_loss: 0.0234\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 0s 497us/step - loss: 0.0139 - val_loss: 0.0140\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 0s 500us/step - loss: 0.0070 - val_loss: 0.0067\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 0s 500us/step - loss: 0.0028 - val_loss: 0.0020\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 0s 472us/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 0s 498us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 0s 490us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 0s 477us/step - loss: 9.8821e-04 - val_loss: 0.0013\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 0s 487us/step - loss: 9.7684e-04 - val_loss: 0.0013\n"
     ]
    }
   ],
   "source": [
    "global mean_acc\n",
    "mean_acc = []\n",
    "def main(df, column):        \n",
    "    train_data, test_data, val_data = test_train_split(df, column)\n",
    "    tempScaler = MinMaxScaler()\n",
    "    scaled_data = tempScaler.fit_transform(train_data)\n",
    "    x_train, y_train, scale_train = train_prep(train_data, 5)\n",
    "    x_test, y_test, test_scale = test_prep(test_data, train_data, 5)\n",
    "    x_val, y_val, val_scale = val_prep(val_data, test_data, 5)\n",
    "    model = create_model(x_train, 16, 32)\n",
    "    history = compile_model(model, x_train, y_train, x_test, y_test)\n",
    "    y_hat = model.predict(x_test)\n",
    "    y_hat_unscaled = y_hat/test_scale[0]\n",
    "    y_test_unscaled = y_test/test_scale[0]\n",
    "    y_hat_val = model.predict(x_val)\n",
    "    y_hat_val_unscaled = y_hat_val/val_scale[0]\n",
    "    y_val_unscaled = y_val/val_scale[0] \n",
    "    \n",
    "    lst, summ = [], 0\n",
    "    for i in range(1, len(y_val_unscaled-1)):\n",
    "        if (y_val_unscaled[i][0] >= y_hat_val_unscaled[i][0]):\n",
    "            accuracy = (y_hat_val_unscaled[i][0]/y_val_unscaled[i][0])*100\n",
    "            lst.append(accuracy)\n",
    "        else:\n",
    "            accuracy = (y_val_unscaled[i][0]/y_hat_val_unscaled[i][0])*100\n",
    "            lst.append(accuracy)\n",
    "    for i in lst:\n",
    "        summ+=i\n",
    "    mean_acc.append(summ/len(lst))\n",
    "    return mean_acc\n",
    "\n",
    "def companies(names):\n",
    "    for i in names:\n",
    "        df = pd.read_csv(i) \n",
    "        mean_acc = main(df, 'Open')\n",
    "    return mean_acc\n",
    "mean_acc = companies(name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy = (sum(mean_acc)/len(mean_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.57899994038615 %\n"
     ]
    }
   ],
   "source": [
    "print(Accuracy,'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
